{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Available at: /rds/general/user/aqa20/home/data/hdspikes/shd_train.h5\n",
      "Available at: /rds/general/user/aqa20/home/data/hdspikes/shd_test.h5\n",
      "Available at: /rds/general/user/aqa20/home/data/hdspikes/shd_train.h5\n",
      "Available at: /rds/general/user/aqa20/home/data/hdspikes/shd_test.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "# print(torch.__version__)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "# import torchvision\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "import pickle\n",
    "import math\n",
    "from utils import get_shd_dataset\n",
    "\n",
    "# The coarse network structure and the time steps are dicated by the SHD dataset.\n",
    "nb_inputs  = 700\n",
    "nb_hidden  = 200\n",
    "nb_outputs = 20\n",
    "\n",
    "time_step = 1e-3\n",
    "nb_steps = 100\n",
    "max_time = 1.4\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "dtype = torch.float\n",
    "\n",
    "# Check whether a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Here we load the Dataset\n",
    "cache_dir = os.path.expanduser(\"~/data\")\n",
    "cache_subdir = \"hdspikes\"\n",
    "get_shd_dataset(cache_dir, cache_subdir)\n",
    "\n",
    "# Here we load the Dataset\n",
    "cache_dir = os.path.expanduser(\"~/data\")\n",
    "cache_subdir = \"hdspikes\"\n",
    "get_shd_dataset(cache_dir, cache_subdir)\n",
    "\n",
    "train_file = h5py.File(os.path.join(cache_dir, cache_subdir, 'shd_train.h5'), 'r')\n",
    "test_file = h5py.File(os.path.join(cache_dir, cache_subdir, 'shd_test.h5'), 'r')\n",
    "\n",
    "x_train = train_file['spikes']\n",
    "y_train = train_file['labels']\n",
    "x_test = test_file['spikes']\n",
    "y_test = test_file['labels']\n",
    "\n",
    "def sparse_data_generator_from_hdf5_spikes(X, y, batch_size, nb_steps, nb_units, max_time, shuffle=True):\n",
    "    \"\"\" This generator takes a spike dataset and generates spiking network input as sparse tensors.\n",
    "\n",
    "    Args:\n",
    "        X: The data ( sample x event x 2 ) the last dim holds (time,neuron) tuples\n",
    "        y: The labels\n",
    "    \"\"\"\n",
    "\n",
    "    labels_ = np.array(y,dtype=int)\n",
    "    number_of_batches = len(labels_)//batch_size\n",
    "    sample_index = np.arange(len(labels_))\n",
    "\n",
    "    # compute discrete firing times\n",
    "    firing_times = X['times']\n",
    "    units_fired = X['units']\n",
    "\n",
    "    time_bins = np.linspace(0, max_time, num=nb_steps)\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "\n",
    "    total_batch_count = 0\n",
    "    counter = 0\n",
    "    while counter<number_of_batches:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "\n",
    "        coo = [ [] for i in range(3) ]\n",
    "        for bc,idx in enumerate(batch_index):\n",
    "            times = np.digitize(firing_times[idx], time_bins)\n",
    "            units = units_fired[idx]\n",
    "            batch = [bc for _ in range(len(times))]\n",
    "\n",
    "            coo[0].extend(batch)\n",
    "            coo[1].extend(times)\n",
    "            coo[2].extend(units)\n",
    "\n",
    "        i = torch.LongTensor(coo).to(device)\n",
    "        v = torch.FloatTensor(np.ones(len(coo[0]))).to(device)\n",
    "\n",
    "        X_batch = torch.sparse.FloatTensor(i, v, torch.Size([batch_size,nb_steps,nb_units])).to(device)\n",
    "        y_batch = torch.tensor(labels_[batch_index],device=device)\n",
    "\n",
    "        yield X_batch.to(device=device), y_batch.to(device=device)\n",
    "\n",
    "        counter += 1\n",
    "        \n",
    "class SurrGradSpike(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Here we implement our spiking nonlinearity which also implements\n",
    "    the surrogate gradient. By subclassing torch.autograd.Function,\n",
    "    we will be able to use all of PyTorch's autograd functionality.\n",
    "    Here we use the normalized negative part of a fast sigmoid\n",
    "    as this was done in Zenke & Ganguli (2018).\n",
    "    \"\"\"\n",
    "\n",
    "    scale = 100.0 # controls steepness of surrogate gradient\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we compute a step function of the input Tensor\n",
    "        and return it. ctx is a context object that we use to stash information which\n",
    "        we need to later backpropagate our error signals. To achieve this we use the\n",
    "        ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        out = torch.zeros_like(input)\n",
    "        out[input > 0] = 1.0\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor we need to compute the\n",
    "        surrogate gradient of the loss with respect to the input.\n",
    "        Here we use the normalized negative part of a fast sigmoid\n",
    "        as this was done in Zenke & Ganguli (2018).\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad = grad_input/(SurrGradSpike.scale*torch.abs(input)+1.0)**2\n",
    "        return grad\n",
    "\n",
    "# here we overwrite our naive spike function by the \"SurrGradSpike\" nonlinearity which implements a surrogate gradient\n",
    "spike_fn  = SurrGradSpike.apply\n",
    "\n",
    "def dist_fn(dist):\n",
    "    return {\n",
    "        'gamma': lambda mean, k, size: np.random.gamma(k, scale=mean/k, size=size),\n",
    "        'normal': lambda mean, k, size: np.random.normal(loc=mean, scale=mean/np.sqrt(k), size=size), #change standard deviation to match gamma\n",
    "        'uniform': lambda _, maximum, size: np.random.uniform(low=0, high=maximum, size=size),\n",
    "    }[dist.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_snn_hetero(inputs):\n",
    "    # Expand parameters locally to match original sizes\n",
    "    alpha_1_local = alpha_hetero_1.repeat_interleave(group_size, dim=1)\n",
    "    beta_1_local = beta_hetero_1.repeat_interleave(group_size, dim=1)\n",
    "    thresholds_local = thresholds_1.repeat_interleave(group_size, dim=1)\n",
    "    reset_local = reset_1.repeat_interleave(group_size, dim=1)\n",
    "    rest_local = rest_1.repeat_interleave(group_size, dim=1)\n",
    "    alpha_2_local = alpha_hetero_2.repeat_interleave(group_size, dim=1)\n",
    "    beta_2_local = beta_hetero_2.repeat_interleave(group_size, dim=1)\n",
    "#     print(beta_2_local)\n",
    "#     print(thresholds_local.shape)\n",
    "\n",
    "\n",
    "    # Initialize memory and synaptic variables\n",
    "    syn = torch.zeros((batch_size_hetero, nb_hidden), device=device, dtype=dtype)\n",
    "    mem = torch.zeros((batch_size_hetero, nb_hidden), device=device, dtype=dtype)\n",
    "\n",
    "    mem_rec = []\n",
    "    spk_rec = []\n",
    "\n",
    "    # Compute hidden layer activity\n",
    "    out = torch.zeros((batch_size_hetero, nb_hidden), device=device, dtype=dtype)\n",
    "    h1_from_input = torch.einsum(\"abc,cd->abd\", (inputs, w1))\n",
    "    for t in range(nb_steps):\n",
    "        h1 = h1_from_input[:, t] + torch.einsum(\"ab,bc->ac\", (out, v1))\n",
    "        mthr = mem - thresholds_local\n",
    "        out = spike_fn(mthr)\n",
    "        rst = torch.zeros_like(mem)\n",
    "        c = (mthr > 0)\n",
    "        rst[c] = torch.ones_like(mem)[c]\n",
    "\n",
    "        new_syn = alpha_1_local * syn + h1\n",
    "        new_mem = beta_1_local * (mem - rest_local) + rest_local + (1 - beta_1_local) * syn - rst * (thresholds_local - reset_local)\n",
    "\n",
    "        mem_rec.append(mem)\n",
    "        spk_rec.append(out)\n",
    "\n",
    "        mem = new_mem\n",
    "        syn = new_syn\n",
    "\n",
    "    mem_rec = torch.stack(mem_rec, dim=1)\n",
    "    spk_rec = torch.stack(spk_rec, dim=1)\n",
    "\n",
    "    # Readout layer\n",
    "    h2 = torch.einsum(\"abc,cd->abd\", (spk_rec, w2))\n",
    "    flt = torch.zeros((batch_size_hetero, nb_outputs), device=device, dtype=dtype)\n",
    "    out = torch.zeros((batch_size_hetero, nb_outputs), device=device, dtype=dtype)\n",
    "    out_rec = [out]\n",
    "    for t in range(nb_steps):\n",
    "        new_flt = alpha_2_local * flt + h2[:, t]\n",
    "        new_out = beta_2_local * out + (1 - beta_2_local) * flt\n",
    "\n",
    "        flt = new_flt\n",
    "        out = new_out\n",
    "\n",
    "        out_rec.append(out)\n",
    "\n",
    "    out_rec = torch.stack(out_rec, dim=1)\n",
    "    other_recs = [mem_rec, spk_rec]\n",
    "    return out_rec, other_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classification_accuracy_hetero(x_data, y_data):\n",
    "    \"\"\" Computes classification accuracy on supplied data in batches. \"\"\"\n",
    "    accs = []\n",
    "    for x_local, y_local in sparse_data_generator_from_hdf5_spikes(x_data, y_data, batch_size_hetero, nb_steps, nb_inputs, max_time, shuffle=False):\n",
    "        output,_ = run_snn_hetero(x_local.to_dense())\n",
    "        m,_= torch.max(output,1) # max over time\n",
    "        _,am=torch.max(m,1)      # argmax over output units\n",
    "        tmp = np.mean((y_local==am).detach().cpu().numpy()) # compare to labels\n",
    "        accs.append(tmp)\n",
    "    return np.mean(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_snn_hetero(x_data, y_data, lr=1e-3, nb_epochs=10):\n",
    "    params = [w1, w2, v1, alpha_hetero_1, beta_hetero_1,\n",
    "              alpha_hetero_2, beta_hetero_2,\n",
    "              thresholds_1, reset_1, rest_1]\n",
    "    optimizer = torch.optim.Adam(params, lr=lr, betas=(0.9, 0.999))\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    log_softmax_fn = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    loss_hist = []\n",
    "    best_accuracy = 0\n",
    "    best_params = params\n",
    "\n",
    "    for e in range(nb_epochs):\n",
    "        local_loss = []\n",
    "        local_ground_loss = []\n",
    "        local_reg_loss = []\n",
    "        accs = []\n",
    "#         print(thresholds_1)\n",
    "#         print(beta_hetero_2)\n",
    "        for x_local, y_local in sparse_data_generator_from_hdf5_spikes(x_data, y_data, batch_size_hetero, nb_steps, nb_inputs, max_time):\n",
    "            output, recs = run_snn_hetero(x_local.to_dense())\n",
    "            _, spks = recs\n",
    "            m, _ = torch.max(output, 1)\n",
    "\n",
    "            _, am = torch.max(m, 1)  # argmax over output units\n",
    "            tmp = np.mean((y_local == am).detach().cpu().numpy())  # compare to labels\n",
    "            accs.append(tmp)\n",
    "\n",
    "            log_p_y = nn.LogSoftmax(dim=1)(m)\n",
    "            ground_loss = loss_fn(log_p_y, y_local)\n",
    "\n",
    "            reg_loss = 1e-6 * torch.sum(spks)  # L1 loss on total number of spikes\n",
    "            reg_loss += 1e-6 * torch.mean(torch.sum(torch.sum(spks, dim=0), dim=0) ** 2)  # L2 loss on spikes per neuron\n",
    "\n",
    "            loss_val = ground_loss + reg_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "            # Clamping the values\n",
    "            with torch.no_grad():\n",
    "                alpha_hetero_1.clamp_(0.5, 0.995)\n",
    "                beta_hetero_1.clamp_(0.5, 0.995)\n",
    "                alpha_hetero_2.clamp_(0.5, 0.995)\n",
    "                beta_hetero_2.clamp_(0.5, 0.995)\n",
    "                thresholds_1.clamp_(0.5, 1.5)\n",
    "                \n",
    "            local_loss.append(loss_val.item())\n",
    "\n",
    "        mean_loss = np.mean(local_loss)\n",
    "        loss_hist.append(mean_loss)\n",
    "        print(f\"Epoch {e + 1}: loss={mean_loss:.5f}\")\n",
    "\n",
    "        current_accuracy = compute_classification_accuracy_hetero(x_test, y_test)\n",
    "        print(f\"Epoch {e + 1}: Train= {np.mean(accs):.5f} Test Accuracy={current_accuracy:.5f}\")\n",
    "\n",
    "        saved_params_hetero = {\n",
    "            'w1': w1.clone(),\n",
    "            'w2': w2.clone(),\n",
    "            'v1': v1.clone(),\n",
    "            'alpha': alpha_hetero_1.clone(),\n",
    "            'beta': beta_hetero_1.clone(),\n",
    "            'threshold': thresholds_1.clone(),\n",
    "            'reset': reset_1.clone(),\n",
    "            'rest': rest_1.clone(),\n",
    "            'alpha_2': alpha_hetero_2.clone(),\n",
    "            'beta_2': beta_hetero_2.clone()\n",
    "        }\n",
    "\n",
    "        # Save parameters along with the current epoch and accuracy\n",
    "        directory = f'hetero_group/epochs_{group_size}'\n",
    "\n",
    "        # Create the directory if it does not exist\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "        # Save the file in the specified directory\n",
    "        file_path = os.path.join(directory, f'snn_{e + 1}.pth')\n",
    "        torch.save({\n",
    "            'epoch': e + 1,\n",
    "            'accuracy': current_accuracy,\n",
    "            'params': saved_params_hetero,\n",
    "            'loss': loss_hist\n",
    "        }, file_path)\n",
    "\n",
    "        # Print the best accuracy so far\n",
    "        if current_accuracy > best_accuracy:\n",
    "            best_accuracy = current_accuracy\n",
    "            print(f\"Epoch {e + 1}: Best Test Accuracy={best_accuracy:.5f}\")\n",
    "\n",
    "            directory = f'hetero_group/best_{group_size}'\n",
    "\n",
    "            # Create the directory if it does not exist\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "\n",
    "            # Save parameters only when a new best accuracy is achieved\n",
    "            # Create a dictionary of current parameters to save\n",
    "            saved_params_hetero = {\n",
    "                'w1': w1.clone(),\n",
    "                'w2': w2.clone(),\n",
    "                'v1': v1.clone(),\n",
    "                'alpha': alpha_hetero_1.clone(),\n",
    "                'beta': beta_hetero_1.clone(),\n",
    "                'threshold': thresholds_1.clone(),\n",
    "                'reset': reset_1.clone(),\n",
    "                'rest': rest_1.clone(),\n",
    "                'alpha_2': alpha_hetero_2.clone(),\n",
    "                'beta_2': beta_hetero_2.clone()\n",
    "            }\n",
    "\n",
    "            # Save the file in the specified directory\n",
    "            file_path = os.path.join(directory, 'best_snn.pth')\n",
    "            torch.save({\n",
    "                'epoch': e + 1,\n",
    "                'accuracy': best_accuracy,\n",
    "                'params': saved_params_hetero,\n",
    "                'loss': loss_hist\n",
    "            }, file_path)\n",
    "        else:\n",
    "            print('Best', best_accuracy)\n",
    "\n",
    "    return loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8771, 0.9271]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_scale = 0.2\n",
    "\n",
    "w1 = torch.empty((nb_inputs, nb_hidden),  device=device, dtype=dtype, requires_grad=True)\n",
    "torch.nn.init.normal_(w1, mean=0.0, std=weight_scale/np.sqrt(nb_inputs))\n",
    "\n",
    "w2 = torch.empty((nb_hidden, nb_outputs), device=device, dtype=dtype, requires_grad=True)\n",
    "torch.nn.init.normal_(w2, mean=0.0, std=weight_scale/np.sqrt(nb_hidden))\n",
    "\n",
    "v1 = torch.empty((nb_hidden, nb_hidden), device=device, dtype=dtype, requires_grad=True)\n",
    "torch.nn.init.normal_(v1, mean=0.0, std=weight_scale/np.sqrt(nb_hidden))\n",
    "\n",
    "group_size = 10\n",
    "\n",
    "tau_syn = 10e-3\n",
    "tau_mem = 20e-3\n",
    "distribution = dist_fn('gamma')\n",
    "\n",
    "# Initialize parameters with sizes divided by group size\n",
    "thresholds_1 = torch.empty((1, nb_hidden // group_size), device=device, dtype=dtype, requires_grad=True)\n",
    "torch.nn.init.uniform_(thresholds_1, a=0.5, b=1.5)\n",
    "\n",
    "reset_1 = torch.empty((1, nb_hidden // group_size), device=device, dtype=dtype, requires_grad=True)\n",
    "torch.nn.init.uniform_(reset_1, a=-0.5, b=0.5)\n",
    "\n",
    "rest_1 = torch.empty((1, nb_hidden // group_size), device=device, dtype=dtype, requires_grad=True)\n",
    "torch.nn.init.uniform_(rest_1, a=-0.5, b=0.5)\n",
    "\n",
    "alpha_hetero_1_dist = torch.tensor(distribution(tau_syn, 3, (1, nb_hidden // group_size)), device=device, dtype=dtype)\n",
    "alpha_hetero_1 = torch.exp(-time_step / alpha_hetero_1_dist)\n",
    "alpha_hetero_1.requires_grad_(True)\n",
    "\n",
    "beta_hetero_1_dist = torch.tensor(distribution(tau_mem, 3, (1, nb_hidden // group_size)), device=device, dtype=dtype)\n",
    "beta_hetero_1 = torch.exp(-time_step / beta_hetero_1_dist)\n",
    "beta_hetero_1.requires_grad_(True)\n",
    "\n",
    "alpha_hetero_2_dist = torch.tensor(distribution(tau_syn, 3, (1, nb_outputs // group_size)), device=device, dtype=dtype)\n",
    "alpha_hetero_2 = torch.exp(-time_step / alpha_hetero_2_dist)\n",
    "alpha_hetero_2.requires_grad_(True)\n",
    "\n",
    "beta_hetero_2_dist = torch.tensor(distribution(tau_mem, 3, (1, nb_outputs // group_size)), device=device, dtype=dtype)\n",
    "beta_hetero_2 = torch.exp(-time_step / beta_hetero_2_dist)\n",
    "beta_hetero_2.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n"
     ]
    }
   ],
   "source": [
    "print(thresholds_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/pbs.9514944.pbs/ipykernel_1094113/2872161497.py:68: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  labels_ = np.array(y,dtype=int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=2.91877\n",
      "Epoch 1: Train= 0.12094 Test Accuracy=0.22232\n",
      "Epoch 1: Best Test Accuracy=0.22232\n",
      "Epoch 2: loss=2.13737\n",
      "Epoch 2: Train= 0.39333 Test Accuracy=0.46563\n",
      "Epoch 2: Best Test Accuracy=0.46563\n",
      "Epoch 3: loss=1.40638\n",
      "Epoch 3: Train= 0.62217 Test Accuracy=0.58348\n",
      "Epoch 3: Best Test Accuracy=0.58348\n",
      "Epoch 4: loss=1.08133\n",
      "Epoch 4: Train= 0.73155 Test Accuracy=0.64018\n",
      "Epoch 4: Best Test Accuracy=0.64018\n",
      "Epoch 5: loss=0.89405\n",
      "Epoch 5: Train= 0.78752 Test Accuracy=0.63482\n",
      "Best 0.6401785714285714\n",
      "Epoch 6: loss=0.65510\n",
      "Epoch 6: Train= 0.84252 Test Accuracy=0.71518\n",
      "Epoch 6: Best Test Accuracy=0.71518\n",
      "Epoch 7: loss=0.52789\n",
      "Epoch 7: Train= 0.88398 Test Accuracy=0.69554\n",
      "Best 0.7151785714285714\n",
      "Epoch 8: loss=0.45395\n",
      "Epoch 8: Train= 0.90650 Test Accuracy=0.71116\n",
      "Best 0.7151785714285714\n",
      "Epoch 9: loss=0.39933\n",
      "Epoch 9: Train= 0.92015 Test Accuracy=0.70759\n",
      "Best 0.7151785714285714\n",
      "Epoch 10: loss=0.35738\n",
      "Epoch 10: Train= 0.93393 Test Accuracy=0.72232\n",
      "Epoch 10: Best Test Accuracy=0.72232\n",
      "Epoch 11: loss=0.31961\n",
      "Epoch 11: Train= 0.94094 Test Accuracy=0.73839\n",
      "Epoch 11: Best Test Accuracy=0.73839\n",
      "Epoch 12: loss=0.29069\n",
      "Epoch 12: Train= 0.95140 Test Accuracy=0.70045\n",
      "Best 0.7383928571428572\n",
      "Epoch 13: loss=0.27988\n",
      "Epoch 13: Train= 0.95116 Test Accuracy=0.74018\n",
      "Epoch 13: Best Test Accuracy=0.74018\n",
      "Epoch 14: loss=0.26243\n",
      "Epoch 14: Train= 0.95472 Test Accuracy=0.75714\n",
      "Epoch 14: Best Test Accuracy=0.75714\n",
      "Epoch 15: loss=0.24141\n",
      "Epoch 15: Train= 0.95940 Test Accuracy=0.74554\n",
      "Best 0.7571428571428571\n",
      "Epoch 16: loss=0.22107\n",
      "Epoch 16: Train= 0.96506 Test Accuracy=0.71875\n",
      "Best 0.7571428571428571\n",
      "Epoch 17: loss=0.22519\n",
      "Epoch 17: Train= 0.96371 Test Accuracy=0.73839\n",
      "Best 0.7571428571428571\n",
      "Epoch 18: loss=0.22333\n",
      "Epoch 18: Train= 0.96235 Test Accuracy=0.73527\n",
      "Best 0.7571428571428571\n",
      "Epoch 19: loss=0.18767\n",
      "Epoch 19: Train= 0.97613 Test Accuracy=0.75536\n",
      "Best 0.7571428571428571\n",
      "Epoch 20: loss=0.17259\n",
      "Epoch 20: Train= 0.97847 Test Accuracy=0.75491\n",
      "Best 0.7571428571428571\n",
      "Epoch 21: loss=0.16124\n",
      "Epoch 21: Train= 0.98130 Test Accuracy=0.75804\n",
      "Epoch 21: Best Test Accuracy=0.75804\n",
      "Epoch 22: loss=0.15795\n",
      "Epoch 22: Train= 0.98044 Test Accuracy=0.75804\n",
      "Best 0.7580357142857143\n",
      "Epoch 23: loss=0.15466\n",
      "Epoch 23: Train= 0.98142 Test Accuracy=0.73214\n",
      "Best 0.7580357142857143\n",
      "Epoch 24: loss=0.16215\n",
      "Epoch 24: Train= 0.97884 Test Accuracy=0.72232\n",
      "Best 0.7580357142857143\n",
      "Epoch 25: loss=0.12617\n",
      "Epoch 25: Train= 0.98831 Test Accuracy=0.74330\n",
      "Best 0.7580357142857143\n",
      "Epoch 26: loss=0.13049\n",
      "Epoch 26: Train= 0.98684 Test Accuracy=0.73393\n",
      "Best 0.7580357142857143\n",
      "Epoch 27: loss=0.12995\n",
      "Epoch 27: Train= 0.98597 Test Accuracy=0.72991\n",
      "Best 0.7580357142857143\n",
      "Epoch 28: loss=0.12703\n",
      "Epoch 28: Train= 0.98819 Test Accuracy=0.73750\n",
      "Best 0.7580357142857143\n",
      "Epoch 29: loss=0.12045\n",
      "Epoch 29: Train= 0.98745 Test Accuracy=0.69152\n",
      "Best 0.7580357142857143\n",
      "Epoch 30: loss=0.14830\n",
      "Epoch 30: Train= 0.97859 Test Accuracy=0.71830\n",
      "Best 0.7580357142857143\n",
      "Epoch 31: loss=0.12766\n",
      "Epoch 31: Train= 0.98585 Test Accuracy=0.73438\n",
      "Best 0.7580357142857143\n",
      "Epoch 32: loss=0.10451\n",
      "Epoch 32: Train= 0.99200 Test Accuracy=0.73259\n",
      "Best 0.7580357142857143\n",
      "Epoch 33: loss=0.12843\n",
      "Epoch 33: Train= 0.98388 Test Accuracy=0.74241\n",
      "Best 0.7580357142857143\n",
      "Epoch 34: loss=0.09041\n",
      "Epoch 34: Train= 0.99557 Test Accuracy=0.73348\n",
      "Best 0.7580357142857143\n",
      "Epoch 35: loss=0.09017\n",
      "Epoch 35: Train= 0.99471 Test Accuracy=0.73080\n",
      "Best 0.7580357142857143\n",
      "Epoch 36: loss=0.10393\n",
      "Epoch 36: Train= 0.99053 Test Accuracy=0.72411\n",
      "Best 0.7580357142857143\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m nb_epochs_snn_hetero \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m150\u001b[39m\n\u001b[1;32m      3\u001b[0m batch_size_hetero \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[0;32m----> 4\u001b[0m loss_hist_snn_hetero \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_snn_hetero\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnb_epochs_snn_hetero\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[92], line 21\u001b[0m, in \u001b[0;36mtrain_snn_hetero\u001b[0;34m(x_data, y_data, lr, nb_epochs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#         print(thresholds_1)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#         print(beta_hetero_2)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x_local, y_local \u001b[38;5;129;01min\u001b[39;00m sparse_data_generator_from_hdf5_spikes(x_data, y_data, batch_size_hetero, nb_steps, nb_inputs, max_time):\n\u001b[0;32m---> 21\u001b[0m             output, recs \u001b[38;5;241m=\u001b[39m \u001b[43mrun_snn_hetero\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_local\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dense\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m             _, spks \u001b[38;5;241m=\u001b[39m recs\n\u001b[1;32m     23\u001b[0m             m, _ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(output, \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[83], line 26\u001b[0m, in \u001b[0;36mrun_snn_hetero\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nb_steps):\n\u001b[1;32m     25\u001b[0m     h1 \u001b[38;5;241m=\u001b[39m h1_from_input[:, t] \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mab,bc->ac\u001b[39m\u001b[38;5;124m\"\u001b[39m, (out, v1))\n\u001b[0;32m---> 26\u001b[0m     mthr \u001b[38;5;241m=\u001b[39m \u001b[43mmem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mthresholds_local\u001b[49m\n\u001b[1;32m     27\u001b[0m     out \u001b[38;5;241m=\u001b[39m spike_fn(mthr)\n\u001b[1;32m     28\u001b[0m     rst \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(mem)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# group size 20\n",
    "nb_epochs_snn_hetero = 150\n",
    "batch_size_hetero = 64\n",
    "loss_hist_snn_hetero = train_snn_hetero(x_train, y_train, lr=2e-4, nb_epochs=nb_epochs_snn_hetero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/pbs.9515603.pbs/ipykernel_2139618/2872161497.py:68: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  labels_ = np.array(y,dtype=int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=2.87604\n",
      "Epoch 1: Train= 0.12623 Test Accuracy=0.25268\n",
      "Epoch 1: Best Test Accuracy=0.25268\n",
      "Epoch 2: loss=2.32391\n",
      "Epoch 2: Train= 0.32074 Test Accuracy=0.48482\n",
      "Epoch 2: Best Test Accuracy=0.48482\n",
      "Epoch 3: loss=1.61333\n",
      "Epoch 3: Train= 0.56398 Test Accuracy=0.58080\n",
      "Epoch 3: Best Test Accuracy=0.58080\n",
      "Epoch 4: loss=1.17592\n",
      "Epoch 4: Train= 0.67938 Test Accuracy=0.62813\n",
      "Epoch 4: Best Test Accuracy=0.62813\n",
      "Epoch 5: loss=0.93247\n",
      "Epoch 5: Train= 0.75283 Test Accuracy=0.67277\n",
      "Epoch 5: Best Test Accuracy=0.67277\n",
      "Epoch 6: loss=0.75331\n",
      "Epoch 6: Train= 0.80536 Test Accuracy=0.71161\n",
      "Epoch 6: Best Test Accuracy=0.71161\n",
      "Epoch 7: loss=0.63463\n",
      "Epoch 7: Train= 0.84732 Test Accuracy=0.72143\n",
      "Epoch 7: Best Test Accuracy=0.72143\n",
      "Epoch 8: loss=0.57061\n",
      "Epoch 8: Train= 0.86270 Test Accuracy=0.68884\n",
      "Best 0.7214285714285714\n",
      "Epoch 9: loss=0.51218\n",
      "Epoch 9: Train= 0.87426 Test Accuracy=0.73661\n",
      "Epoch 9: Best Test Accuracy=0.73661\n",
      "Epoch 10: loss=0.45911\n",
      "Epoch 10: Train= 0.89087 Test Accuracy=0.73973\n",
      "Epoch 10: Best Test Accuracy=0.73973\n",
      "Epoch 11: loss=0.42254\n",
      "Epoch 11: Train= 0.89985 Test Accuracy=0.74286\n",
      "Epoch 11: Best Test Accuracy=0.74286\n",
      "Epoch 12: loss=0.38888\n",
      "Epoch 12: Train= 0.91216 Test Accuracy=0.75446\n",
      "Epoch 12: Best Test Accuracy=0.75446\n",
      "Epoch 13: loss=0.36261\n",
      "Epoch 13: Train= 0.92347 Test Accuracy=0.72679\n",
      "Best 0.7544642857142857\n",
      "Epoch 14: loss=0.34803\n",
      "Epoch 14: Train= 0.92852 Test Accuracy=0.72679\n",
      "Best 0.7544642857142857\n",
      "Epoch 15: loss=0.33865\n",
      "Epoch 15: Train= 0.93135 Test Accuracy=0.72232\n",
      "Best 0.7544642857142857\n",
      "Epoch 16: loss=0.30722\n",
      "Epoch 16: Train= 0.93910 Test Accuracy=0.72946\n",
      "Best 0.7544642857142857\n",
      "Epoch 17: loss=0.30138\n",
      "Epoch 17: Train= 0.94021 Test Accuracy=0.71920\n",
      "Best 0.7544642857142857\n",
      "Epoch 18: loss=0.26911\n",
      "Epoch 18: Train= 0.95288 Test Accuracy=0.73125\n",
      "Best 0.7544642857142857\n",
      "Epoch 19: loss=0.24919\n",
      "Epoch 19: Train= 0.95669 Test Accuracy=0.73884\n",
      "Best 0.7544642857142857\n",
      "Epoch 20: loss=0.24872\n",
      "Epoch 20: Train= 0.95251 Test Accuracy=0.74330\n",
      "Best 0.7544642857142857\n",
      "Epoch 21: loss=0.22441\n",
      "Epoch 21: Train= 0.96444 Test Accuracy=0.73616\n",
      "Best 0.7544642857142857\n",
      "Epoch 22: loss=0.21833\n",
      "Epoch 22: Train= 0.96469 Test Accuracy=0.75893\n",
      "Epoch 22: Best Test Accuracy=0.75893\n",
      "Epoch 23: loss=0.20558\n",
      "Epoch 23: Train= 0.96764 Test Accuracy=0.74241\n",
      "Best 0.7589285714285714\n",
      "Epoch 24: loss=0.21514\n",
      "Epoch 24: Train= 0.96420 Test Accuracy=0.75134\n",
      "Best 0.7589285714285714\n",
      "Epoch 25: loss=0.19893\n",
      "Epoch 25: Train= 0.97121 Test Accuracy=0.73036\n",
      "Best 0.7589285714285714\n",
      "Epoch 26: loss=0.18352\n",
      "Epoch 26: Train= 0.97343 Test Accuracy=0.72545\n",
      "Best 0.7589285714285714\n",
      "Epoch 27: loss=0.17974\n",
      "Epoch 27: Train= 0.97527 Test Accuracy=0.71429\n",
      "Best 0.7589285714285714\n",
      "Epoch 28: loss=0.17109\n",
      "Epoch 28: Train= 0.97662 Test Accuracy=0.75491\n",
      "Best 0.7589285714285714\n",
      "Epoch 29: loss=0.17756\n",
      "Epoch 29: Train= 0.97281 Test Accuracy=0.73839\n",
      "Best 0.7589285714285714\n",
      "Epoch 30: loss=0.16281\n",
      "Epoch 30: Train= 0.97896 Test Accuracy=0.72500\n",
      "Best 0.7589285714285714\n",
      "Epoch 31: loss=0.16945\n",
      "Epoch 31: Train= 0.97527 Test Accuracy=0.75134\n",
      "Best 0.7589285714285714\n",
      "Epoch 32: loss=0.14733\n",
      "Epoch 32: Train= 0.98241 Test Accuracy=0.73750\n",
      "Best 0.7589285714285714\n",
      "Epoch 33: loss=0.13815\n",
      "Epoch 33: Train= 0.98634 Test Accuracy=0.72545\n",
      "Best 0.7589285714285714\n",
      "Epoch 34: loss=0.12547\n",
      "Epoch 34: Train= 0.99053 Test Accuracy=0.75313\n",
      "Best 0.7589285714285714\n",
      "Epoch 35: loss=0.13518\n",
      "Epoch 35: Train= 0.98548 Test Accuracy=0.72143\n",
      "Best 0.7589285714285714\n",
      "Epoch 36: loss=0.12167\n",
      "Epoch 36: Train= 0.98868 Test Accuracy=0.72813\n",
      "Best 0.7589285714285714\n",
      "Epoch 37: loss=0.13521\n",
      "Epoch 37: Train= 0.98499 Test Accuracy=0.72277\n",
      "Best 0.7589285714285714\n",
      "Epoch 38: loss=0.13618\n",
      "Epoch 38: Train= 0.98253 Test Accuracy=0.74554\n",
      "Best 0.7589285714285714\n",
      "Epoch 39: loss=0.11004\n",
      "Epoch 39: Train= 0.99225 Test Accuracy=0.72232\n",
      "Best 0.7589285714285714\n",
      "Epoch 40: loss=0.11070\n",
      "Epoch 40: Train= 0.98954 Test Accuracy=0.70759\n",
      "Best 0.7589285714285714\n",
      "Epoch 41: loss=0.12045\n",
      "Epoch 41: Train= 0.98659 Test Accuracy=0.72188\n",
      "Best 0.7589285714285714\n",
      "Epoch 42: loss=0.10508\n",
      "Epoch 42: Train= 0.99077 Test Accuracy=0.74241\n",
      "Best 0.7589285714285714\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[134], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m nb_epochs_snn_hetero \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m150\u001b[39m\n\u001b[1;32m      3\u001b[0m batch_size_hetero \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[0;32m----> 4\u001b[0m loss_hist_snn_hetero \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_snn_hetero\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnb_epochs_snn_hetero\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[130], line 38\u001b[0m, in \u001b[0;36mtrain_snn_hetero\u001b[0;34m(x_data, y_data, lr, nb_epochs)\u001b[0m\n\u001b[1;32m     35\u001b[0m loss_val \u001b[38;5;241m=\u001b[39m ground_loss \u001b[38;5;241m+\u001b[39m reg_loss\n\u001b[1;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 38\u001b[0m \u001b[43mloss_val\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Clamping the values\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/test1/lib/python3.9/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/test1/lib/python3.9/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/test1/lib/python3.9/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# group size 10\n",
    "nb_epochs_snn_hetero = 150\n",
    "batch_size_hetero = 64\n",
    "loss_hist_snn_hetero = train_snn_hetero(x_train, y_train, lr=2e-4, nb_epochs=nb_epochs_snn_hetero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONT RUN NEXT ONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/pbs.9514944.pbs/ipykernel_1094113/2872161497.py:68: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  labels_ = np.array(y,dtype=int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=2.94189\n",
      "Epoch 1: Train= 0.09560 Test Accuracy=0.20714\n",
      "Epoch 1: Best Test Accuracy=0.20714\n",
      "Epoch 2: loss=2.50568\n",
      "Epoch 2: Train= 0.27153 Test Accuracy=0.37545\n",
      "Epoch 2: Best Test Accuracy=0.37545\n",
      "Epoch 3: loss=2.00341\n",
      "Epoch 3: Train= 0.41941 Test Accuracy=0.52054\n",
      "Epoch 3: Best Test Accuracy=0.52054\n",
      "Epoch 4: loss=1.55043\n",
      "Epoch 4: Train= 0.55217 Test Accuracy=0.55357\n",
      "Epoch 4: Best Test Accuracy=0.55357\n",
      "Epoch 5: loss=1.17684\n",
      "Epoch 5: Train= 0.66302 Test Accuracy=0.63393\n",
      "Epoch 5: Best Test Accuracy=0.63393\n",
      "Epoch 6: loss=0.93456\n",
      "Epoch 6: Train= 0.74975 Test Accuracy=0.65045\n",
      "Epoch 6: Best Test Accuracy=0.65045\n",
      "Epoch 7: loss=0.78834\n",
      "Epoch 7: Train= 0.79860 Test Accuracy=0.68973\n",
      "Epoch 7: Best Test Accuracy=0.68973\n",
      "Epoch 8: loss=0.67215\n",
      "Epoch 8: Train= 0.83022 Test Accuracy=0.70045\n",
      "Epoch 8: Best Test Accuracy=0.70045\n",
      "Epoch 9: loss=0.59558\n",
      "Epoch 9: Train= 0.85273 Test Accuracy=0.73527\n",
      "Epoch 9: Best Test Accuracy=0.73527\n",
      "Epoch 10: loss=0.52889\n",
      "Epoch 10: Train= 0.87180 Test Accuracy=0.73482\n",
      "Best 0.7352678571428571\n",
      "Epoch 11: loss=0.47758\n",
      "Epoch 11: Train= 0.88989 Test Accuracy=0.76518\n",
      "Epoch 11: Best Test Accuracy=0.76518\n",
      "Epoch 12: loss=0.42825\n",
      "Epoch 12: Train= 0.90625 Test Accuracy=0.75446\n",
      "Best 0.7651785714285714\n",
      "Epoch 13: loss=0.39151\n",
      "Epoch 13: Train= 0.91449 Test Accuracy=0.75223\n",
      "Best 0.7651785714285714\n",
      "Epoch 14: loss=0.36648\n",
      "Epoch 14: Train= 0.92212 Test Accuracy=0.73571\n",
      "Best 0.7651785714285714\n",
      "Epoch 15: loss=0.34156\n",
      "Epoch 15: Train= 0.93012 Test Accuracy=0.76786\n",
      "Epoch 15: Best Test Accuracy=0.76786\n",
      "Epoch 16: loss=0.32141\n",
      "Epoch 16: Train= 0.93836 Test Accuracy=0.75179\n",
      "Best 0.7678571428571429\n",
      "Epoch 17: loss=0.30162\n",
      "Epoch 17: Train= 0.94070 Test Accuracy=0.73929\n",
      "Best 0.7678571428571429\n",
      "Epoch 18: loss=0.28165\n",
      "Epoch 18: Train= 0.94931 Test Accuracy=0.74732\n",
      "Best 0.7678571428571429\n",
      "Epoch 19: loss=0.28037\n",
      "Epoch 19: Train= 0.94747 Test Accuracy=0.76473\n",
      "Best 0.7678571428571429\n",
      "Epoch 20: loss=0.25291\n",
      "Epoch 20: Train= 0.95645 Test Accuracy=0.76741\n",
      "Best 0.7678571428571429\n",
      "Epoch 21: loss=0.23974\n",
      "Epoch 21: Train= 0.96223 Test Accuracy=0.74911\n",
      "Best 0.7678571428571429\n",
      "Epoch 22: loss=0.23238\n",
      "Epoch 22: Train= 0.96272 Test Accuracy=0.76518\n",
      "Best 0.7678571428571429\n",
      "Epoch 23: loss=0.22209\n",
      "Epoch 23: Train= 0.96457 Test Accuracy=0.73348\n",
      "Best 0.7678571428571429\n",
      "Epoch 24: loss=0.21413\n",
      "Epoch 24: Train= 0.96764 Test Accuracy=0.74241\n",
      "Best 0.7678571428571429\n",
      "Epoch 25: loss=0.20402\n",
      "Epoch 25: Train= 0.96924 Test Accuracy=0.74420\n",
      "Best 0.7678571428571429\n",
      "Epoch 26: loss=0.19141\n",
      "Epoch 26: Train= 0.97293 Test Accuracy=0.75045\n",
      "Best 0.7678571428571429\n",
      "Epoch 27: loss=0.17270\n",
      "Epoch 27: Train= 0.97933 Test Accuracy=0.74241\n",
      "Best 0.7678571428571429\n",
      "Epoch 28: loss=0.16997\n",
      "Epoch 28: Train= 0.98019 Test Accuracy=0.74554\n",
      "Best 0.7678571428571429\n",
      "Epoch 29: loss=0.15937\n",
      "Epoch 29: Train= 0.97945 Test Accuracy=0.74062\n",
      "Best 0.7678571428571429\n",
      "Epoch 30: loss=0.16420\n",
      "Epoch 30: Train= 0.98019 Test Accuracy=0.74286\n",
      "Best 0.7678571428571429\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m nb_epochs_snn_hetero \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m150\u001b[39m\n\u001b[1;32m      2\u001b[0m batch_size_hetero \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[0;32m----> 3\u001b[0m loss_hist_snn_hetero \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_snn_hetero\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnb_epochs_snn_hetero\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[88], line 20\u001b[0m, in \u001b[0;36mtrain_snn_hetero\u001b[0;34m(x_data, y_data, lr, nb_epochs)\u001b[0m\n\u001b[1;32m     17\u001b[0m         accs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#         print(thresholds_1)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#         print(beta_hetero_2)\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x_local, y_local \u001b[38;5;129;01min\u001b[39;00m sparse_data_generator_from_hdf5_spikes(x_data, y_data, batch_size_hetero, nb_steps, nb_inputs, max_time):\n\u001b[1;32m     21\u001b[0m             output, recs \u001b[38;5;241m=\u001b[39m run_snn_hetero(x_local\u001b[38;5;241m.\u001b[39mto_dense())\n\u001b[1;32m     22\u001b[0m             _, spks \u001b[38;5;241m=\u001b[39m recs\n",
      "Cell \u001b[0;32mIn[2], line 87\u001b[0m, in \u001b[0;36msparse_data_generator_from_hdf5_spikes\u001b[0;34m(X, y, batch_size, nb_steps, nb_units, max_time, shuffle)\u001b[0m\n\u001b[1;32m     84\u001b[0m batch_index \u001b[38;5;241m=\u001b[39m sample_index[batch_size\u001b[38;5;241m*\u001b[39mcounter:batch_size\u001b[38;5;241m*\u001b[39m(counter\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m     86\u001b[0m coo \u001b[38;5;241m=\u001b[39m [ [] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m) ]\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bc,idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m(batch_index):\n\u001b[1;32m     88\u001b[0m     times \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdigitize(firing_times[idx], time_bins)\n\u001b[1;32m     89\u001b[0m     units \u001b[38;5;241m=\u001b[39m units_fired[idx]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# group size 5\n",
    "nb_epochs_snn_hetero = 150\n",
    "batch_size_hetero = 64\n",
    "loss_hist_snn_hetero = train_snn_hetero(x_train, y_train, lr=2e-4, nb_epochs=nb_epochs_snn_hetero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hetero_mlp_a_b_spikes(nn.Module):\n",
    "    def __init__(self, group_size=1):\n",
    "        super(hetero_mlp_a_b_spikes, self).__init__()\n",
    "        self.input_size = 1961 #(adding 40 for alpha and beta 2)\n",
    "        self.hidden_size = 2048\n",
    "        self.group_size = group_size\n",
    "        self.output_size = (200 // group_size) * 5 + (20 // group_size) * 2\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(self.input_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, self.output_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    def init_weights(self):\n",
    "        with torch.no_grad():\n",
    "            # Initialize first layer weights and biases\n",
    "            self.layers[0].weight.fill_(0)\n",
    "            self.layers[0].bias.fill_(0)\n",
    "\n",
    "            # Pass the first 1000 inputs directly to the hidden layer; the rest to 0\n",
    "            for i in range(1040):\n",
    "                self.layers[0].weight[i, i] = 1\n",
    "\n",
    "            # Initialize second layer weights and biases\n",
    "            self.layers[2].weight.fill_(0)\n",
    "            self.layers[2].bias.fill_(0)\n",
    "\n",
    "            # Pass the first 1000 inputs directly to the hidden layer; the rest to 0\n",
    "            for i in range(1040):\n",
    "                self.layers[2].weight[i, i] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_snn_hybrid_alpha_beta_spikes_HETERO(inputs, mlp, mlp_interval, batch_size_MLP):\n",
    "    device, dtype = inputs.device, inputs.dtype\n",
    "\n",
    "    # Initialize local copies of alpha, beta, threshold, reset, and rest for all 200 hidden neurons\n",
    "    alpha_1_local = alpha_hetero_1.repeat_interleave(group_size, dim=1).expand(batch_size_MLP, 200).detach().clone()\n",
    "    beta_1_local = beta_hetero_1.repeat_interleave(group_size, dim=1).expand(batch_size_MLP, 200).detach().clone()\n",
    "    thresholds_local = thresholds_1.repeat_interleave(group_size, dim=1).expand(batch_size_MLP, 200).detach().clone()\n",
    "    reset_local = reset_1.repeat_interleave(group_size, dim=1).expand(batch_size_MLP, 200).detach().clone()\n",
    "    rest_local = rest_1.repeat_interleave(group_size, dim=1).expand(batch_size_MLP, 200).detach().clone()\n",
    "    alpha_2_local = alpha_hetero_2.repeat_interleave(group_size, dim=1).expand(batch_size_MLP, 20).detach().clone()\n",
    "    beta_2_local = beta_hetero_2.repeat_interleave(group_size, dim=1).expand(batch_size_MLP, 20).detach().clone()\n",
    "\n",
    "    # Initialize synaptic and membrane potentials\n",
    "    syn = torch.zeros((batch_size_MLP, nb_hidden), device=device, dtype=dtype)\n",
    "    mem = torch.zeros((batch_size_MLP, nb_hidden), device=device, dtype=dtype)\n",
    "\n",
    "    # Initialize recordings for membrane potentials and spikes\n",
    "    mem_rec = []\n",
    "    spk_rec = []\n",
    "\n",
    "    # Initialize outputs for the hidden layer\n",
    "    out = torch.zeros((batch_size_MLP, nb_hidden), device=device, dtype=dtype)\n",
    "    h1_from_input = torch.einsum(\"abc,cd->abd\", (inputs, w1))\n",
    "\n",
    "    # Prepare readout layer variables\n",
    "    flt2 = torch.zeros((batch_size_MLP, nb_outputs), device=device, dtype=dtype)\n",
    "    out2 = torch.zeros((batch_size_MLP, nb_outputs), device=device, dtype=dtype)\n",
    "    out_rec = [out2]\n",
    "\n",
    "    for t in range(nb_steps):\n",
    "        h1 = h1_from_input[:, t] + torch.einsum(\"ab,bc->ac\", (out, v1))\n",
    "        mthr = mem - thresholds_local\n",
    "        out = spike_fn(mthr)\n",
    "        rst = torch.zeros_like(mem)\n",
    "        c = (mthr > 0)\n",
    "        rst[c] = torch.ones_like(mem)[c]\n",
    "\n",
    "        # Update synaptic and membrane potentials\n",
    "        syn = alpha_1_local * syn + h1\n",
    "        mem = beta_1_local * (mem - rest_local) + rest_local + (1 - beta_1_local) * syn - rst * (thresholds_local - reset_local)\n",
    "\n",
    "        # Record membrane potentials and spikes\n",
    "        mem_rec.append(mem)\n",
    "        spk_rec.append(out)\n",
    "\n",
    "        # Now compute h2 on the fly\n",
    "        h2_t = torch.einsum(\"ab,bc->ac\", (out, w2))\n",
    "        flt2 = alpha_2_local * flt2 + h2_t\n",
    "        out2 = beta_2_local * out2 + flt2 * (1 - beta_2_local)\n",
    "        out_rec.append(out2)\n",
    "\n",
    "        # Flatten and concatenate spikes for each item in the batch\n",
    "        input_spikes_flat = inputs[:, t, :].reshape(batch_size_MLP, -1)  # Shape: [batch_size, 700]\n",
    "        hidden_spikes_flat = out.reshape(batch_size_MLP, -1)  # Shape: [batch_size, 200]\n",
    "        output_spikes_flat = out2.reshape(batch_size_MLP, -1)  # Shape: [batch_size, 20]\n",
    "\n",
    "        # Time tensor\n",
    "        time_tensor = torch.full((batch_size_MLP, 1), t, device=device, dtype=dtype)\n",
    "\n",
    "        # Concatenate tensors\n",
    "        mlp_input = torch.cat([\n",
    "            alpha_1_local, beta_1_local, thresholds_local, reset_local, rest_local,\n",
    "            alpha_2_local, beta_2_local,\n",
    "            time_tensor,\n",
    "            input_spikes_flat, hidden_spikes_flat, output_spikes_flat\n",
    "        ], dim=1)\n",
    "\n",
    "        # Process with MLP (in a single call for the whole batch)\n",
    "        mlp_outputs = mlp(mlp_input)\n",
    "\n",
    "        if t % mlp_interval == 0:\n",
    "            # Update alpha_local and beta_local based on MLP outputs\n",
    "            num_groups = 200 // group_size\n",
    "            for i in range(num_groups):\n",
    "                alpha_1_update = mlp_outputs[:, i].unsqueeze(1).expand(-1, group_size)\n",
    "                beta_1_update = mlp_outputs[:, num_groups + i].unsqueeze(1).expand(-1, group_size)\n",
    "                thresholds_update = mlp_outputs[:, 2*num_groups + i].unsqueeze(1).expand(-1, group_size) + 0.5\n",
    "                reset_update = mlp_outputs[:, 3*num_groups + i].unsqueeze(1).expand(-1, group_size) - 0.5\n",
    "                rest_update = mlp_outputs[:, 4*num_groups + i].unsqueeze(1).expand(-1, group_size) - 0.5\n",
    "                \n",
    "                alpha_1_local = alpha_1_local.clone()\n",
    "                beta_1_local = beta_1_local.clone()\n",
    "                thresholds_local = thresholds_local.clone()\n",
    "                reset_local = reset_local.clone()\n",
    "                rest_local = rest_local.clone()\n",
    "                \n",
    "                alpha_1_local[:, i*group_size:(i+1)*group_size] = alpha_1_update\n",
    "                beta_1_local[:, i*group_size:(i+1)*group_size] = beta_1_update\n",
    "                thresholds_local[:, i*group_size:(i+1)*group_size] = thresholds_update\n",
    "                reset_local[:, i*group_size:(i+1)*group_size] = reset_update\n",
    "                rest_local[:, i*group_size:(i+1)*group_size] = rest_update\n",
    "\n",
    "            num_groups2 = 20 // group_size\n",
    "            for i in range(num_groups2):\n",
    "                alpha_2_update = mlp_outputs[:, 5*num_groups + i].unsqueeze(1).expand(-1, group_size)\n",
    "                beta_2_update = mlp_outputs[:, 5*num_groups + num_groups2 + i].unsqueeze(1).expand(-1, group_size)\n",
    "                \n",
    "                alpha_2_local = alpha_2_local.clone()\n",
    "                beta_2_local = beta_2_local.clone()\n",
    "                \n",
    "                alpha_2_local[:, i*group_size:(i+1)*group_size] = alpha_2_update\n",
    "                beta_2_local[:, i*group_size:(i+1)*group_size] = beta_2_update\n",
    "\n",
    "    # Stack recordings for output\n",
    "    mem_rec = torch.stack(mem_rec, dim=1).to(device)\n",
    "    spk_rec = torch.stack(spk_rec, dim=1).to(device)\n",
    "    out_rec = torch.stack(out_rec[1:], dim=1).to(device)  # Skip the initial zero tensor\n",
    "\n",
    "    other_recs = [mem_rec, spk_rec]\n",
    "\n",
    "    return out_rec, other_recs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classification_accuracy_MLP(x_data, y_data, mlp, mlp_interval):\n",
    "    \"\"\" Computes classification accuracy on supplied data in batches. \"\"\"\n",
    "    accs = []\n",
    "    with torch.no_grad():\n",
    "        for x_local, y_local in sparse_data_generator_from_hdf5_spikes(x_data, y_data, batch_size_hetero, nb_steps, nb_inputs, max_time, shuffle=False):\n",
    "            output, _ = run_snn_hybrid_alpha_beta_spikes_HETERO(x_local.to_dense(),  mlp=mlp, mlp_interval=mlp_interval, batch_size_MLP=batch_size_hetero)\n",
    "            m,_= torch.max(output,1) # max over time\n",
    "            _,am=torch.max(m,1)      # argmax over output units\n",
    "            tmp = np.mean((y_local==am).detach().cpu().numpy()) # compare to labels\n",
    "            accs.append(tmp)\n",
    "    return np.mean(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hybrid(mlp, x_data, y_data, lr=1e-3, nb_epochs=10, mlp_interval=10):\n",
    "\n",
    "    snn_params = [w1, w2, v1, \n",
    "                  alpha_hetero_1, beta_hetero_1,\n",
    "                  thresholds_1, reset_1, rest_1,\n",
    "                  alpha_hetero_2, beta_hetero_2]\n",
    "#  \n",
    "\n",
    "    # Optimizers\n",
    "    combined_params = [\n",
    "        {'params': snn_params, 'lr': lr},  # Parameters for SNN with specific learning rate\n",
    "        {'params': mlp.parameters(), 'lr': lr}  # Parameters for MLP with its own learning rate\n",
    "    ]\n",
    "\n",
    "    # Using a single optimizer for both SNN and MLP\n",
    "    combined_optimizer = torch.optim.Adam(combined_params)\n",
    "\n",
    "\n",
    "    #Loss functions\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    log_softmax_fn = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    best_accuracy = 0\n",
    "\n",
    "    loss_hist = []\n",
    "    for epoch in range(nb_epochs):\n",
    "        local_loss = []\n",
    "        local_ground_loss = []\n",
    "        local_reg_loss = []\n",
    "        accs = []\n",
    "#         print(w1)\n",
    "#         print(alpha_hetero_1)\n",
    "#         print(reset_1)\n",
    "        for x_local, y_local in sparse_data_generator_from_hdf5_spikes(x_data, y_data, batch_size_hetero, nb_steps, nb_inputs, max_time):\n",
    "                output, recs = run_snn_hybrid_alpha_beta_spikes_HETERO(inputs=x_local.to_dense(), mlp=mlp, mlp_interval=mlp_interval, batch_size_MLP=batch_size_hetero)\n",
    "                _ , spks = recs\n",
    "                m, _ = torch.max(output,1)\n",
    "\n",
    "                _,am=torch.max(m,1)      # argmax over output units\n",
    "                tmp = np.mean((y_local==am).detach().cpu().numpy()) # compare to labels\n",
    "                accs.append(tmp)\n",
    "\n",
    "                log_p_y = log_softmax_fn(m)\n",
    "                ground_loss = loss_fn(log_p_y, y_local)\n",
    "                reg_loss = 1e-6*torch.sum(spks) # L1 loss on total number of spikes\n",
    "                reg_loss += 1e-6*torch.mean(torch.sum(torch.sum(spks,dim=0),dim=0)**2) # L2 loss on spikes per neuron\n",
    "\n",
    "                loss_MLP = ground_loss + reg_loss\n",
    "\n",
    "                combined_optimizer.zero_grad()\n",
    "                loss_MLP.backward()\n",
    "                combined_optimizer.step()\n",
    "                \n",
    "                # Clamping the values\n",
    "                with torch.no_grad():\n",
    "                    alpha_hetero_1.clamp_(0.367, 0.995)\n",
    "                    beta_hetero_1.clamp_(0.367, 0.995)\n",
    "                    alpha_hetero_2.clamp_(0.367, 0.995)\n",
    "                    beta_hetero_2.clamp_(0.367, 0.995)\n",
    "                    thresholds_1.clamp_(0.5, 1.5)\n",
    "\n",
    "                local_loss.append(loss_MLP.item())\n",
    "                local_ground_loss.append(ground_loss.item())\n",
    "                local_reg_loss.append(reg_loss.item())\n",
    "\n",
    "\n",
    "        mean_loss = np.mean(local_loss)\n",
    "        loss_hist.append(mean_loss)\n",
    "        print(f\"Epoch {epoch+1}: loss={mean_loss:.5f}\")\n",
    "        print(\"ground_loss\", np.mean(local_ground_loss))\n",
    "        print(\"reg_loss\", np.mean(local_reg_loss))\n",
    "        current_accuracy = compute_classification_accuracy_MLP(x_test, y_test, mlp, mlp_interval)\n",
    "        print(f\"Epoch {epoch+1}: Train= {np.mean(accs):.5f} Test Accuracy={current_accuracy:.5f}\")\n",
    "\n",
    "        # Print the best accuracy so far\n",
    "        if current_accuracy > best_accuracy:\n",
    "            best_accuracy = current_accuracy\n",
    "\n",
    "            directory = f'hetero_group/hybrid_{group_size}'\n",
    "\n",
    "            # Create the directory if it does not exist\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "\n",
    "            best_model_state = mlp_mlp.state_dict()\n",
    "\n",
    "            # Save the file in the specified directory\n",
    "            file_path = os.path.join(directory, 'mlp.pt')\n",
    "            torch.save(best_model_state, file_path)\n",
    "\n",
    "            # Save parameters only when a new best accuracy is achieved\n",
    "            # Create a dictionary of current parameters to save\n",
    "            saved_params_hetero = {\n",
    "                'w1': w1.clone(),\n",
    "                'w2': w2.clone(),\n",
    "                'v1': v1.clone(),\n",
    "                'alpha': alpha_hetero_1.clone(),\n",
    "                'beta': beta_hetero_1.clone(),\n",
    "                'threshold': thresholds_1.clone(),\n",
    "                'reset': reset_1.clone(),\n",
    "                'rest': rest_1.clone(),\n",
    "                'alpha_2': alpha_hetero_2.clone(),\n",
    "                'beta_2': beta_hetero_2.clone()\n",
    "            }\n",
    "\n",
    "            # Save the file in the specified directory\n",
    "            file_path = os.path.join(directory, 'snn.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'accuracy': best_accuracy,\n",
    "                'params': saved_params_hetero,\n",
    "                'loss': loss_hist\n",
    "            }, file_path)\n",
    "        else:\n",
    "            print(\"Best\", best_accuracy)\n",
    "\n",
    "    return loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7651785714285714\n"
     ]
    }
   ],
   "source": [
    "# loaded_weights_snn = torch.load('Hetero/3_Hetero/epochs_2048/snn_9.pth')\n",
    "loaded_weights_snn = torch.load('hetero_group/epochs_5/snn_11.pth')\n",
    "# loaded_weights_snn = torch.load('hybrid_hetero/snn_2048.pth')\n",
    "# Convert tensors to parameters and ensure they are leaf tensors by re-wrapping them\n",
    "# Move tensors to device first and then wrap them as parameters\n",
    "w1 = torch.nn.Parameter(loaded_weights_snn['params']['w1'].to(device))\n",
    "w2 = torch.nn.Parameter(loaded_weights_snn['params']['w2'].to(device))\n",
    "v1 = torch.nn.Parameter(loaded_weights_snn['params']['v1'].to(device))\n",
    "alpha_hetero_1 = torch.nn.Parameter(loaded_weights_snn['params']['alpha'].to(device))\n",
    "beta_hetero_1 = torch.nn.Parameter(loaded_weights_snn['params']['beta'].to(device))\n",
    "thresholds_1 = torch.nn.Parameter(loaded_weights_snn['params']['threshold'].to(device))\n",
    "reset_1 = torch.nn.Parameter(loaded_weights_snn['params']['reset'].to(device))\n",
    "rest_1 = torch.nn.Parameter(loaded_weights_snn['params']['rest'].to(device))\n",
    "alpha_hetero_2 = torch.nn.Parameter(loaded_weights_snn['params']['alpha_2'].to(device))\n",
    "beta_hetero_2 = torch.nn.Parameter(loaded_weights_snn['params']['beta_2'].to(device))\n",
    "print(loaded_weights_snn['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/pbs.9515603.pbs/ipykernel_2139618/2872161497.py:68: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  labels_ = np.array(y,dtype=int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=0.74033\n",
      "ground_loss 0.6297724763239463\n",
      "reg_loss 0.11056003267840138\n",
      "Epoch 1: Train= 0.79970 Test Accuracy=0.75089\n",
      "Epoch 2: loss=0.55367\n",
      "ground_loss 0.4610331496150475\n",
      "reg_loss 0.09263711023753084\n",
      "Epoch 2: Train= 0.84953 Test Accuracy=0.70625\n",
      "Best 0.7508928571428571\n",
      "Epoch 3: loss=0.51194\n",
      "ground_loss 0.42002318811228895\n",
      "reg_loss 0.09191734707496298\n",
      "Epoch 3: Train= 0.86097 Test Accuracy=0.71964\n",
      "Best 0.7508928571428571\n",
      "Epoch 4: loss=0.47671\n",
      "ground_loss 0.3872542736802514\n",
      "reg_loss 0.0894587017361104\n",
      "Epoch 4: Train= 0.87525 Test Accuracy=0.66473\n",
      "Best 0.7508928571428571\n",
      "Epoch 5: loss=0.45719\n",
      "ground_loss 0.37025028197314797\n",
      "reg_loss 0.08693560752577668\n",
      "Epoch 5: Train= 0.88017 Test Accuracy=0.76250\n",
      "Epoch 6: loss=0.41809\n",
      "ground_loss 0.33189681160637713\n",
      "reg_loss 0.08619182879530539\n",
      "Epoch 6: Train= 0.89542 Test Accuracy=0.74062\n",
      "Best 0.7625\n",
      "Epoch 7: loss=0.38184\n",
      "ground_loss 0.3024492536004134\n",
      "reg_loss 0.07939259238599793\n",
      "Epoch 7: Train= 0.90674 Test Accuracy=0.73571\n",
      "Best 0.7625\n",
      "Epoch 8: loss=0.35210\n",
      "ground_loss 0.27428838117854804\n",
      "reg_loss 0.07780722754560118\n",
      "Epoch 8: Train= 0.91708 Test Accuracy=0.74152\n",
      "Best 0.7625\n",
      "Epoch 9: loss=0.33388\n",
      "ground_loss 0.2571433410635145\n",
      "reg_loss 0.07673676531967216\n",
      "Epoch 9: Train= 0.92064 Test Accuracy=0.77812\n",
      "Epoch 10: loss=0.31286\n",
      "ground_loss 0.23664399166041472\n",
      "reg_loss 0.07621755063768447\n",
      "Epoch 10: Train= 0.92987 Test Accuracy=0.77098\n",
      "Best 0.778125\n",
      "Epoch 11: loss=0.28832\n",
      "ground_loss 0.2110057766512623\n",
      "reg_loss 0.07731214664348467\n",
      "Epoch 11: Train= 0.93652 Test Accuracy=0.80714\n",
      "Epoch 12: loss=0.29775\n",
      "ground_loss 0.22334344702677464\n",
      "reg_loss 0.07441029354460596\n",
      "Epoch 12: Train= 0.93172 Test Accuracy=0.77277\n",
      "Best 0.8071428571428572\n",
      "Epoch 13: loss=0.25295\n",
      "ground_loss 0.17670716269044426\n",
      "reg_loss 0.07623929207719217\n",
      "Epoch 13: Train= 0.94906 Test Accuracy=0.77634\n",
      "Best 0.8071428571428572\n",
      "Epoch 14: loss=0.25963\n",
      "ground_loss 0.17724822444005275\n",
      "reg_loss 0.08237692983601037\n",
      "Epoch 14: Train= 0.94808 Test Accuracy=0.75223\n",
      "Best 0.8071428571428572\n",
      "Epoch 15: loss=0.28791\n",
      "ground_loss 0.2155561465916671\n",
      "reg_loss 0.07235372057697904\n",
      "Epoch 15: Train= 0.93615 Test Accuracy=0.77768\n",
      "Best 0.8071428571428572\n",
      "Epoch 16: loss=0.23512\n",
      "ground_loss 0.16238267652279748\n",
      "reg_loss 0.07273659526597796\n",
      "Epoch 16: Train= 0.95288 Test Accuracy=0.78438\n",
      "Best 0.8071428571428572\n",
      "Epoch 17: loss=0.20173\n",
      "ground_loss 0.13110099131549438\n",
      "reg_loss 0.07063115499620362\n",
      "Epoch 17: Train= 0.96580 Test Accuracy=0.79107\n",
      "Best 0.8071428571428572\n",
      "Epoch 18: loss=0.19604\n",
      "ground_loss 0.12601848398371945\n",
      "reg_loss 0.07002460862707904\n",
      "Epoch 18: Train= 0.96813 Test Accuracy=0.81563\n",
      "Epoch 19: loss=0.20307\n",
      "ground_loss 0.13771903640993938\n",
      "reg_loss 0.06534924762924825\n",
      "Epoch 19: Train= 0.96297 Test Accuracy=0.79598\n",
      "Best 0.815625\n",
      "Epoch 20: loss=0.39882\n",
      "ground_loss 0.3373881318261774\n",
      "reg_loss 0.06143248336404327\n",
      "Epoch 20: Train= 0.90207 Test Accuracy=0.73795\n",
      "Best 0.815625\n",
      "Epoch 21: loss=0.31847\n",
      "ground_loss 0.24992020956174596\n",
      "reg_loss 0.06855059726031747\n",
      "Epoch 21: Train= 0.92864 Test Accuracy=0.74018\n",
      "Best 0.815625\n",
      "Epoch 22: loss=0.26229\n",
      "ground_loss 0.1927261323206068\n",
      "reg_loss 0.06956210720726824\n",
      "Epoch 22: Train= 0.94833 Test Accuracy=0.77098\n",
      "Best 0.815625\n",
      "Epoch 23: loss=0.22117\n",
      "ground_loss 0.15405631948291787\n",
      "reg_loss 0.06711268606852358\n",
      "Epoch 23: Train= 0.95915 Test Accuracy=0.78348\n",
      "Best 0.815625\n",
      "Epoch 24: loss=0.20379\n",
      "ground_loss 0.13618602216478407\n",
      "reg_loss 0.06760642850610214\n",
      "Epoch 24: Train= 0.96567 Test Accuracy=0.78795\n",
      "Best 0.815625\n",
      "Epoch 25: loss=0.20181\n",
      "ground_loss 0.13601829654236477\n",
      "reg_loss 0.06578961143812795\n",
      "Epoch 25: Train= 0.96592 Test Accuracy=0.76696\n",
      "Best 0.815625\n",
      "Epoch 26: loss=0.19313\n",
      "ground_loss 0.12673029335817015\n",
      "reg_loss 0.06640313964659773\n",
      "Epoch 26: Train= 0.96432 Test Accuracy=0.80000\n",
      "Best 0.815625\n",
      "Epoch 27: loss=0.18324\n",
      "ground_loss 0.11965144111767528\n",
      "reg_loss 0.06358439331566255\n",
      "Epoch 27: Train= 0.96863 Test Accuracy=0.74911\n",
      "Best 0.815625\n",
      "Epoch 28: loss=0.18240\n",
      "ground_loss 0.12060956900396685\n",
      "reg_loss 0.061788211598640355\n",
      "Epoch 28: Train= 0.96912 Test Accuracy=0.79330\n",
      "Best 0.815625\n",
      "Epoch 29: loss=0.16359\n",
      "ground_loss 0.100973123611193\n",
      "reg_loss 0.06261458693761525\n",
      "Epoch 29: Train= 0.97662 Test Accuracy=0.78839\n",
      "Best 0.815625\n",
      "Epoch 30: loss=0.14067\n",
      "ground_loss 0.08032182510208896\n",
      "reg_loss 0.06035142251121716\n",
      "Epoch 30: Train= 0.98118 Test Accuracy=0.79598\n",
      "Best 0.815625\n",
      "Epoch 31: loss=0.14796\n",
      "ground_loss 0.08855279740386122\n",
      "reg_loss 0.059411556320631595\n",
      "Epoch 31: Train= 0.97625 Test Accuracy=0.77679\n",
      "Best 0.815625\n",
      "Epoch 32: loss=0.14693\n",
      "ground_loss 0.09029522546985018\n",
      "reg_loss 0.05663722248997275\n",
      "Epoch 32: Train= 0.97859 Test Accuracy=0.76562\n",
      "Best 0.815625\n",
      "Epoch 33: loss=0.13285\n",
      "ground_loss 0.07454871076826505\n",
      "reg_loss 0.058299457669023454\n",
      "Epoch 33: Train= 0.98204 Test Accuracy=0.79286\n",
      "Best 0.815625\n",
      "Epoch 34: loss=0.11740\n",
      "ground_loss 0.06078262549392351\n",
      "reg_loss 0.05662035505016019\n",
      "Epoch 34: Train= 0.98831 Test Accuracy=0.76652\n",
      "Best 0.815625\n",
      "Epoch 35: loss=0.11543\n",
      "ground_loss 0.05946533524203958\n",
      "reg_loss 0.05596363811394361\n",
      "Epoch 35: Train= 0.98831 Test Accuracy=0.77054\n",
      "Best 0.815625\n",
      "Epoch 36: loss=0.12306\n",
      "ground_loss 0.07101681392903872\n",
      "reg_loss 0.05204792660991038\n",
      "Epoch 36: Train= 0.98474 Test Accuracy=0.80179\n",
      "Best 0.815625\n",
      "Epoch 37: loss=0.12839\n",
      "ground_loss 0.07367850540895161\n",
      "reg_loss 0.05471350915554002\n",
      "Epoch 37: Train= 0.98130 Test Accuracy=0.78438\n",
      "Best 0.815625\n",
      "Epoch 38: loss=0.10422\n",
      "ground_loss 0.05191896877830892\n",
      "reg_loss 0.05230030613973385\n",
      "Epoch 38: Train= 0.99053 Test Accuracy=0.80491\n",
      "Best 0.815625\n",
      "Epoch 39: loss=0.09460\n",
      "ground_loss 0.04358949200580205\n",
      "reg_loss 0.051014720924257295\n",
      "Epoch 39: Train= 0.99348 Test Accuracy=0.78125\n",
      "Best 0.815625\n",
      "Epoch 40: loss=0.11557\n",
      "ground_loss 0.06365805292340714\n",
      "reg_loss 0.05191652641052336\n",
      "Epoch 40: Train= 0.98597 Test Accuracy=0.78705\n",
      "Best 0.815625\n",
      "Epoch 41: loss=0.13301\n",
      "ground_loss 0.07790267910426996\n",
      "reg_loss 0.055108336277130084\n",
      "Epoch 41: Train= 0.98019 Test Accuracy=0.78393\n",
      "Best 0.815625\n",
      "Epoch 42: loss=0.09991\n",
      "ground_loss 0.050612717155572466\n",
      "reg_loss 0.0492955335364567\n",
      "Epoch 42: Train= 0.99151 Test Accuracy=0.78795\n",
      "Best 0.815625\n",
      "Epoch 43: loss=0.08688\n",
      "ground_loss 0.03774124243861343\n",
      "reg_loss 0.04914073666601669\n",
      "Epoch 43: Train= 0.99483 Test Accuracy=0.78839\n",
      "Best 0.815625\n",
      "Epoch 44: loss=0.11342\n",
      "ground_loss 0.06590439414048171\n",
      "reg_loss 0.04751921236867041\n",
      "Epoch 44: Train= 0.98659 Test Accuracy=0.78036\n",
      "Best 0.815625\n",
      "Epoch 45: loss=0.11494\n",
      "ground_loss 0.0647466858247603\n",
      "reg_loss 0.0501940863749643\n",
      "Epoch 45: Train= 0.98499 Test Accuracy=0.78170\n",
      "Best 0.815625\n",
      "Epoch 46: loss=0.09084\n",
      "ground_loss 0.04068150223914798\n",
      "reg_loss 0.05015470064061833\n",
      "Epoch 46: Train= 0.99311 Test Accuracy=0.82589\n",
      "Epoch 47: loss=0.08996\n",
      "ground_loss 0.04253740512626612\n",
      "reg_loss 0.04742670502132318\n",
      "Epoch 47: Train= 0.99200 Test Accuracy=0.77723\n",
      "Best 0.8258928571428571\n",
      "Epoch 48: loss=0.11824\n",
      "ground_loss 0.06800239571318852\n",
      "reg_loss 0.050235379604608055\n",
      "Epoch 48: Train= 0.98462 Test Accuracy=0.77366\n",
      "Best 0.8258928571428571\n",
      "Epoch 49: loss=0.13187\n",
      "ground_loss 0.08374740080687944\n",
      "reg_loss 0.04812349484661433\n",
      "Epoch 49: Train= 0.97749 Test Accuracy=0.77679\n",
      "Best 0.8258928571428571\n",
      "Epoch 50: loss=0.14832\n",
      "ground_loss 0.09915761960538354\n",
      "reg_loss 0.049159393447825286\n",
      "Epoch 50: Train= 0.97207 Test Accuracy=0.76384\n",
      "Best 0.8258928571428571\n",
      "Epoch 51: loss=0.13786\n",
      "ground_loss 0.08682802681789153\n",
      "reg_loss 0.051028294884783075\n",
      "Epoch 51: Train= 0.97798 Test Accuracy=0.79554\n",
      "Best 0.8258928571428571\n",
      "Epoch 52: loss=0.10910\n",
      "ground_loss 0.061815971581954655\n",
      "reg_loss 0.04728435232179371\n",
      "Epoch 52: Train= 0.98548 Test Accuracy=0.78661\n",
      "Best 0.8258928571428571\n",
      "Epoch 53: loss=0.09612\n",
      "ground_loss 0.050585642156345166\n",
      "reg_loss 0.045538856933905386\n",
      "Epoch 53: Train= 0.98967 Test Accuracy=0.79598\n",
      "Best 0.8258928571428571\n",
      "Epoch 54: loss=0.08935\n",
      "ground_loss 0.04477176568140899\n",
      "reg_loss 0.04457744950149942\n",
      "Epoch 54: Train= 0.99102 Test Accuracy=0.78973\n",
      "Best 0.8258928571428571\n",
      "Epoch 55: loss=0.07978\n",
      "ground_loss 0.03546871951099221\n",
      "reg_loss 0.04431243114701406\n",
      "Epoch 55: Train= 0.99422 Test Accuracy=0.77812\n",
      "Best 0.8258928571428571\n",
      "Epoch 56: loss=0.08968\n",
      "ground_loss 0.04501630007252684\n",
      "reg_loss 0.0446679227523447\n",
      "Epoch 56: Train= 0.99040 Test Accuracy=0.78482\n",
      "Best 0.8258928571428571\n",
      "Epoch 57: loss=0.07874\n",
      "ground_loss 0.034901341053063244\n",
      "reg_loss 0.04383655452704805\n",
      "Epoch 57: Train= 0.99434 Test Accuracy=0.80134\n",
      "Best 0.8258928571428571\n",
      "Epoch 58: loss=0.06828\n",
      "ground_loss 0.02575317988040175\n",
      "reg_loss 0.042522010664770925\n",
      "Epoch 58: Train= 0.99668 Test Accuracy=0.80714\n",
      "Best 0.8258928571428571\n",
      "Epoch 59: loss=0.05804\n",
      "ground_loss 0.019752202551870598\n",
      "reg_loss 0.03828731358168632\n",
      "Epoch 59: Train= 0.99815 Test Accuracy=0.78705\n",
      "Best 0.8258928571428571\n",
      "Epoch 60: loss=0.05392\n",
      "ground_loss 0.016229113589561597\n",
      "reg_loss 0.03769115563922041\n",
      "Epoch 60: Train= 0.99951 Test Accuracy=0.79152\n",
      "Best 0.8258928571428571\n",
      "Epoch 61: loss=0.05324\n",
      "ground_loss 0.014423038088166572\n",
      "reg_loss 0.0388216718327342\n",
      "Epoch 61: Train= 0.99914 Test Accuracy=0.82232\n",
      "Best 0.8258928571428571\n",
      "Epoch 62: loss=0.06205\n",
      "ground_loss 0.02310678642243147\n",
      "reg_loss 0.03894600251645554\n",
      "Epoch 62: Train= 0.99705 Test Accuracy=0.77768\n",
      "Best 0.8258928571428571\n",
      "Epoch 63: loss=0.08834\n",
      "ground_loss 0.04865365781212651\n",
      "reg_loss 0.03968135648825037\n",
      "Epoch 63: Train= 0.99028 Test Accuracy=0.75580\n",
      "Best 0.8258928571428571\n",
      "Epoch 64: loss=0.13123\n",
      "ground_loss 0.08750731190739888\n",
      "reg_loss 0.04372531275345585\n",
      "Epoch 64: Train= 0.97318 Test Accuracy=0.79509\n",
      "Best 0.8258928571428571\n",
      "Epoch 65: loss=0.09099\n",
      "ground_loss 0.05038008610094626\n",
      "reg_loss 0.040614328809141176\n",
      "Epoch 65: Train= 0.98942 Test Accuracy=0.80045\n",
      "Best 0.8258928571428571\n",
      "Epoch 66: loss=0.07272\n",
      "ground_loss 0.03168737816837008\n",
      "reg_loss 0.041036002191267615\n",
      "Epoch 66: Train= 0.99459 Test Accuracy=0.76875\n",
      "Best 0.8258928571428571\n",
      "Epoch 67: loss=0.05758\n",
      "ground_loss 0.01996117111984787\n",
      "reg_loss 0.037615391242457184\n",
      "Epoch 67: Train= 0.99852 Test Accuracy=0.79018\n",
      "Best 0.8258928571428571\n",
      "Epoch 68: loss=0.04962\n",
      "ground_loss 0.013917536980758502\n",
      "reg_loss 0.035698139585378603\n",
      "Epoch 68: Train= 0.99963 Test Accuracy=0.82054\n",
      "Best 0.8258928571428571\n",
      "Epoch 69: loss=0.04743\n",
      "ground_loss 0.013349693994529135\n",
      "reg_loss 0.03407959688425533\n",
      "Epoch 69: Train= 0.99951 Test Accuracy=0.79107\n",
      "Best 0.8258928571428571\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m mlp_mlp \u001b[38;5;241m=\u001b[39m hetero_mlp_a_b_spikes()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# mlp_mlp = hetero_mlp_no_hidden().to(device)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m loss_hist_MLP \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_hybrid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlp_mlp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnb_epochs_mlp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp_interval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[124], line 51\u001b[0m, in \u001b[0;36mtrain_hybrid\u001b[0;34m(mlp, x_data, y_data, lr, nb_epochs, mlp_interval)\u001b[0m\n\u001b[1;32m     48\u001b[0m loss_MLP \u001b[38;5;241m=\u001b[39m ground_loss \u001b[38;5;241m+\u001b[39m reg_loss\n\u001b[1;32m     50\u001b[0m combined_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 51\u001b[0m \u001b[43mloss_MLP\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m combined_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Clamping the values\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/test1/lib/python3.9/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/test1/lib/python3.9/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/test1/lib/python3.9/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nb_epochs_mlp = 100\n",
    "batch_size_hetero = 64\n",
    "group_size = 5\n",
    "mlp_mlp = hetero_mlp_a_b_spikes().to(device)\n",
    "# mlp_mlp = hetero_mlp_no_hidden().to(device)\n",
    "loss_hist_MLP = train_hybrid(mlp_mlp, x_train, y_train, lr=2e-4, nb_epochs=nb_epochs_mlp, mlp_interval = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7214285714285714\n"
     ]
    }
   ],
   "source": [
    "# loaded_weights_snn = torch.load('Hetero/3_Hetero/epochs_2048/snn_9.pth')\n",
    "loaded_weights_snn = torch.load('hetero_group/epochs_10/snn_7.pth')\n",
    "# loaded_weights_snn = torch.load('hybrid_hetero/snn_2048.pth')\n",
    "# Convert tensors to parameters and ensure they are leaf tensors by re-wrapping them\n",
    "# Move tensors to device first and then wrap them as parameters\n",
    "w1 = torch.nn.Parameter(loaded_weights_snn['params']['w1'].to(device))\n",
    "w2 = torch.nn.Parameter(loaded_weights_snn['params']['w2'].to(device))\n",
    "v1 = torch.nn.Parameter(loaded_weights_snn['params']['v1'].to(device))\n",
    "alpha_hetero_1 = torch.nn.Parameter(loaded_weights_snn['params']['alpha'].to(device))\n",
    "beta_hetero_1 = torch.nn.Parameter(loaded_weights_snn['params']['beta'].to(device))\n",
    "thresholds_1 = torch.nn.Parameter(loaded_weights_snn['params']['threshold'].to(device))\n",
    "reset_1 = torch.nn.Parameter(loaded_weights_snn['params']['reset'].to(device))\n",
    "rest_1 = torch.nn.Parameter(loaded_weights_snn['params']['rest'].to(device))\n",
    "alpha_hetero_2 = torch.nn.Parameter(loaded_weights_snn['params']['alpha_2'].to(device))\n",
    "beta_hetero_2 = torch.nn.Parameter(loaded_weights_snn['params']['beta_2'].to(device))\n",
    "print(loaded_weights_snn['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.9841, 0.9950]], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(alpha_hetero_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/pbs.9515603.pbs/ipykernel_2139618/2872161497.py:68: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  labels_ = np.array(y,dtype=int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=0.72540\n",
      "ground_loss 0.6006458985993243\n",
      "reg_loss 0.1247497781759172\n",
      "Epoch 1: Train= 0.80438 Test Accuracy=0.72991\n",
      "Epoch 2: loss=0.58929\n",
      "ground_loss 0.4800604441034512\n",
      "reg_loss 0.10922743649933282\n",
      "Epoch 2: Train= 0.84781 Test Accuracy=0.71920\n",
      "Best 0.7299107142857143\n",
      "Epoch 3: loss=0.55015\n",
      "ground_loss 0.4474364751436579\n",
      "reg_loss 0.10271459336825244\n",
      "Epoch 3: Train= 0.85605 Test Accuracy=0.71875\n",
      "Best 0.7299107142857143\n",
      "Epoch 4: loss=0.49637\n",
      "ground_loss 0.3964260559617065\n",
      "reg_loss 0.09994044307414002\n",
      "Epoch 4: Train= 0.87574 Test Accuracy=0.76339\n",
      "Epoch 5: loss=0.49527\n",
      "ground_loss 0.3982166673724107\n",
      "reg_loss 0.09705540461568382\n",
      "Epoch 5: Train= 0.87315 Test Accuracy=0.73661\n",
      "Best 0.7633928571428571\n",
      "Epoch 6: loss=0.43391\n",
      "ground_loss 0.3408663220058276\n",
      "reg_loss 0.09304651829201406\n",
      "Epoch 6: Train= 0.89579 Test Accuracy=0.73884\n",
      "Best 0.7633928571428571\n",
      "Epoch 7: loss=0.41770\n",
      "ground_loss 0.32535451298623574\n",
      "reg_loss 0.09234427731102846\n",
      "Epoch 7: Train= 0.90367 Test Accuracy=0.77857\n",
      "Epoch 8: loss=0.38233\n",
      "ground_loss 0.2898286075338604\n",
      "reg_loss 0.09250031206316835\n",
      "Epoch 8: Train= 0.91622 Test Accuracy=0.77768\n",
      "Best 0.7785714285714286\n",
      "Epoch 9: loss=0.37048\n",
      "ground_loss 0.2838518298047734\n",
      "reg_loss 0.08662491485359163\n",
      "Epoch 9: Train= 0.91769 Test Accuracy=0.72009\n",
      "Best 0.7785714285714286\n",
      "Epoch 10: loss=0.33566\n",
      "ground_loss 0.2496795224275176\n",
      "reg_loss 0.08598163946876376\n",
      "Epoch 10: Train= 0.92975 Test Accuracy=0.77009\n",
      "Best 0.7785714285714286\n",
      "Epoch 11: loss=0.32485\n",
      "ground_loss 0.23472252329738122\n",
      "reg_loss 0.0901238377990685\n",
      "Epoch 11: Train= 0.93147 Test Accuracy=0.77768\n",
      "Best 0.7785714285714286\n",
      "Epoch 12: loss=0.31363\n",
      "ground_loss 0.23017870417730077\n",
      "reg_loss 0.08344851205433447\n",
      "Epoch 12: Train= 0.93590 Test Accuracy=0.76920\n",
      "Best 0.7785714285714286\n",
      "Epoch 13: loss=0.29820\n",
      "ground_loss 0.21920678881913658\n",
      "reg_loss 0.0789975079847133\n",
      "Epoch 13: Train= 0.93910 Test Accuracy=0.75714\n",
      "Best 0.7785714285714286\n",
      "Epoch 14: loss=0.27620\n",
      "ground_loss 0.19898355975160448\n",
      "reg_loss 0.07721468263606387\n",
      "Epoch 14: Train= 0.94894 Test Accuracy=0.74911\n",
      "Best 0.7785714285714286\n",
      "Epoch 15: loss=0.26054\n",
      "ground_loss 0.18364544101352767\n",
      "reg_loss 0.07689488479706245\n",
      "Epoch 15: Train= 0.94956 Test Accuracy=0.76562\n",
      "Best 0.7785714285714286\n",
      "Epoch 16: loss=0.25244\n",
      "ground_loss 0.17513421500527013\n",
      "reg_loss 0.07730587524926569\n",
      "Epoch 16: Train= 0.95448 Test Accuracy=0.78884\n",
      "Epoch 17: loss=0.24272\n",
      "ground_loss 0.16268751594259984\n",
      "reg_loss 0.08003033545073562\n",
      "Epoch 17: Train= 0.95632 Test Accuracy=0.75938\n",
      "Best 0.7888392857142857\n",
      "Epoch 18: loss=0.22693\n",
      "ground_loss 0.15022803174229118\n",
      "reg_loss 0.07670564245520614\n",
      "Epoch 18: Train= 0.96001 Test Accuracy=0.77009\n",
      "Best 0.7888392857142857\n",
      "Epoch 19: loss=0.22279\n",
      "ground_loss 0.1497109415610944\n",
      "reg_loss 0.0730822800592644\n",
      "Epoch 19: Train= 0.96088 Test Accuracy=0.77455\n",
      "Best 0.7888392857142857\n",
      "Epoch 20: loss=0.20588\n",
      "ground_loss 0.135686138630148\n",
      "reg_loss 0.07019237349704495\n",
      "Epoch 20: Train= 0.96617 Test Accuracy=0.70179\n",
      "Best 0.7888392857142857\n",
      "Epoch 21: loss=0.22672\n",
      "ground_loss 0.15114549743964917\n",
      "reg_loss 0.07557637872189049\n",
      "Epoch 21: Train= 0.95854 Test Accuracy=0.77411\n",
      "Best 0.7888392857142857\n",
      "Epoch 22: loss=0.21650\n",
      "ground_loss 0.14735099602871993\n",
      "reg_loss 0.06914487217120298\n",
      "Epoch 22: Train= 0.96321 Test Accuracy=0.77679\n",
      "Best 0.7888392857142857\n",
      "Epoch 23: loss=0.18868\n",
      "ground_loss 0.12102484788129649\n",
      "reg_loss 0.06765593591285503\n",
      "Epoch 23: Train= 0.96863 Test Accuracy=0.77545\n",
      "Best 0.7888392857142857\n",
      "Epoch 24: loss=0.19368\n",
      "ground_loss 0.12203470074872333\n",
      "reg_loss 0.07164711402509157\n",
      "Epoch 24: Train= 0.96617 Test Accuracy=0.77054\n",
      "Best 0.7888392857142857\n",
      "Epoch 25: loss=0.23283\n",
      "ground_loss 0.16704603082199734\n",
      "reg_loss 0.06578861203367316\n",
      "Epoch 25: Train= 0.95325 Test Accuracy=0.77455\n",
      "Best 0.7888392857142857\n",
      "Epoch 26: loss=0.19117\n",
      "ground_loss 0.12570426290429484\n",
      "reg_loss 0.06546491780501651\n",
      "Epoch 26: Train= 0.96567 Test Accuracy=0.76161\n",
      "Best 0.7888392857142857\n",
      "Epoch 27: loss=0.19232\n",
      "ground_loss 0.12293149874083639\n",
      "reg_loss 0.06938767037171079\n",
      "Epoch 27: Train= 0.96813 Test Accuracy=0.77768\n",
      "Best 0.7888392857142857\n",
      "Epoch 28: loss=0.15950\n",
      "ground_loss 0.09241652135245913\n",
      "reg_loss 0.06708697924815764\n",
      "Epoch 28: Train= 0.97638 Test Accuracy=0.76027\n",
      "Best 0.7888392857142857\n",
      "Epoch 29: loss=0.14296\n",
      "ground_loss 0.07904068951120996\n",
      "reg_loss 0.06391979748104501\n",
      "Epoch 29: Train= 0.98204 Test Accuracy=0.74687\n",
      "Best 0.7888392857142857\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[151], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m mlp_mlp \u001b[38;5;241m=\u001b[39m hetero_mlp_a_b_spikes()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# mlp_mlp = hetero_mlp_no_hidden().to(device)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m loss_hist_MLP \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_hybrid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlp_mlp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnb_epochs_mlp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp_interval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[143], line 51\u001b[0m, in \u001b[0;36mtrain_hybrid\u001b[0;34m(mlp, x_data, y_data, lr, nb_epochs, mlp_interval)\u001b[0m\n\u001b[1;32m     48\u001b[0m loss_MLP \u001b[38;5;241m=\u001b[39m ground_loss \u001b[38;5;241m+\u001b[39m reg_loss\n\u001b[1;32m     50\u001b[0m combined_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 51\u001b[0m \u001b[43mloss_MLP\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m combined_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Clamping the values\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/test1/lib/python3.9/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/test1/lib/python3.9/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/test1/lib/python3.9/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nb_epochs_mlp = 100\n",
    "batch_size_hetero = 64\n",
    "group_size = 10\n",
    "mlp_mlp = hetero_mlp_a_b_spikes().to(device)\n",
    "# mlp_mlp = hetero_mlp_no_hidden().to(device)\n",
    "loss_hist_MLP = train_hybrid(mlp_mlp, x_train, y_train, lr=2e-4, nb_epochs=nb_epochs_mlp, mlp_interval = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7580357142857143\n"
     ]
    }
   ],
   "source": [
    "loaded_weights_snn = torch.load('SHD_Results/hetero_group/best_20/best_snn.pth')\n",
    "\n",
    "# Convert tensors to parameters and ensure they are leaf tensors by re-wrapping them\n",
    "# Move tensors to device first and then wrap them as parameters\n",
    "# w1 = torch.nn.Parameter(loaded_weights_snn['params']['w1'].to(device))\n",
    "# w2 = torch.nn.Parameter(loaded_weights_snn['params']['w2'].to(device))\n",
    "# v1 = torch.nn.Parameter(loaded_weights_snn['params']['v1'].to(device))\n",
    "# alpha_hetero_1 = torch.nn.Parameter(loaded_weights_snn['params']['alpha'].to(device))\n",
    "# beta_hetero_1 = torch.nn.Parameter(loaded_weights_snn['params']['beta'].to(device))\n",
    "# thresholds_1 = torch.nn.Parameter(loaded_weights_snn['params']['threshold'].to(device))\n",
    "# reset_1 = torch.nn.Parameter(loaded_weights_snn['params']['reset'].to(device))\n",
    "# rest_1 = torch.nn.Parameter(loaded_weights_snn['params']['rest'].to(device))\n",
    "# alpha_hetero_2 = torch.nn.Parameter(loaded_weights_snn['params']['alpha_2'].to(device))\n",
    "# beta_hetero_2 = torch.nn.Parameter(loaded_weights_snn['params']['beta_2'].to(device))\n",
    "# print(loaded_weights_snn['train_acc_hist'])\n",
    "# print(loaded_weights_snn['test_acc_hist'])\n",
    "print(loaded_weights_snn['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49601092027141647, 0.5752358990670059, 0.6082882739609838, 0.6287505301102629, 0.641128604749788, 0.6568861323155216, 0.664585983884648, 0.6778917514843087, 0.6891831000848176, 0.6979696776929601, 0.7086779050042409, 0.7160596904156065]\n",
      "[0.5218160377358491, 0.5707547169811321, 0.584561713836478, 0.60844143081761, 0.6238207547169812, 0.6235259433962265, 0.6201356132075472, 0.6298643867924528, 0.6335986635220126, 0.6460298742138365, 0.6479952830188679, 0.6422955974842768]\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "loaded_weights_snn = torch.load('SSC/Python_Tests/Hybrid_Hetero_no_reg/epochs/snn_12.pth')\n",
    "# loaded_weights_snn = torch.load('SSC/Python_Tests/Hybrid_Homo_no_reg/epochs/snn_24.pth')\n",
    "# loaded_weights_snn = torch.load('SSC/Python_Tests/Hybrid_Hetero_no_reg/snn_best.pth')\n",
    "# loaded_weights_snn = torch.load('SSC/Python_Tests/Hybrid_Homo_no_reg/snn_best.pth')\n",
    "\n",
    "# Convert tensors to parameters and ensure they are leaf tensors by re-wrapping them\n",
    "# Move tensors to device first and then wrap them as parameters\n",
    "# w1 = torch.nn.Parameter(loaded_weights_snn['params']['w1'].to(device))\n",
    "# w2 = torch.nn.Parameter(loaded_weights_snn['params']['w2'].to(device))\n",
    "# v1 = torch.nn.Parameter(loaded_weights_snn['params']['v1'].to(device))\n",
    "# alpha_hetero_1 = torch.nn.Parameter(loaded_weights_snn['params']['alpha'].to(device))\n",
    "# beta_hetero_1 = torch.nn.Parameter(loaded_weights_snn['params']['beta'].to(device))\n",
    "# thresholds_1 = torch.nn.Parameter(loaded_weights_snn['params']['threshold'].to(device))\n",
    "# reset_1 = torch.nn.Parameter(loaded_weights_snn['params']['reset'].to(device))\n",
    "# rest_1 = torch.nn.Parameter(loaded_weights_snn['params']['rest'].to(device))\n",
    "# alpha_hetero_2 = torch.nn.Parameter(loaded_weights_snn['params']['alpha_2'].to(device))\n",
    "# beta_hetero_2 = torch.nn.Parameter(loaded_weights_snn['params']['beta_2'].to(device))\n",
    "print(loaded_weights_snn['train_acc_hist'])\n",
    "print(loaded_weights_snn['test_acc_hist'])\n",
    "print(loaded_weights_snn['epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.9 (test1)",
   "language": "python",
   "name": "python39_test1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
