{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "init done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "# print(torch.__version__)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "# import torchvision\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "import pickle\n",
    "import math\n",
    "from utils import get_shd_dataset\n",
    "\n",
    "# The coarse network structure and the time steps are dicated by the SHD dataset.\n",
    "nb_inputs  = 700\n",
    "nb_hidden  = 200\n",
    "nb_outputs = 35\n",
    "\n",
    "time_step = 1e-3\n",
    "nb_steps = 100\n",
    "max_time = 1.4\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "dtype = torch.float\n",
    "\n",
    "# Check whether a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Here we load the Dataset\n",
    "# cache_dir = os.path.expanduser(\"~/data\")\n",
    "# cache_subdir = \"hdspikes\"\n",
    "# get_shd_dataset(cache_dir, cache_subdir)\n",
    "\n",
    "# Here we load the Dataset\n",
    "cache_dir = os.path.expanduser(\"~/data\")\n",
    "cache_subdir = \"ssc_data\"\n",
    "train_file = h5py.File(os.path.join(cache_dir, cache_subdir, 'ssc_train.h5'), 'r')\n",
    "validation_file = h5py.File(os.path.join(cache_dir, cache_subdir, 'ssc_valid.h5'), 'r')\n",
    "test_file = h5py.File(os.path.join(cache_dir, cache_subdir, 'ssc_test.h5'), 'r')\n",
    "\n",
    "x_train = train_file['spikes']\n",
    "y_train = train_file['labels']\n",
    "x_valid = validation_file['spikes']\n",
    "y_valid = validation_file['labels']\n",
    "x_test = test_file['spikes']\n",
    "y_test = test_file['labels']\n",
    "\n",
    "# Here we load the Dataset\n",
    "# cache_dir = os.path.expanduser(\"~/data\")\n",
    "# cache_subdir = \"hdspikes\"\n",
    "# get_shd_dataset(cache_dir, cache_subdir)\n",
    "\n",
    "# train_file = h5py.File(os.path.join(cache_dir, cache_subdir, 'shd_train.h5'), 'r')\n",
    "# test_file = h5py.File(os.path.join(cache_dir, cache_subdir, 'shd_test.h5'), 'r')\n",
    "\n",
    "# x_train = train_file['spikes']\n",
    "# y_train = train_file['labels']\n",
    "# x_test = test_file['spikes']\n",
    "# y_test = test_file['labels']\n",
    "\n",
    "\n",
    "\n",
    "def sparse_data_generator_from_hdf5_spikes(X, y, batch_size, nb_steps, nb_units, max_time, shuffle=True):\n",
    "    \"\"\" This generator takes a spike dataset and generates spiking network input as sparse tensors.\n",
    "\n",
    "    Args:\n",
    "        X: The data ( sample x event x 2 ) the last dim holds (time,neuron) tuples\n",
    "        y: The labels\n",
    "    \"\"\"\n",
    "\n",
    "    labels_ = np.array(y,dtype=int)\n",
    "    number_of_batches = len(labels_)//batch_size\n",
    "    sample_index = np.arange(len(labels_))\n",
    "\n",
    "    # compute discrete firing times\n",
    "    firing_times = X['times']\n",
    "    units_fired = X['units']\n",
    "\n",
    "    time_bins = np.linspace(0, max_time, num=nb_steps)\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "\n",
    "    total_batch_count = 0\n",
    "    counter = 0\n",
    "    while counter<number_of_batches:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "\n",
    "        coo = [ [] for i in range(3) ]\n",
    "        for bc,idx in enumerate(batch_index):\n",
    "            times = np.digitize(firing_times[idx], time_bins)\n",
    "            units = units_fired[idx]\n",
    "            batch = [bc for _ in range(len(times))]\n",
    "\n",
    "            coo[0].extend(batch)\n",
    "            coo[1].extend(times)\n",
    "            coo[2].extend(units)\n",
    "\n",
    "        i = torch.LongTensor(coo).to(device)\n",
    "        v = torch.FloatTensor(np.ones(len(coo[0]))).to(device)\n",
    "\n",
    "        X_batch = torch.sparse.FloatTensor(i, v, torch.Size([batch_size,nb_steps,nb_units])).to(device)\n",
    "        y_batch = torch.tensor(labels_[batch_index],device=device)\n",
    "\n",
    "        yield X_batch.to(device=device), y_batch.to(device=device)\n",
    "\n",
    "        counter += 1\n",
    "        \n",
    "class SurrGradSpike(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Here we implement our spiking nonlinearity which also implements\n",
    "    the surrogate gradient. By subclassing torch.autograd.Function,\n",
    "    we will be able to use all of PyTorch's autograd functionality.\n",
    "    Here we use the normalized negative part of a fast sigmoid\n",
    "    as this was done in Zenke & Ganguli (2018).\n",
    "    \"\"\"\n",
    "\n",
    "    scale = 100.0 # controls steepness of surrogate gradient\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we compute a step function of the input Tensor\n",
    "        and return it. ctx is a context object that we use to stash information which\n",
    "        we need to later backpropagate our error signals. To achieve this we use the\n",
    "        ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        out = torch.zeros_like(input)\n",
    "        out[input > 0] = 1.0\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor we need to compute the\n",
    "        surrogate gradient of the loss with respect to the input.\n",
    "        Here we use the normalized negative part of a fast sigmoid\n",
    "        as this was done in Zenke & Ganguli (2018).\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad = grad_input/(SurrGradSpike.scale*torch.abs(input)+1.0)**2\n",
    "        return grad\n",
    "\n",
    "# here we overwrite our naive spike function by the \"SurrGradSpike\" nonlinearity which implements a surrogate gradient\n",
    "spike_fn  = SurrGradSpike.apply\n",
    "\n",
    "def dist_fn(dist):\n",
    "    return {\n",
    "        'gamma': lambda mean, k, size: np.random.gamma(k, scale=mean/k, size=size),\n",
    "        'normal': lambda mean, k, size: np.random.normal(loc=mean, scale=mean/np.sqrt(k), size=size), #change standard deviation to match gamma\n",
    "        'uniform': lambda _, maximum, size: np.random.uniform(low=0, high=maximum, size=size),\n",
    "    }[dist.lower()]\n",
    "print(\"init done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_snn_hetero(inputs):\n",
    "    # Initialize memory and synaptic variables\n",
    "    syn = torch.zeros((batch_size_hetero, nb_hidden), device=device, dtype=dtype)\n",
    "    mem = torch.zeros((batch_size_hetero, nb_hidden), device=device, dtype=dtype)\n",
    "\n",
    "    mem_rec = []\n",
    "    spk_rec = []\n",
    "\n",
    "    # Compute hidden layer activity\n",
    "    out = torch.zeros((batch_size_hetero, nb_hidden), device=device, dtype=dtype)\n",
    "    h1_from_input = torch.einsum(\"abc,cd->abd\", (inputs, w1))\n",
    "    for t in range(nb_steps):\n",
    "        h1 = h1_from_input[:, t] + torch.einsum(\"ab,bc->ac\", (out, v1))\n",
    "        mthr = mem - thresholds_1\n",
    "        out = spike_fn(mthr)\n",
    "        rst = torch.zeros_like(mem)\n",
    "        c = (mthr > 0)\n",
    "        rst[c] = torch.ones_like(mem)[c]\n",
    "        # rst = out.detach() * reset  # Reset mechanism considering individual reset values\n",
    "\n",
    "        new_syn = alpha_hetero_1 * syn + h1\n",
    "        new_mem = beta_hetero_1 * (mem - rest_1) + rest_1 + (1 - beta_hetero_1) * syn - rst * (thresholds_1 - reset_1)\n",
    "\n",
    "        mem_rec.append(mem)\n",
    "        spk_rec.append(out)\n",
    "\n",
    "        mem = new_mem\n",
    "        syn = new_syn\n",
    "\n",
    "    mem_rec = torch.stack(mem_rec, dim=1)\n",
    "    spk_rec = torch.stack(spk_rec, dim=1)\n",
    "\n",
    "    # Readout layer\n",
    "    h2 = torch.einsum(\"abc,cd->abd\", (spk_rec, w2))\n",
    "    flt = torch.zeros((batch_size_hetero, nb_outputs), device=device, dtype=dtype)\n",
    "    out = torch.zeros((batch_size_hetero, nb_outputs), device=device, dtype=dtype)\n",
    "    out_rec = [out]\n",
    "    for t in range(nb_steps):\n",
    "        # print(alpha_hetero.shape)\n",
    "        new_flt = alpha_hetero_2 * flt + h2[:, t]  # Assume alpha for the output layer\n",
    "        # new_out = beta_hetero_2 * out + flt\n",
    "        new_out = beta_hetero_2 * out + (1 - beta_hetero_2)*flt  # Assume beta for the output layer\n",
    "\n",
    "        flt = new_flt\n",
    "        out = new_out\n",
    "\n",
    "        out_rec.append(out)\n",
    "\n",
    "    out_rec = torch.stack(out_rec, dim=1)\n",
    "    other_recs = [mem_rec, spk_rec]\n",
    "    return out_rec, other_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classification_accuracy_hetero(x_data, y_data):\n",
    "    \"\"\" Computes classification accuracy on supplied data in batches. \"\"\"\n",
    "    accs = []\n",
    "    for x_local, y_local in sparse_data_generator_from_hdf5_spikes(x_data, y_data, batch_size_hetero, nb_steps, nb_inputs, max_time, shuffle=False):\n",
    "        output,_ = run_snn_hetero(x_local.to_dense())\n",
    "        m,_= torch.max(output,1) # max over time\n",
    "        _,am=torch.max(m,1)      # argmax over output units\n",
    "        tmp = np.mean((y_local==am).detach().cpu().numpy()) # compare to labels\n",
    "        accs.append(tmp)\n",
    "    return np.mean(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_snn_hetero(x_data, y_data, lr=1e-3, nb_epochs=10):\n",
    "    params = [w1, w2, v1, alpha_hetero_1, beta_hetero_1,\n",
    "              alpha_hetero_2, beta_hetero_2,\n",
    "              thresholds_1, reset_1, rest_1]\n",
    "    optimizer = torch.optim.Adam(params, lr=lr, betas=(0.9, 0.999))\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    log_softmax_fn = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    loss_hist = []\n",
    "    train_acc_hist = []\n",
    "    test_acc_hist = []\n",
    "    best_accuracy = 0\n",
    "    best_params = params\n",
    "\n",
    "    for e in range(nb_epochs):\n",
    "        local_loss = []\n",
    "        local_ground_loss = []\n",
    "        local_reg_loss = []\n",
    "        accs = []\n",
    "\n",
    "        for x_local, y_local in sparse_data_generator_from_hdf5_spikes(x_data, y_data, batch_size_hetero, nb_steps, nb_inputs, max_time):\n",
    "            output, recs = run_snn_hetero(x_local.to_dense())\n",
    "            _, spks = recs\n",
    "            m, _ = torch.max(output, 1)\n",
    "\n",
    "            _, am = torch.max(m, 1)  # argmax over output units\n",
    "            tmp = np.mean((y_local == am).detach().cpu().numpy())  # compare to labels\n",
    "            accs.append(tmp)\n",
    "\n",
    "            log_p_y = nn.LogSoftmax(dim=1)(m)\n",
    "\n",
    "            ground_loss = loss_fn(log_p_y, y_local)\n",
    "\n",
    "            reg_loss = 1e-6 * torch.sum(spks)  # L1 loss on total number of spikes\n",
    "            reg_loss += 1e-6 * torch.mean(torch.sum(torch.sum(spks, dim=0), dim=0) ** 2)  # L2 loss on spikes per neuron\n",
    "\n",
    "            loss_val = ground_loss + reg_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "            # Clamping the values\n",
    "            with torch.no_grad():\n",
    "                alpha_hetero_1.clamp_(0.367, 0.995)\n",
    "                beta_hetero_1.clamp_(0.367, 0.995)\n",
    "                alpha_hetero_2.clamp_(0.367, 0.995)\n",
    "                beta_hetero_2.clamp_(0.367, 0.995)\n",
    "                thresholds_1.clamp_(0.5, 1.5)\n",
    "                \n",
    "            local_loss.append(loss_val.item())\n",
    "\n",
    "        mean_loss = np.mean(local_loss)\n",
    "        loss_hist.append(mean_loss)\n",
    "        print(f\"Epoch {e + 1}: loss={mean_loss:.5f}\")\n",
    "\n",
    "        train_accuracy = np.mean(accs)\n",
    "        test_accuracy = compute_classification_accuracy_hetero(x_test, y_test)\n",
    "        train_acc_hist.append(train_accuracy)\n",
    "        test_acc_hist.append(test_accuracy)\n",
    "        print(f\"Epoch {e + 1}: Train= {train_accuracy:.5f} Test Accuracy={test_accuracy:.5f}\")\n",
    "\n",
    "        saved_params_hetero = {\n",
    "            'w1': w1.clone(),\n",
    "            'w2': w2.clone(),\n",
    "            'v1': v1.clone(),\n",
    "            'alpha': alpha_hetero_1.clone(),\n",
    "            'beta': beta_hetero_1.clone(),\n",
    "            'threshold': thresholds_1.clone(),\n",
    "            'reset': reset_1.clone(),\n",
    "            'rest': rest_1.clone(),\n",
    "            'alpha_2': alpha_hetero_2.clone(),\n",
    "            'beta_2': beta_hetero_2.clone()\n",
    "        }\n",
    "\n",
    "        # Save parameters along with the current epoch and accuracy\n",
    "        directory = 'SSC_test_2/epochs_hetero'\n",
    "\n",
    "        # Create the directory if it does not exist\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "        # Save the file in the specified directory\n",
    "        file_path = os.path.join(directory, f'snn_{e + 1}.pth')\n",
    "        torch.save({\n",
    "            'epoch': e + 1,\n",
    "            'accuracy': test_accuracy,\n",
    "            'params': saved_params_hetero,\n",
    "            'loss': loss_hist,\n",
    "            'train_acc_hist': train_acc_hist,\n",
    "            'test_acc_hist': test_acc_hist\n",
    "        }, file_path)\n",
    "\n",
    "        # Print the best accuracy so far\n",
    "        if test_accuracy > best_accuracy:\n",
    "            best_accuracy = test_accuracy\n",
    "            print(f\"Epoch {e + 1}: Best Test Accuracy={best_accuracy:.5f}\")\n",
    "\n",
    "            directory = 'SSC_test_2/best_hetero'\n",
    "\n",
    "            # Create the directory if it does not exist\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "\n",
    "            # Save parameters only when a new best accuracy is achieved\n",
    "            # Create a dictionary of current parameters to save\n",
    "            saved_params_hetero = {\n",
    "                'w1': w1.clone(),\n",
    "                'w2': w2.clone(),\n",
    "                'v1': v1.clone(),\n",
    "                'alpha': alpha_hetero_1.clone(),\n",
    "                'beta': beta_hetero_1.clone(),\n",
    "                'threshold': thresholds_1.clone(),\n",
    "                'reset': reset_1.clone(),\n",
    "                'rest': rest_1.clone(),\n",
    "                'alpha_2': alpha_hetero_2.clone(),\n",
    "                'beta_2': beta_hetero_2.clone()\n",
    "            }\n",
    "\n",
    "            # Save the file in the specified directory\n",
    "            file_path = os.path.join(directory, 'best_snn.pth')\n",
    "            torch.save({\n",
    "                'epoch': e + 1,\n",
    "                'accuracy': best_accuracy,\n",
    "                'params': saved_params_hetero,\n",
    "                'loss': loss_hist,\n",
    "                'train_acc_hist': train_acc_hist,\n",
    "                'test_acc_hist': test_acc_hist\n",
    "            }, file_path)\n",
    "        else:\n",
    "            print('Best', best_accuracy)\n",
    "\n",
    "    return loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0034,  0.0015, -0.0127,  ..., -0.0058, -0.0292, -0.0093],\n",
       "        [ 0.0188,  0.0311, -0.0013,  ...,  0.0205, -0.0194, -0.0179],\n",
       "        [ 0.0107,  0.0169,  0.0124,  ..., -0.0079,  0.0329, -0.0007],\n",
       "        ...,\n",
       "        [-0.0110, -0.0262,  0.0215,  ...,  0.0261,  0.0097, -0.0130],\n",
       "        [ 0.0009, -0.0216, -0.0070,  ..., -0.0128,  0.0114,  0.0172],\n",
       "        [-0.0007,  0.0024, -0.0260,  ...,  0.0034,  0.0070,  0.0336]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating tensors with requires_grad=True\n",
    "thresholds_1 = torch.empty((1, nb_hidden), device=device, dtype=dtype, requires_grad=True)\n",
    "torch.nn.init.uniform_(thresholds_1, a=0.5, b=1.5)  # Thresholds uniformly distributed between 0.5 and 1.5\n",
    "\n",
    "reset_1 = torch.empty((1, nb_hidden), device=device, dtype=dtype, requires_grad=True)\n",
    "torch.nn.init.uniform_(reset_1, a=-0.5, b=0.5)  # Reset potentials uniformly distributed between -0.5 and 0.5\n",
    "\n",
    "rest_1 = torch.empty((1, nb_hidden), device=device, dtype=dtype, requires_grad=True)\n",
    "torch.nn.init.uniform_(rest_1, a=-0.5, b=0.5)  # Rest potentials uniformly distributed between -0.5 and 0.5\n",
    "\n",
    "# Gamma distribution for alpha and beta\n",
    "tau_syn = 10e-3\n",
    "tau_mem = 20e-3\n",
    "distribution = dist_fn('gamma')\n",
    "\n",
    "alpha_hetero_1_dist = torch.tensor(distribution(tau_syn, 3, (1, nb_hidden)), device=device, dtype=dtype)\n",
    "alpha_hetero_1 = torch.exp(-time_step / alpha_hetero_1_dist)\n",
    "alpha_hetero_1.requires_grad_(True)\n",
    "\n",
    "beta_hetero_1_dist = torch.tensor(distribution(tau_mem, 3, (1, nb_hidden)), device=device, dtype=dtype)\n",
    "beta_hetero_1 = torch.exp(-time_step / beta_hetero_1_dist)\n",
    "beta_hetero_1.requires_grad_(True)\n",
    "\n",
    "alpha_hetero_2_dist = torch.tensor(distribution(tau_syn, 3, (1, nb_outputs)), device=device, dtype=dtype)\n",
    "alpha_hetero_2 = torch.exp(-time_step / alpha_hetero_2_dist)\n",
    "alpha_hetero_2.requires_grad_(True)\n",
    "\n",
    "beta_hetero_2_dist = torch.tensor(distribution(tau_mem, 3, (1, nb_outputs)), device=device, dtype=dtype)\n",
    "beta_hetero_2 = torch.exp(-time_step / beta_hetero_2_dist)\n",
    "beta_hetero_2.requires_grad_(True)\n",
    "\n",
    "weight_scale = 0.2\n",
    "\n",
    "w1 = torch.empty((nb_inputs, nb_hidden),  device=device, dtype=dtype, requires_grad=True)\n",
    "torch.nn.init.normal_(w1, mean=0.0, std=weight_scale/np.sqrt(nb_inputs))\n",
    "\n",
    "w2 = torch.empty((nb_hidden, nb_outputs), device=device, dtype=dtype, requires_grad=True)\n",
    "torch.nn.init.normal_(w2, mean=0.0, std=weight_scale/np.sqrt(nb_hidden))\n",
    "\n",
    "v1 = torch.empty((nb_hidden, nb_hidden), device=device, dtype=dtype, requires_grad=True)\n",
    "torch.nn.init.normal_(v1, mean=0.0, std=weight_scale/np.sqrt(nb_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/pbs.9525514.pbs/ipykernel_1328006/3323606593.py:84: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  labels_ = np.array(y,dtype=int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=3.10464\n",
      "Epoch 1: Train= 0.17645 Test Accuracy=0.24887\n",
      "Epoch 1: Best Test Accuracy=0.24887\n",
      "Epoch 2: loss=2.57702\n",
      "Epoch 2: Train= 0.29619 Test Accuracy=0.32188\n",
      "Epoch 2: Best Test Accuracy=0.32188\n",
      "Epoch 3: loss=2.34167\n",
      "Epoch 3: Train= 0.36191 Test Accuracy=0.37534\n",
      "Epoch 3: Best Test Accuracy=0.37534\n",
      "Epoch 4: loss=2.17475\n",
      "Epoch 4: Train= 0.40488 Test Accuracy=0.40247\n",
      "Epoch 4: Best Test Accuracy=0.40247\n",
      "Epoch 5: loss=2.03988\n",
      "Epoch 5: Train= 0.43808 Test Accuracy=0.43519\n",
      "Epoch 5: Best Test Accuracy=0.43519\n",
      "Epoch 6: loss=1.94486\n",
      "Epoch 6: Train= 0.46371 Test Accuracy=0.44084\n",
      "Epoch 6: Best Test Accuracy=0.44084\n",
      "Epoch 7: loss=1.86686\n",
      "Epoch 7: Train= 0.48324 Test Accuracy=0.45765\n",
      "Epoch 7: Best Test Accuracy=0.45765\n",
      "Epoch 8: loss=1.80510\n",
      "Epoch 8: Train= 0.49720 Test Accuracy=0.47003\n",
      "Epoch 8: Best Test Accuracy=0.47003\n",
      "Epoch 9: loss=1.74801\n",
      "Epoch 9: Train= 0.51343 Test Accuracy=0.47666\n",
      "Epoch 9: Best Test Accuracy=0.47666\n",
      "Epoch 10: loss=1.71089\n",
      "Epoch 10: Train= 0.52074 Test Accuracy=0.49450\n",
      "Epoch 10: Best Test Accuracy=0.49450\n",
      "Epoch 11: loss=1.66597\n",
      "Epoch 11: Train= 0.53560 Test Accuracy=0.50324\n",
      "Epoch 11: Best Test Accuracy=0.50324\n",
      "Epoch 12: loss=1.63686\n",
      "Epoch 12: Train= 0.54009 Test Accuracy=0.49430\n",
      "Best 0.5032429245283019\n",
      "Epoch 13: loss=1.60248\n",
      "Epoch 13: Train= 0.54917 Test Accuracy=0.50929\n",
      "Epoch 13: Best Test Accuracy=0.50929\n",
      "Epoch 14: loss=1.56618\n",
      "Epoch 14: Train= 0.56083 Test Accuracy=0.50290\n",
      "Best 0.5092865566037735\n",
      "Epoch 15: loss=1.53738\n",
      "Epoch 15: Train= 0.56669 Test Accuracy=0.52462\n",
      "Epoch 15: Best Test Accuracy=0.52462\n",
      "Epoch 16: loss=1.51605\n",
      "Epoch 16: Train= 0.57153 Test Accuracy=0.51219\n",
      "Best 0.5246167452830188\n",
      "Epoch 17: loss=1.48490\n",
      "Epoch 17: Train= 0.58225 Test Accuracy=0.50349\n",
      "Best 0.5246167452830188\n",
      "Epoch 18: loss=1.46071\n",
      "Epoch 18: Train= 0.58791 Test Accuracy=0.51150\n",
      "Best 0.5246167452830188\n",
      "Epoch 19: loss=1.43998\n",
      "Epoch 19: Train= 0.59376 Test Accuracy=0.50069\n",
      "Best 0.5246167452830188\n",
      "Epoch 20: loss=1.41937\n",
      "Epoch 20: Train= 0.60063 Test Accuracy=0.50821\n",
      "Best 0.5246167452830188\n",
      "Epoch 21: loss=1.40118\n",
      "Epoch 21: Train= 0.60494 Test Accuracy=0.51302\n",
      "Best 0.5246167452830188\n",
      "Epoch 22: loss=1.36713\n",
      "Epoch 22: Train= 0.61462 Test Accuracy=0.51096\n",
      "Best 0.5246167452830188\n",
      "Epoch 23: Train= 0.62060 Test Accuracy=0.51415\n",
      "Best 0.5246167452830188\n",
      "Epoch 24: loss=1.32273\n",
      "Epoch 24: Train= 0.62753 Test Accuracy=0.51872\n",
      "Best 0.5246167452830188\n",
      "Epoch 25: loss=1.30284\n",
      "Epoch 25: Train= 0.63286 Test Accuracy=0.50963\n",
      "Best 0.5246167452830188\n",
      "Epoch 26: loss=1.28294\n",
      "Epoch 26: Train= 0.63881 Test Accuracy=0.51730\n",
      "Best 0.5246167452830188\n",
      "Epoch 27: loss=1.26064\n",
      "Epoch 27: Train= 0.64500 Test Accuracy=0.51887\n",
      "Best 0.5246167452830188\n",
      "Epoch 28: loss=1.23880\n",
      "Epoch 29: Train= 0.65536 Test Accuracy=0.52594\n",
      "Epoch 29: Best Test Accuracy=0.52594\n",
      "Epoch 30: loss=1.19862\n",
      "Epoch 30: Train= 0.66078 Test Accuracy=0.51690\n",
      "Best 0.5259433962264151\n",
      "Epoch 31: loss=1.17777\n",
      "Epoch 31: Train= 0.66706 Test Accuracy=0.53007\n",
      "Epoch 31: Best Test Accuracy=0.53007\n",
      "Epoch 32: loss=1.15800\n",
      "Epoch 32: Train= 0.67361 Test Accuracy=0.52363\n",
      "Best 0.5300707547169812\n",
      "Epoch 33: loss=1.14056\n",
      "Epoch 33: Train= 0.67840 Test Accuracy=0.51002\n",
      "Best 0.5300707547169812\n",
      "Epoch 34: loss=1.12301\n",
      "Epoch 34: Train= 0.68342 Test Accuracy=0.51951\n",
      "Best 0.5300707547169812\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m nb_epochs_snn_hetero \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m150\u001b[39m\n\u001b[1;32m      2\u001b[0m batch_size_hetero \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[0;32m----> 3\u001b[0m loss_hist_snn_hetero \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_snn_hetero\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnb_epochs_snn_hetero\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 21\u001b[0m, in \u001b[0;36mtrain_snn_hetero\u001b[0;34m(x_data, y_data, lr, nb_epochs)\u001b[0m\n\u001b[1;32m     18\u001b[0m local_reg_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     19\u001b[0m accs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_local, y_local \u001b[38;5;129;01min\u001b[39;00m sparse_data_generator_from_hdf5_spikes(x_data, y_data, batch_size_hetero, nb_steps, nb_inputs, max_time):\n\u001b[1;32m     22\u001b[0m     output, recs \u001b[38;5;241m=\u001b[39m run_snn_hetero(x_local\u001b[38;5;241m.\u001b[39mto_dense())\n\u001b[1;32m     23\u001b[0m     _, spks \u001b[38;5;241m=\u001b[39m recs\n",
      "Cell \u001b[0;32mIn[1], line 112\u001b[0m, in \u001b[0;36msparse_data_generator_from_hdf5_spikes\u001b[0;34m(X, y, batch_size, nb_steps, nb_units, max_time, shuffle)\u001b[0m\n\u001b[1;32m    109\u001b[0m     coo[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mextend(times)\n\u001b[1;32m    110\u001b[0m     coo[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mextend(units)\n\u001b[0;32m--> 112\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLongTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoo\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    113\u001b[0m v \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(np\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;28mlen\u001b[39m(coo[\u001b[38;5;241m0\u001b[39m])))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    115\u001b[0m X_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mFloatTensor(i, v, torch\u001b[38;5;241m.\u001b[39mSize([batch_size,nb_steps,nb_units]))\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nb_epochs_snn_hetero = 150\n",
    "batch_size_hetero = 64\n",
    "loss_hist_snn_hetero = train_snn_hetero(x_train, y_train, lr=2e-4, nb_epochs=nb_epochs_snn_hetero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2368317610062893\n",
      "0.3331367924528302\n",
      "0.377063679245283\n",
      "0.4137185534591195\n",
      "0.42914701257861637\n",
      "0.45936517295597484\n",
      "0.4750884433962264\n",
      "0.48452240566037735\n",
      "0.49164701257861637\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,11):\n",
    "    loaded_weights_snn = torch.load(f'SSC_first_test/epochs_hetero/snn_{i}.pth')\n",
    "    print(compute_classification_accuracy_hetero(s_train, y_train))\n",
    "    print(loaded_weights_snn['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5548349056603774\n"
     ]
    }
   ],
   "source": [
    "loaded_weights_snn = torch.load('Python_Tests/SSC_test_2/best_hetero/best_snn.pth')\n",
    "\n",
    "w1 = torch.nn.Parameter(loaded_weights_snn['params']['w1'].to(device))\n",
    "w2 = torch.nn.Parameter(loaded_weights_snn['params']['w2'].to(device))\n",
    "v1 = torch.nn.Parameter(loaded_weights_snn['params']['v1'].to(device))\n",
    "alpha_hetero_1 = torch.nn.Parameter(loaded_weights_snn['params']['alpha'].to(device))\n",
    "beta_hetero_1 = torch.nn.Parameter(loaded_weights_snn['params']['beta'].to(device))\n",
    "thresholds_1 = torch.nn.Parameter(loaded_weights_snn['params']['threshold'].to(device))\n",
    "reset_1 = torch.nn.Parameter(loaded_weights_snn['params']['reset'].to(device))\n",
    "rest_1 = torch.nn.Parameter(loaded_weights_snn['params']['rest'].to(device))\n",
    "alpha_hetero_2 = torch.nn.Parameter(loaded_weights_snn['params']['alpha_2'].to(device))\n",
    "beta_hetero_2 = torch.nn.Parameter(loaded_weights_snn['params']['beta_2'].to(device))\n",
    "print(loaded_weights_snn['test_acc_hist'])\n",
    "# print(compute_classification_accuracy_hetero(x_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21363993710691823, 0.2592865566037736, 0.2797268081761006, 0.30625982704402516, 0.33176100628930816, 0.35834316037735847, 0.36453419811320753, 0.3553950471698113, 0.40433372641509435, 0.40521816037735847, 0.415438286163522, 0.41037735849056606, 0.4165683962264151, 0.42914701257861637, 0.44079205974842767, 0.4392688679245283, 0.41592963836477986, 0.44089033018867924, 0.44978380503144655, 0.39219732704402516, 0.4367138364779874, 0.45902122641509435, 0.4495381289308176, 0.4537637578616352, 0.4410868710691824, 0.44025157232704404, 0.4054147012578616, 0.4374508647798742, 0.43209512578616355, 0.4349941037735849, 0.4679638364779874, 0.4427574685534591, 0.4785770440251572, 0.4221206761006289, 0.44585298742138363, 0.455188679245283, 0.4410377358490566, 0.4641312893081761, 0.47110849056603776, 0.4659001572327044, 0.47420400943396224, 0.4490467767295597, 0.4882566823899371, 0.47214033018867924, 0.4900746855345912, 0.47420400943396224, 0.4878636006289308, 0.47921580188679247, 0.47066627358490565, 0.4846206761006289, 0.44580385220125784, 0.49174528301886794, 0.4537637578616352, 0.45867727987421386, 0.4858981918238994, 0.4719437893081761, 0.4690448113207547, 0.49086084905660377, 0.49375982704402516, 0.49228577044025157, 0.4860455974842767, 0.48806014150943394, 0.48634040880503143, 0.46648977987421386, 0.45725235849056606, 0.485750786163522, 0.48167256289308175, 0.4913030660377358, 0.4470322327044025]\n"
     ]
    }
   ],
   "source": [
    "# loaded_weights_snn = torch.load('Python_Tests/SSC_hetero_lr_1e-3/epochs_hetero/snn_70.pth', map_location=torch.device('cpu'))\n",
    "# loaded_weights_snn = torch.load('Python_Tests/SSC_homo_lr_1e-3/epochs/snn_69.pth', map_location=torch.device('cpu'))\n",
    "# loaded_weights_snn = torch.load('Python_Tests/SSC_hetero_no_reg/epochs_hetero/snn_71.pth', map_location=torch.device('cpu'))\n",
    "# loaded_weights_snn = torch.load('Python_Tests/SSC_homo_no_reg/epochs/snn_68.pth', map_location=torch.device('cpu'))\n",
    "# loaded_weights_snn = torch.load('Python_Tests/SSC_hetero_no_reg/best_hetero/best_snn.pth', map_location=torch.device('cpu'))\n",
    "# loaded_weights_snn = torch.load('Python_Tests/SSC_homo_no_reg/best/best_snn.pth', map_location=torch.device('cpu'))\n",
    "w1 = torch.nn.Parameter(loaded_weights_snn['params']['w1'].to(device))\n",
    "w2 = torch.nn.Parameter(loaded_weights_snn['params']['w2'].to(device))\n",
    "v1 = torch.nn.Parameter(loaded_weights_snn['params']['v1'].to(device))\n",
    "alpha_hetero_1 = torch.nn.Parameter(loaded_weights_snn['params']['alpha'].to(device))\n",
    "beta_hetero_1 = torch.nn.Parameter(loaded_weights_snn['params']['beta'].to(device))\n",
    "thresholds_1 = torch.nn.Parameter(loaded_weights_snn['params']['threshold'].to(device))\n",
    "reset_1 = torch.nn.Parameter(loaded_weights_snn['params']['reset'].to(device))\n",
    "rest_1 = torch.nn.Parameter(loaded_weights_snn['params']['rest'].to(device))\n",
    "alpha_hetero_2 = torch.nn.Parameter(loaded_weights_snn['params']['alpha_2'].to(device))\n",
    "beta_hetero_2 = torch.nn.Parameter(loaded_weights_snn['params']['beta_2'].to(device))\n",
    "print(loaded_weights_snn['test_acc_hist'])\n",
    "# print(rest_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5519850628930818, 0.5764544025157232, 0.5718356918238994, 0.5355738993710691, 0.5731623427672956, 0.5780758647798742, 0.5762087264150944, 0.5833333333333334, 0.5679048742138365, 0.58500393081761, 0.5812205188679245, 0.5873624213836478, 0.5796973270440252, 0.5815153301886793, 0.5829402515723271, 0.584561713836478, 0.5839720911949685, 0.5828419811320755, 0.5840703616352201, 0.5792059748427673, 0.5759139150943396, 0.5826945754716981, 0.5714426100628931, 0.5782232704402516, 0.5714917452830188, 0.5747838050314465, 0.5715408805031447, 0.5723270440251572, 0.5613207547169812, 0.5681014150943396, 0.5709512578616353, 0.56343356918239, 0.567560927672956, 0.5658411949685535, 0.5675117924528302, 0.5656937893081762, 0.568936713836478, 0.5602397798742138, 0.566185141509434, 0.5497739779874213, 0.5555227987421384, 0.5637775157232704, 0.5609768081761006, 0.5491843553459119, 0.5337067610062893, 0.5500196540880503, 0.5556702044025157, 0.5448113207547169, 0.5343946540880503, 0.5593062106918238, 0.5463345125786163, 0.5472680817610063, 0.544123427672956, 0.5343455188679245, 0.5457448899371069, 0.5565546383647799, 0.5497248427672956, 0.5398977987421384, 0.5401434748427673, 0.5391607704402516, 0.5372444968553459, 0.5386694182389937, 0.5318396226415094, 0.5429441823899371, 0.5379323899371069, 0.5424036949685535]\n"
     ]
    }
   ],
   "source": [
    "# loaded_weights_snn = torch.load('Python_Tests/Hybrid_Hetero_int_5/snn_best.pth', map_location=torch.device('cpu'))\n",
    "loaded_weights_snn = torch.load('Python_Tests/Hybrid_Homo/epochs/snn_66.pth', map_location=torch.device('cpu'))\n",
    "print(loaded_weights_snn['test_acc_hist'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOMO HYBRID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "init done\n",
      "0.5752751572327044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/pbs.9529077.pbs/ipykernel_3553303/502186389.py:84: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  labels_ = np.array(y,dtype=int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=1.65236\n",
      "ground_loss 1.6100557048609947\n",
      "reg_loss 0.04229970570366144\n",
      "Epoch 1: Train= 0.55821 Test Accuracy=0.56014\n",
      "Epoch 2: loss=1.41226\n",
      "ground_loss 1.3753598979006791\n",
      "reg_loss 0.036897322429980735\n",
      "Epoch 2: Train= 0.61599 Test Accuracy=0.56604\n",
      "Epoch 3: loss=1.37327\n",
      "ground_loss 1.3384665056646021\n",
      "reg_loss 0.034807233291538314\n",
      "Epoch 3: Train= 0.62356 Test Accuracy=0.55120\n",
      "Best 0.5660377358490566\n",
      "Epoch 4: loss=1.34750\n",
      "ground_loss 1.3122466347094206\n",
      "reg_loss 0.035249515772308206\n",
      "Epoch 4: Train= 0.63078 Test Accuracy=0.57370\n",
      "Epoch 5: loss=1.32519\n",
      "ground_loss 1.2906800935691043\n",
      "reg_loss 0.03451130066487221\n",
      "Epoch 5: Train= 0.63298 Test Accuracy=0.57690\n",
      "Epoch 6: loss=1.30405\n",
      "ground_loss 1.269622088338077\n",
      "reg_loss 0.034432012666778304\n",
      "Epoch 6: Train= 0.63788 Test Accuracy=0.56820\n",
      "Best 0.5768966194968553\n",
      "Epoch 7: loss=1.28070\n",
      "ground_loss 1.2464253988379235\n",
      "reg_loss 0.034277205176152244\n",
      "Epoch 7: Train= 0.64471 Test Accuracy=0.56815\n",
      "Best 0.5768966194968553\n",
      "Epoch 8: loss=1.26482\n",
      "ground_loss 1.231087009263706\n",
      "reg_loss 0.03373502258075507\n",
      "Epoch 8: Train= 0.64688 Test Accuracy=0.57650\n",
      "Best 0.5768966194968553\n",
      "Epoch 9: loss=1.25725\n",
      "ground_loss 1.224772308871744\n",
      "reg_loss 0.03248074983955188\n",
      "Epoch 9: Train= 0.64958 Test Accuracy=0.58088\n",
      "Epoch 10: loss=1.23743\n",
      "ground_loss 1.2047835476222535\n",
      "reg_loss 0.032645158033637905\n",
      "Epoch 10: Train= 0.65470 Test Accuracy=0.58048\n",
      "Best 0.580876572327044\n",
      "Epoch 11: loss=1.22989\n",
      "ground_loss 1.196990340670051\n",
      "reg_loss 0.03289871692398375\n",
      "Epoch 11: Train= 0.65471 Test Accuracy=0.58520\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "# print(torch.__version__)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "# import torchvision\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "import pickle\n",
    "import math\n",
    "# from utils import get_shd_dataset\n",
    "\n",
    "# The coarse network structure and the time steps are dicated by the SHD dataset.\n",
    "nb_inputs  = 700\n",
    "nb_hidden  = 200\n",
    "nb_outputs = 35\n",
    "\n",
    "time_step = 1e-3\n",
    "nb_steps = 100\n",
    "max_time = 1.4\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "dtype = torch.float\n",
    "\n",
    "# Check whether a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Here we load the Dataset\n",
    "# cache_dir = os.path.expanduser(\"~/data\")\n",
    "# cache_subdir = \"hdspikes\"\n",
    "# get_shd_dataset(cache_dir, cache_subdir)\n",
    "\n",
    "# Here we load the Dataset\n",
    "cache_dir = os.path.expanduser(\"~/data\")\n",
    "cache_subdir = \"ssc_data\"\n",
    "train_file = h5py.File(os.path.join(cache_dir, cache_subdir, 'ssc_train.h5'), 'r')\n",
    "validation_file = h5py.File(os.path.join(cache_dir, cache_subdir, 'ssc_valid.h5'), 'r')\n",
    "test_file = h5py.File(os.path.join(cache_dir, cache_subdir, 'ssc_test.h5'), 'r')\n",
    "\n",
    "x_train = train_file['spikes']\n",
    "y_train = train_file['labels']\n",
    "x_valid = validation_file['spikes']\n",
    "y_valid = validation_file['labels']\n",
    "x_test = test_file['spikes']\n",
    "y_test = test_file['labels']\n",
    "\n",
    "# Here we load the Dataset\n",
    "# cache_dir = os.path.expanduser(\"~/data\")\n",
    "# cache_subdir = \"hdspikes\"\n",
    "# get_shd_dataset(cache_dir, cache_subdir)\n",
    "\n",
    "# train_file = h5py.File(os.path.join(cache_dir, cache_subdir, 'shd_train.h5'), 'r')\n",
    "# test_file = h5py.File(os.path.join(cache_dir, cache_subdir, 'shd_test.h5'), 'r')\n",
    "\n",
    "# x_train = train_file['spikes']\n",
    "# y_train = train_file['labels']\n",
    "# x_test = test_file['spikes']\n",
    "# y_test = test_file['labels']\n",
    "\n",
    "\n",
    "\n",
    "def sparse_data_generator_from_hdf5_spikes(X, y, batch_size, nb_steps, nb_units, max_time, shuffle=True):\n",
    "    \"\"\" This generator takes a spike dataset and generates spiking network input as sparse tensors.\n",
    "\n",
    "    Args:\n",
    "        X: The data ( sample x event x 2 ) the last dim holds (time,neuron) tuples\n",
    "        y: The labels\n",
    "    \"\"\"\n",
    "\n",
    "    labels_ = np.array(y,dtype=int)\n",
    "    number_of_batches = len(labels_)//batch_size\n",
    "    sample_index = np.arange(len(labels_))\n",
    "\n",
    "    # compute discrete firing times\n",
    "    firing_times = X['times']\n",
    "    units_fired = X['units']\n",
    "\n",
    "    time_bins = np.linspace(0, max_time, num=nb_steps)\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "\n",
    "    total_batch_count = 0\n",
    "    counter = 0\n",
    "    while counter<number_of_batches:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "\n",
    "        coo = [ [] for i in range(3) ]\n",
    "        for bc,idx in enumerate(batch_index):\n",
    "            times = np.digitize(firing_times[idx], time_bins)\n",
    "            units = units_fired[idx]\n",
    "            batch = [bc for _ in range(len(times))]\n",
    "\n",
    "            coo[0].extend(batch)\n",
    "            coo[1].extend(times)\n",
    "            coo[2].extend(units)\n",
    "\n",
    "        i = torch.LongTensor(coo).to(device)\n",
    "        v = torch.FloatTensor(np.ones(len(coo[0]))).to(device)\n",
    "\n",
    "        X_batch = torch.sparse.FloatTensor(i, v, torch.Size([batch_size,nb_steps,nb_units])).to(device)\n",
    "        y_batch = torch.tensor(labels_[batch_index],device=device)\n",
    "\n",
    "        yield X_batch.to(device=device), y_batch.to(device=device)\n",
    "\n",
    "        counter += 1\n",
    "        \n",
    "class SurrGradSpike(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Here we implement our spiking nonlinearity which also implements\n",
    "    the surrogate gradient. By subclassing torch.autograd.Function,\n",
    "    we will be able to use all of PyTorch's autograd functionality.\n",
    "    Here we use the normalized negative part of a fast sigmoid\n",
    "    as this was done in Zenke & Ganguli (2018).\n",
    "    \"\"\"\n",
    "\n",
    "    scale = 100.0 # controls steepness of surrogate gradient\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we compute a step function of the input Tensor\n",
    "        and return it. ctx is a context object that we use to stash information which\n",
    "        we need to later backpropagate our error signals. To achieve this we use the\n",
    "        ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        out = torch.zeros_like(input)\n",
    "        out[input > 0] = 1.0\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor we need to compute the\n",
    "        surrogate gradient of the loss with respect to the input.\n",
    "        Here we use the normalized negative part of a fast sigmoid\n",
    "        as this was done in Zenke & Ganguli (2018).\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad = grad_input/(SurrGradSpike.scale*torch.abs(input)+1.0)**2\n",
    "        return grad\n",
    "\n",
    "# here we overwrite our naive spike function by the \"SurrGradSpike\" nonlinearity which implements a surrogate gradient\n",
    "spike_fn  = SurrGradSpike.apply\n",
    "\n",
    "def dist_fn(dist):\n",
    "    return {\n",
    "        'gamma': lambda mean, k, size: np.random.gamma(k, scale=mean/k, size=size),\n",
    "        'normal': lambda mean, k, size: np.random.normal(loc=mean, scale=mean/np.sqrt(k), size=size), #change standard deviation to match gamma\n",
    "        'uniform': lambda _, maximum, size: np.random.uniform(low=0, high=maximum, size=size),\n",
    "    }[dist.lower()]\n",
    "print(\"init done\")\n",
    "\n",
    "\n",
    "class MLP_alpha_beta_single1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_alpha_beta_single1, self).__init__()\n",
    "        self.input_size = 928 + 15\n",
    "        self.hidden_size = 1024\n",
    "        self.output_size = 7\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(self.input_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, self.output_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    def init_weights(self):\n",
    "        with torch.no_grad():\n",
    "            # Initialize first layer weights and biases\n",
    "            self.layers[0].weight.fill_(0)\n",
    "            self.layers[0].bias.fill_(0)\n",
    "            for i in range(self.output_size):\n",
    "                self.layers[0].weight[i, i] = 1\n",
    "\n",
    "            # Initialize second layer weights and biases\n",
    "            self.layers[2].weight.fill_(0)\n",
    "            self.layers[2].bias.fill_(0)\n",
    "            for i in range(self.output_size):\n",
    "                self.layers[2].weight[i, i] = 1\n",
    "                \n",
    "                \n",
    "def run_snn_hybrid_alpha_beta_spikes_homo(inputs, mlp, mlp_interval, batch_size_MLP):\n",
    "\n",
    "    # Initialize local copies of alpha, beta, threshold, reset and rest for all 200 hidden neurons\n",
    "    alpha_1_local = alpha_homo_1\n",
    "    beta_1_local = beta_homo_1\n",
    "    thresholds_local = thresholds_1\n",
    "    reset_local = reset_1\n",
    "    rest_local = rest_1\n",
    "    alpha_2_local = alpha_homo_2\n",
    "    beta_2_local = beta_homo_2\n",
    "    alpha_1_local = alpha_1_local.expand(batch_size_MLP, 1)\n",
    "    beta_1_local = beta_1_local.expand(batch_size_MLP, 1)\n",
    "    thresholds_local = thresholds_local.expand(batch_size_MLP, 1)\n",
    "    reset_local = reset_local.expand(batch_size_MLP, 1)\n",
    "    rest_local = rest_local.expand(batch_size_MLP, 1)\n",
    "    alpha_2_local = alpha_2_local.expand(batch_size_MLP, 1)\n",
    "    beta_2_local = beta_2_local.expand(batch_size_MLP, 1)\n",
    "\n",
    "    # Initialize synaptic and membrane potentials\n",
    "    syn = torch.zeros((batch_size_MLP, nb_hidden), device=device, dtype=dtype)\n",
    "    mem = torch.zeros((batch_size_MLP, nb_hidden), device=device, dtype=dtype)\n",
    "\n",
    "    # Initialize recordings for membrane potentials and spikes\n",
    "    mem_rec = []\n",
    "    spk_rec = []\n",
    "\n",
    "    test = 0\n",
    "\n",
    "    # Initialize outputs for the hidden layer\n",
    "    out = torch.zeros((batch_size_MLP, nb_hidden), device=device, dtype=dtype)\n",
    "    h1_from_input = torch.einsum(\"abc,cd->abd\", (inputs, w1))\n",
    "\n",
    "    # Prepare readout layer variables\n",
    "    flt2 = torch.zeros((batch_size_MLP, nb_outputs), device=device, dtype=dtype)\n",
    "    out2 = torch.zeros((batch_size_MLP, nb_outputs), device=device, dtype=dtype)\n",
    "    out_rec = [out2]\n",
    "\n",
    "    for t in range(nb_steps):\n",
    "        # Compute hidden layer activity\n",
    "        h1 = h1_from_input[:, t] + torch.einsum(\"ab,bc->ac\", (out, v1))\n",
    "        mthr = mem - thresholds_local\n",
    "        out = spike_fn(mthr)\n",
    "        rst = torch.zeros_like(mem)\n",
    "        c = (mthr > 0)\n",
    "        rst[c] = torch.ones_like(mem)[c]\n",
    "        # rst = out.detach() * reset  # Reset mechanism considering individual reset values\n",
    "\n",
    "        # Update synaptic and membrane potentials\n",
    "        syn = alpha_1_local * syn + h1\n",
    "\n",
    "        mem = beta_1_local * (mem - rest_local) + rest_local + (1 - beta_1_local) * syn - rst * (thresholds_local - reset_local)\n",
    "\n",
    "        # Record membrane potentials and spikes\n",
    "        mem_rec.append(mem)\n",
    "        spk_rec.append(out)\n",
    "\n",
    "\n",
    "        # Now compute h2 on the fly\n",
    "        h2_t = torch.einsum(\"ab,bc->ac\", (out, w2))\n",
    "#         flt2 = alpha_1_local * flt2 + h2_t\n",
    "#         out2 = beta_1_local * out2 + flt2*(1-beta_1_local)\n",
    "        flt2 = alpha_2_local * flt2 + h2_t\n",
    "        out2 = beta_2_local * out2 + flt2*(1-beta_2_local)\n",
    "        \n",
    "        out_rec.append(out2)\n",
    "\n",
    "         # Flatten and concatenate spikes for each item in the batch\n",
    "        input_spikes_flat = inputs[:, t, :].reshape(batch_size_MLP, -1)  # Shape: [batch_size, 700]\n",
    "        hidden_spikes_flat = out.reshape(batch_size_MLP, -1)  # Shape: [batch_size, 200]\n",
    "        output_spikes_flat = out2.reshape(batch_size_MLP, -1)  # Shape: [batch_size, 20]\n",
    "\n",
    "        # Time tensor\n",
    "        time_tensor = torch.full((batch_size_MLP, 1), t, device=device, dtype=dtype)\n",
    "\n",
    "        # Concatenate tensors\n",
    "        mlp_input = torch.cat([\n",
    "            alpha_1_local, beta_1_local, \n",
    "            thresholds_local, reset_local, rest_local,\n",
    "            alpha_2_local, beta_2_local,\n",
    "            time_tensor,\n",
    "            input_spikes_flat, hidden_spikes_flat, output_spikes_flat\n",
    "        ], dim=1)\n",
    "\n",
    "        # Process with MLP (in a single call for the whole batch)\n",
    "       \n",
    "\n",
    "        if t % mlp_interval == 0 and t != 0:\n",
    "            mlp_outputs = mlp(mlp_input)\n",
    "            # Update alpha_local and beta_local based on MLP outputs\n",
    "            alpha_1_local = mlp_outputs[:, 0].unsqueeze(1)\n",
    "            beta_1_local = mlp_outputs[:, 1].unsqueeze(1)\n",
    "            threshold_local = (mlp_outputs[:, 2] + 0.5).unsqueeze(1)\n",
    "            reset_local = (mlp_outputs[:, 3] - 0.5).unsqueeze(1)\n",
    "            rest_local = (mlp_outputs[:, 4] - 0.5).unsqueeze(1)\n",
    "            alpha_2_local = mlp_outputs[:, 5].unsqueeze(1)\n",
    "            beta_2_local = mlp_outputs[:, 6].unsqueeze(1)\n",
    "\n",
    "\n",
    "    # Stack recordings for output\n",
    "    mem_rec = torch.stack(mem_rec, dim=1).to(device)\n",
    "    spk_rec = torch.stack(spk_rec, dim=1).to(device)\n",
    "    out_rec = torch.stack(out_rec[1:], dim=1).to(device)  # Skip the initial zero tensor\n",
    "\n",
    "    other_recs = [mem_rec, spk_rec]\n",
    "\n",
    "    return out_rec, other_recs\n",
    "\n",
    "\n",
    "def compute_classification_accuracy_MLP_homo(x_data, y_data, mlp, mlp_interval):\n",
    "    \"\"\" Computes classification accuracy on supplied data in batches. \"\"\"\n",
    "    accs = []\n",
    "    for x_local, y_local in sparse_data_generator_from_hdf5_spikes(x_data, y_data, batch_size_homo, nb_steps, nb_inputs, max_time, shuffle=False):\n",
    "        output, _ = run_snn_hybrid_alpha_beta_spikes_homo(x_local.to_dense(),  mlp=mlp, mlp_interval=mlp_interval, batch_size_MLP=batch_size_homo)\n",
    "        m,_= torch.max(output,1) # max over time\n",
    "        _,am=torch.max(m,1)      # argmax over output units\n",
    "        tmp = np.mean((y_local==am).detach().cpu().numpy()) # compare to labels\n",
    "        accs.append(tmp)\n",
    "    return np.mean(accs)\n",
    "\n",
    "def train_hybrid(mlp, x_data, y_data, lr=1e-3, nb_epochs=10, mlp_interval=10):\n",
    "\n",
    "    snn_params = [w1, w2, v1, \n",
    "                  alpha_homo_1, beta_homo_1,\n",
    "                  thresholds_1, reset_1, rest_1,\n",
    "                  alpha_homo_2, beta_homo_2]\n",
    "#  \n",
    "\n",
    "    # Optimizers\n",
    "    combined_params = [\n",
    "        {'params': snn_params, 'lr': lr},  # Parameters for SNN with specific learning rate\n",
    "        {'params': mlp.parameters(), 'lr': lr}  # Parameters for MLP with its own learning rate\n",
    "    ]\n",
    "\n",
    "    # Using a single optimizer for both SNN and MLP\n",
    "    combined_optimizer = torch.optim.Adam(combined_params)\n",
    "\n",
    "\n",
    "    #Loss functions\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    log_softmax_fn = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    best_accuracy = 0\n",
    "\n",
    "    loss_hist = []\n",
    "    train_acc_hist = []\n",
    "    test_acc_hist = []\n",
    "    for epoch in range(nb_epochs):\n",
    "        local_loss = []\n",
    "        local_ground_loss = []\n",
    "        local_reg_loss = []\n",
    "        accs = []\n",
    "        for x_local, y_local in sparse_data_generator_from_hdf5_spikes(x_data, y_data, batch_size_homo, nb_steps, nb_inputs, max_time):\n",
    "                output, recs = run_snn_hybrid_alpha_beta_spikes_homo(inputs=x_local.to_dense(), mlp=mlp, mlp_interval=mlp_interval, batch_size_MLP=batch_size_homo)\n",
    "                _ , spks = recs\n",
    "                m, _ = torch.max(output,1)\n",
    "\n",
    "                _,am=torch.max(m,1)      # argmax over output units\n",
    "                tmp = np.mean((y_local==am).detach().cpu().numpy()) # compare to labels\n",
    "                accs.append(tmp)\n",
    "\n",
    "                log_p_y = log_softmax_fn(m)\n",
    "                ground_loss = loss_fn(log_p_y, y_local)\n",
    "                reg_loss = 1e-6*torch.sum(spks) # L1 loss on total number of spikes\n",
    "                reg_loss += 1e-6*torch.mean(torch.sum(torch.sum(spks,dim=0),dim=0)**2) # L2 loss on spikes per neuron\n",
    "\n",
    "                loss_MLP = ground_loss + reg_loss\n",
    "\n",
    "                combined_optimizer.zero_grad()\n",
    "                loss_MLP.backward()\n",
    "                combined_optimizer.step()\n",
    "                \n",
    "                # Clamping the values\n",
    "                with torch.no_grad():\n",
    "                    alpha_homo_1.clamp_(0.367, 0.995)\n",
    "                    beta_homo_1.clamp_(0.367, 0.995)\n",
    "                    alpha_homo_2.clamp_(0.367, 0.995)\n",
    "                    beta_homo_2.clamp_(0.367, 0.995)\n",
    "                    thresholds_1.clamp_(0.5, 1.5)\n",
    "\n",
    "                local_loss.append(loss_MLP.item())\n",
    "                local_ground_loss.append(ground_loss.item())\n",
    "                local_reg_loss.append(reg_loss.item())\n",
    "\n",
    "\n",
    "        mean_loss = np.mean(local_loss)\n",
    "        loss_hist.append(mean_loss)\n",
    "        print(f\"Epoch {epoch+1}: loss={mean_loss:.5f}\")\n",
    "        print(\"ground_loss\", np.mean(local_ground_loss))\n",
    "        print(\"reg_loss\", np.mean(local_reg_loss))\n",
    "        train_accuracy = np.mean(accs)\n",
    "        test_accuracy = compute_classification_accuracy_MLP_homo(x_test, y_test, mlp, mlp_interval)\n",
    "        train_acc_hist.append(train_accuracy)\n",
    "        test_acc_hist.append(test_accuracy)\n",
    "        print(f\"Epoch {epoch + 1}: Train= {train_accuracy:.5f} Test Accuracy={test_accuracy:.5f}\")\n",
    "\n",
    "        # Print the best accuracy so far\n",
    "        if test_accuracy > best_accuracy:\n",
    "            best_accuracy = test_accuracy\n",
    "\n",
    "            directory = 'Hybrid_Homo'\n",
    "\n",
    "            # Create the directory if it does not exist\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "\n",
    "            best_model_state = mlp_mlp.state_dict()\n",
    "\n",
    "            # Save the file in the specified directory\n",
    "            file_path = os.path.join(directory, 'mlp.pt')\n",
    "            torch.save(best_model_state, file_path)\n",
    "\n",
    "            # Save parameters only when a new best accuracy is achieved\n",
    "            # Create a dictionary of current parameters to save\n",
    "            saved_params_homo = {\n",
    "                'w1': w1.clone(),\n",
    "                'w2': w2.clone(),\n",
    "                'v1': v1.clone(),\n",
    "                'alpha': alpha_homo_1.clone(),\n",
    "                'beta': beta_homo_1.clone(),\n",
    "                'threshold': thresholds_1.clone(),\n",
    "                'reset': reset_1.clone(),\n",
    "                'rest': rest_1.clone(),\n",
    "                'alpha_2': alpha_homo_2.clone(),\n",
    "                'beta_2': beta_homo_2.clone()\n",
    "            }\n",
    "\n",
    "            # Save the file in the specified directory\n",
    "            file_path = os.path.join(directory, 'snn.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'accuracy': best_accuracy,\n",
    "                'params': saved_params_homo,\n",
    "                'loss': loss_hist,\n",
    "                'train_acc_hist': train_acc_hist,\n",
    "                'test_acc_hist': test_acc_hist\n",
    "            }, file_path)\n",
    "        else:\n",
    "            print(\"Best\", best_accuracy)\n",
    "\n",
    "    return loss_hist\n",
    "\n",
    "loaded_weights_snn = torch.load('Python_Tests/SSC_homo/epochs/snn_35.pth')\n",
    "\n",
    "w1 = torch.nn.Parameter(loaded_weights_snn['params']['w1'].to(device))\n",
    "w2 = torch.nn.Parameter(loaded_weights_snn['params']['w2'].to(device))\n",
    "v1 = torch.nn.Parameter(loaded_weights_snn['params']['v1'].to(device))\n",
    "alpha_homo_1 = torch.nn.Parameter(loaded_weights_snn['params']['alpha'].to(device))\n",
    "beta_homo_1 = torch.nn.Parameter(loaded_weights_snn['params']['beta'].to(device))\n",
    "thresholds_1 = torch.nn.Parameter(loaded_weights_snn['params']['threshold'].to(device))\n",
    "reset_1 = torch.nn.Parameter(loaded_weights_snn['params']['reset'].to(device))\n",
    "rest_1 = torch.nn.Parameter(loaded_weights_snn['params']['rest'].to(device))\n",
    "alpha_homo_2 = torch.nn.Parameter(loaded_weights_snn['params']['alpha_2'].to(device))\n",
    "beta_homo_2 = torch.nn.Parameter(loaded_weights_snn['params']['beta_2'].to(device))\n",
    "print(loaded_weights_snn['accuracy'])\n",
    "# print(compute_classification_accuracy_homo(x_train, y_train))\n",
    "\n",
    "\n",
    "nb_epochs_mlp = 80\n",
    "batch_size_homo = 64\n",
    "mlp_interval = 10\n",
    "mlp_mlp = MLP_alpha_beta_single1().to(device)\n",
    "loss_hist_MLP = train_hybrid(mlp_mlp, x_train, y_train, lr=2e-4, nb_epochs=nb_epochs_mlp, mlp_interval = mlp_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HETERO HYBRID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "init done\n",
      "0.5246167452830188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/pbs.9531024.pbs/ipykernel_3597807/645863402.py:84: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  labels_ = np.array(y,dtype=int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=2.34134\n",
      "ground_loss 2.307302144546444\n",
      "reg_loss 0.03403388401785703\n",
      "Epoch 1: Train= 0.35103 Test Accuracy=0.42448\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 463\u001b[0m\n\u001b[1;32m    461\u001b[0m mlp_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m    462\u001b[0m mlp_mlp \u001b[38;5;241m=\u001b[39m hetero_mlp_a_b_spikes()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 463\u001b[0m loss_hist_MLP \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_hybrid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlp_mlp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnb_epochs_mlp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp_interval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmlp_interval\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 353\u001b[0m, in \u001b[0;36mtrain_hybrid\u001b[0;34m(mlp, x_data, y_data, lr, nb_epochs, mlp_interval)\u001b[0m\n\u001b[1;32m    349\u001b[0m         accs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m#         print(w1)\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m#         print(alpha_hetero_1)\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m#         print(reset_1)\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x_local, y_local \u001b[38;5;129;01min\u001b[39;00m sparse_data_generator_from_hdf5_spikes(x_data, y_data, batch_size_hetero, nb_steps, nb_inputs, max_time):\n\u001b[1;32m    354\u001b[0m                 output, recs \u001b[38;5;241m=\u001b[39m run_snn_hybrid_alpha_beta_spikes_HETERO(inputs\u001b[38;5;241m=\u001b[39mx_local\u001b[38;5;241m.\u001b[39mto_dense(), mlp\u001b[38;5;241m=\u001b[39mmlp, mlp_interval\u001b[38;5;241m=\u001b[39mmlp_interval, batch_size_MLP\u001b[38;5;241m=\u001b[39mbatch_size_hetero)\n\u001b[1;32m    355\u001b[0m                 _ , spks \u001b[38;5;241m=\u001b[39m recs\n",
      "Cell \u001b[0;32mIn[2], line 113\u001b[0m, in \u001b[0;36msparse_data_generator_from_hdf5_spikes\u001b[0;34m(X, y, batch_size, nb_steps, nb_units, max_time, shuffle)\u001b[0m\n\u001b[1;32m    110\u001b[0m     coo[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mextend(units)\n\u001b[1;32m    112\u001b[0m i \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(coo)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 113\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcoo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    115\u001b[0m X_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mFloatTensor(i, v, torch\u001b[38;5;241m.\u001b[39mSize([batch_size,nb_steps,nb_units]))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    116\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(labels_[batch_index],device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "# print(torch.__version__)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "# import torchvision\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "import pickle\n",
    "import math\n",
    "# from utils import get_shd_dataset\n",
    "\n",
    "# The coarse network structure and the time steps are dicated by the SHD dataset.\n",
    "nb_inputs  = 700\n",
    "nb_hidden  = 200\n",
    "nb_outputs = 35\n",
    "\n",
    "time_step = 1e-3\n",
    "nb_steps = 100\n",
    "max_time = 1.4\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "dtype = torch.float\n",
    "\n",
    "# Check whether a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Here we load the Dataset\n",
    "# cache_dir = os.path.expanduser(\"~/data\")\n",
    "# cache_subdir = \"hdspikes\"\n",
    "# get_shd_dataset(cache_dir, cache_subdir)\n",
    "\n",
    "# Here we load the Dataset\n",
    "cache_dir = os.path.expanduser(\"~/data\")\n",
    "cache_subdir = \"ssc_data\"\n",
    "train_file = h5py.File(os.path.join(cache_dir, cache_subdir, 'ssc_train.h5'), 'r')\n",
    "validation_file = h5py.File(os.path.join(cache_dir, cache_subdir, 'ssc_valid.h5'), 'r')\n",
    "test_file = h5py.File(os.path.join(cache_dir, cache_subdir, 'ssc_test.h5'), 'r')\n",
    "\n",
    "x_train = train_file['spikes']\n",
    "y_train = train_file['labels']\n",
    "x_valid = validation_file['spikes']\n",
    "y_valid = validation_file['labels']\n",
    "x_test = test_file['spikes']\n",
    "y_test = test_file['labels']\n",
    "\n",
    "# Here we load the Dataset\n",
    "# cache_dir = os.path.expanduser(\"~/data\")\n",
    "# cache_subdir = \"hdspikes\"\n",
    "# get_shd_dataset(cache_dir, cache_subdir)\n",
    "\n",
    "# train_file = h5py.File(os.path.join(cache_dir, cache_subdir, 'shd_train.h5'), 'r')\n",
    "# test_file = h5py.File(os.path.join(cache_dir, cache_subdir, 'shd_test.h5'), 'r')\n",
    "\n",
    "# x_train = train_file['spikes']\n",
    "# y_train = train_file['labels']\n",
    "# x_test = test_file['spikes']\n",
    "# y_test = test_file['labels']\n",
    "\n",
    "\n",
    "\n",
    "def sparse_data_generator_from_hdf5_spikes(X, y, batch_size, nb_steps, nb_units, max_time, shuffle=True):\n",
    "    \"\"\" This generator takes a spike dataset and generates spiking network input as sparse tensors.\n",
    "\n",
    "    Args:\n",
    "        X: The data ( sample x event x 2 ) the last dim holds (time,neuron) tuples\n",
    "        y: The labels\n",
    "    \"\"\"\n",
    "\n",
    "    labels_ = np.array(y,dtype=int)\n",
    "    number_of_batches = len(labels_)//batch_size\n",
    "    sample_index = np.arange(len(labels_))\n",
    "\n",
    "    # compute discrete firing times\n",
    "    firing_times = X['times']\n",
    "    units_fired = X['units']\n",
    "\n",
    "    time_bins = np.linspace(0, max_time, num=nb_steps)\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "\n",
    "    total_batch_count = 0\n",
    "    counter = 0\n",
    "    while counter<number_of_batches:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "\n",
    "        coo = [ [] for i in range(3) ]\n",
    "        for bc,idx in enumerate(batch_index):\n",
    "            times = np.digitize(firing_times[idx], time_bins)\n",
    "            units = units_fired[idx]\n",
    "            batch = [bc for _ in range(len(times))]\n",
    "\n",
    "            coo[0].extend(batch)\n",
    "            coo[1].extend(times)\n",
    "            coo[2].extend(units)\n",
    "\n",
    "        i = torch.LongTensor(coo).to(device)\n",
    "        v = torch.FloatTensor(np.ones(len(coo[0]))).to(device)\n",
    "\n",
    "        X_batch = torch.sparse.FloatTensor(i, v, torch.Size([batch_size,nb_steps,nb_units])).to(device)\n",
    "        y_batch = torch.tensor(labels_[batch_index],device=device)\n",
    "\n",
    "        yield X_batch.to(device=device), y_batch.to(device=device)\n",
    "\n",
    "        counter += 1\n",
    "        \n",
    "class SurrGradSpike(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Here we implement our spiking nonlinearity which also implements\n",
    "    the surrogate gradient. By subclassing torch.autograd.Function,\n",
    "    we will be able to use all of PyTorch's autograd functionality.\n",
    "    Here we use the normalized negative part of a fast sigmoid\n",
    "    as this was done in Zenke & Ganguli (2018).\n",
    "    \"\"\"\n",
    "\n",
    "    scale = 100.0 # controls steepness of surrogate gradient\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we compute a step function of the input Tensor\n",
    "        and return it. ctx is a context object that we use to stash information which\n",
    "        we need to later backpropagate our error signals. To achieve this we use the\n",
    "        ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        out = torch.zeros_like(input)\n",
    "        out[input > 0] = 1.0\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor we need to compute the\n",
    "        surrogate gradient of the loss with respect to the input.\n",
    "        Here we use the normalized negative part of a fast sigmoid\n",
    "        as this was done in Zenke & Ganguli (2018).\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad = grad_input/(SurrGradSpike.scale*torch.abs(input)+1.0)**2\n",
    "        return grad\n",
    "\n",
    "# here we overwrite our naive spike function by the \"SurrGradSpike\" nonlinearity which implements a surrogate gradient\n",
    "spike_fn  = SurrGradSpike.apply\n",
    "\n",
    "def dist_fn(dist):\n",
    "    return {\n",
    "        'gamma': lambda mean, k, size: np.random.gamma(k, scale=mean/k, size=size),\n",
    "        'normal': lambda mean, k, size: np.random.normal(loc=mean, scale=mean/np.sqrt(k), size=size), #change standard deviation to match gamma\n",
    "        'uniform': lambda _, maximum, size: np.random.uniform(low=0, high=maximum, size=size),\n",
    "    }[dist.lower()]\n",
    "print(\"init done\")\n",
    "\n",
    "\n",
    "class hetero_mlp_a_b_spikes(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(hetero_mlp_a_b_spikes, self).__init__()\n",
    "        self.input_size = 1961+45 #(adding 40 for alpha and beta 2)\n",
    "        self.hidden_size = 2048\n",
    "        self.output_size = 1070 # 200 each for alpha, beta, threshold, reset, rest (adding 40 for alpha and beta 2)\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(self.input_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, self.output_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    def init_weights(self):\n",
    "        with torch.no_grad():\n",
    "            # Initialize first layer weights and biases\n",
    "            self.layers[0].weight.fill_(0)\n",
    "            self.layers[0].bias.fill_(0)\n",
    "\n",
    "            # Pass the first 1000 inputs directly to the hidden layer; the rest to 0\n",
    "            for i in range(1040):\n",
    "                self.layers[0].weight[i, i] = 1\n",
    "\n",
    "            # Initialize second layer weights and biases\n",
    "            self.layers[2].weight.fill_(0)\n",
    "            self.layers[2].bias.fill_(0)\n",
    "\n",
    "            # Pass the first 1000 inputs directly to the hidden layer; the rest to 0\n",
    "            for i in range(1040):\n",
    "                self.layers[2].weight[i, i] = 1\n",
    "                \n",
    "                \n",
    "def run_snn_hybrid_alpha_beta_spikes_HETERO(inputs, mlp, mlp_interval, batch_size_MLP):\n",
    "\n",
    "    # Initialize local copies of alpha, beta, threshold, reset and rest for all 200 hidden neurons\n",
    "    alpha_1_local = alpha_hetero_1\n",
    "    beta_1_local = beta_hetero_1\n",
    "    thresholds_local = thresholds_1\n",
    "    reset_local = reset_1\n",
    "    rest_local = rest_1\n",
    "    alpha_2_local = alpha_hetero_2\n",
    "    beta_2_local = beta_hetero_2\n",
    "    alpha_1_local = alpha_1_local.expand(batch_size_MLP, 200)\n",
    "    beta_1_local = beta_1_local.expand(batch_size_MLP, 200)\n",
    "    thresholds_local = thresholds_local.expand(batch_size_MLP, 200)\n",
    "    reset_local = reset_local.expand(batch_size_MLP, 200)\n",
    "    rest_local = rest_local.expand(batch_size_MLP, 200)\n",
    "    alpha_2_local = alpha_2_local.expand(batch_size_MLP, 35)\n",
    "    beta_2_local = beta_2_local.expand(batch_size_MLP, 35)\n",
    "\n",
    "    # Initialize synaptic and membrane potentials\n",
    "    syn = torch.zeros((batch_size_MLP, nb_hidden), device=device, dtype=dtype)\n",
    "    mem = torch.zeros((batch_size_MLP, nb_hidden), device=device, dtype=dtype)\n",
    "\n",
    "    # Initialize recordings for membrane potentials and spikes\n",
    "    mem_rec = []\n",
    "    spk_rec = []\n",
    "\n",
    "    # Initialize outputs for the hidden layer\n",
    "    out = torch.zeros((batch_size_MLP, nb_hidden), device=device, dtype=dtype)\n",
    "    h1_from_input = torch.einsum(\"abc,cd->abd\", (inputs, w1))\n",
    "\n",
    "    # Prepare readout layer variables\n",
    "    flt2 = torch.zeros((batch_size_MLP, nb_outputs), device=device, dtype=dtype)\n",
    "    out2 = torch.zeros((batch_size_MLP, nb_outputs), device=device, dtype=dtype)\n",
    "    out_rec = [out2]\n",
    "\n",
    "    for t in range(nb_steps):\n",
    "\n",
    "        h1 = h1_from_input[:, t] + torch.einsum(\"ab,bc->ac\", (out, v1))\n",
    "        mthr = mem - thresholds_local\n",
    "        out = spike_fn(mthr)\n",
    "        rst = torch.zeros_like(mem)\n",
    "        c = (mthr > 0)\n",
    "        rst[c] = torch.ones_like(mem)[c]\n",
    "        # rst = out.detach() * reset  # Reset mechanism considering individual reset values\n",
    "\n",
    "        # Update synaptic and membrane potentials\n",
    "        syn = alpha_1_local * syn + h1\n",
    "        mem = beta_1_local * (mem - rest_local) + rest_local + (1 - beta_1_local) * syn - rst * (thresholds_local - reset_local)\n",
    "\n",
    "        # Record membrane potentials and spikes\n",
    "        mem_rec.append(mem)\n",
    "        spk_rec.append(out)\n",
    "\n",
    "        # Now compute h2 on the fly\n",
    "        h2_t = torch.einsum(\"ab,bc->ac\", (out, w2))\n",
    "        flt2 = alpha_2_local * flt2 + h2_t\n",
    "#         out2 = beta_2_local * out2 + flt2\n",
    "        out2 = beta_2_local * out2 + flt2*(1-beta_2_local)\n",
    "        out_rec.append(out2)\n",
    "\n",
    "         # Flatten and concatenate spikes for each item in the batch\n",
    "        input_spikes_flat = inputs[:, t, :].reshape(batch_size_MLP, -1)  # Shape: [batch_size, 700]\n",
    "        hidden_spikes_flat = out.reshape(batch_size_MLP, -1)  # Shape: [batch_size, 200]\n",
    "        output_spikes_flat = out2.reshape(batch_size_MLP, -1)  # Shape: [batch_size, 20]\n",
    "\n",
    "        # Time tensor\n",
    "        time_tensor = torch.full((batch_size_MLP, 1), t, device=device, dtype=dtype)\n",
    "\n",
    "        # Concatenate tensors\n",
    "        mlp_input = torch.cat([\n",
    "            alpha_1_local, beta_1_local, thresholds_local, reset_local, rest_local,\n",
    "            alpha_2_local, beta_2_local,\n",
    "            time_tensor,\n",
    "            input_spikes_flat, hidden_spikes_flat, output_spikes_flat\n",
    "        ], dim=1)\n",
    "\n",
    "\n",
    "        if t % mlp_interval == 0:\n",
    "            mlp_outputs = mlp(mlp_input)\n",
    "            # Update alpha_local and beta_local based on MLP outputs\n",
    "            alpha_1_local, beta_1_local = mlp_outputs[:, :200], mlp_outputs[:, 200:400]\n",
    "            threshold_local = mlp_outputs[:, 400:600] + 0.5\n",
    "            reset_local, rest_local = mlp_outputs[:, 600:800] - 0.5, mlp_outputs[:, 800:1000] - 0.5\n",
    "#             print(beta_2_local.shape)\n",
    "            alpha_2_local, beta_2_local = mlp_outputs[:, 1000:1035], mlp_outputs[:, 1035:1070]\n",
    "#             print(beta_2_local.shape)\n",
    "\n",
    "    # Stack recordings for output\n",
    "    mem_rec = torch.stack(mem_rec, dim=1).to(device)\n",
    "    spk_rec = torch.stack(spk_rec, dim=1).to(device)\n",
    "    out_rec = torch.stack(out_rec[1:], dim=1).to(device)  # Skip the initial zero tensor\n",
    "\n",
    "    other_recs = [mem_rec, spk_rec]\n",
    "\n",
    "    return out_rec, other_recs\n",
    "\n",
    "\n",
    "def compute_classification_accuracy_MLP(x_data, y_data, mlp, mlp_interval):\n",
    "    \"\"\" Computes classification accuracy on supplied data in batches. \"\"\"\n",
    "    accs = []\n",
    "    with torch.no_grad():\n",
    "        for x_local, y_local in sparse_data_generator_from_hdf5_spikes(x_data, y_data, batch_size_hetero, nb_steps, nb_inputs, max_time, shuffle=False):\n",
    "            output, _ = run_snn_hybrid_alpha_beta_spikes_HETERO(x_local.to_dense(),  mlp=mlp, mlp_interval=mlp_interval, batch_size_MLP=batch_size_hetero)\n",
    "            m,_= torch.max(output,1) # max over time\n",
    "            _,am=torch.max(m,1)      # argmax over output units\n",
    "            tmp = np.mean((y_local==am).detach().cpu().numpy()) # compare to labels\n",
    "            accs.append(tmp)\n",
    "    return np.mean(accs)\n",
    "\n",
    "def train_hybrid(mlp, x_data, y_data, lr=1e-3, nb_epochs=10, mlp_interval=10):\n",
    "\n",
    "    snn_params = [w1, w2, v1, \n",
    "                  alpha_hetero_1, beta_hetero_1,\n",
    "                  thresholds_1, reset_1, rest_1,\n",
    "                  alpha_hetero_2, beta_hetero_2]\n",
    "#  \n",
    "\n",
    "    # Optimizers\n",
    "    combined_params = [\n",
    "        {'params': snn_params, 'lr': lr},  # Parameters for SNN with specific learning rate\n",
    "        {'params': mlp.parameters(), 'lr': lr}  # Parameters for MLP with its own learning rate\n",
    "    ]\n",
    "\n",
    "    # Using a single optimizer for both SNN and MLP\n",
    "    combined_optimizer = torch.optim.Adam(combined_params)\n",
    "\n",
    "\n",
    "    #Loss functions\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    log_softmax_fn = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    best_accuracy = 0\n",
    "\n",
    "    loss_hist = []\n",
    "    train_acc_hist = []\n",
    "    test_acc_hist = []\n",
    "    for epoch in range(nb_epochs):\n",
    "        local_loss = []\n",
    "        local_ground_loss = []\n",
    "        local_reg_loss = []\n",
    "        accs = []\n",
    "#         print(w1)\n",
    "#         print(alpha_hetero_1)\n",
    "#         print(reset_1)\n",
    "        for x_local, y_local in sparse_data_generator_from_hdf5_spikes(x_data, y_data, batch_size_hetero, nb_steps, nb_inputs, max_time):\n",
    "                output, recs = run_snn_hybrid_alpha_beta_spikes_HETERO(inputs=x_local.to_dense(), mlp=mlp, mlp_interval=mlp_interval, batch_size_MLP=batch_size_hetero)\n",
    "                _ , spks = recs\n",
    "                m, _ = torch.max(output,1)\n",
    "\n",
    "                _,am=torch.max(m,1)      # argmax over output units\n",
    "                tmp = np.mean((y_local==am).detach().cpu().numpy()) # compare to labels\n",
    "                accs.append(tmp)\n",
    "\n",
    "                log_p_y = log_softmax_fn(m)\n",
    "                ground_loss = loss_fn(log_p_y, y_local)\n",
    "                reg_loss = 1e-6*torch.sum(spks) # L1 loss on total number of spikes\n",
    "                reg_loss += 1e-6*torch.mean(torch.sum(torch.sum(spks,dim=0),dim=0)**2) # L2 loss on spikes per neuron\n",
    "\n",
    "                loss_MLP = ground_loss + reg_loss\n",
    "\n",
    "                combined_optimizer.zero_grad()\n",
    "                loss_MLP.backward()\n",
    "                combined_optimizer.step()\n",
    "                \n",
    "                # Clamping the values\n",
    "                with torch.no_grad():\n",
    "                    alpha_hetero_1.clamp_(0.367, 0.995)\n",
    "                    beta_hetero_1.clamp_(0.367, 0.995)\n",
    "                    alpha_hetero_2.clamp_(0.367, 0.995)\n",
    "                    beta_hetero_2.clamp_(0.367, 0.995)\n",
    "                    thresholds_1.clamp_(0.5, 1.5)\n",
    "\n",
    "                local_loss.append(loss_MLP.item())\n",
    "                local_ground_loss.append(ground_loss.item())\n",
    "                local_reg_loss.append(reg_loss.item())\n",
    "\n",
    "\n",
    "        mean_loss = np.mean(local_loss)\n",
    "        loss_hist.append(mean_loss)\n",
    "        print(f\"Epoch {epoch+1}: loss={mean_loss:.5f}\")\n",
    "        print(\"ground_loss\", np.mean(local_ground_loss))\n",
    "        print(\"reg_loss\", np.mean(local_reg_loss))\n",
    "        train_accuracy = np.mean(accs)\n",
    "        test_accuracy = compute_classification_accuracy_MLP(x_test, y_test, mlp, mlp_interval)\n",
    "        train_acc_hist.append(train_accuracy)\n",
    "        test_acc_hist.append(test_accuracy)\n",
    "        print(f\"Epoch {epoch + 1}: Train= {train_accuracy:.5f} Test Accuracy={test_accuracy:.5f}\")\n",
    "\n",
    "        # Print the best accuracy so far\n",
    "        if test_accuracy > best_accuracy:\n",
    "            best_accuracy = test_accuracy\n",
    "\n",
    "            directory = 'Hybrid_Hetero'\n",
    "\n",
    "            # Create the directory if it does not exist\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "\n",
    "            best_model_state = mlp_mlp.state_dict()\n",
    "\n",
    "            # Save the file in the specified directory\n",
    "            file_path = os.path.join(directory, 'mlp.pt')\n",
    "            torch.save(best_model_state, file_path)\n",
    "\n",
    "            # Save parameters only when a new best accuracy is achieved\n",
    "            # Create a dictionary of current parameters to save\n",
    "            saved_params_hetero = {\n",
    "                'w1': w1.clone(),\n",
    "                'w2': w2.clone(),\n",
    "                'v1': v1.clone(),\n",
    "                'alpha': alpha_hetero_1.clone(),\n",
    "                'beta': beta_hetero_1.clone(),\n",
    "                'threshold': thresholds_1.clone(),\n",
    "                'reset': reset_1.clone(),\n",
    "                'rest': rest_1.clone(),\n",
    "                'alpha_2': alpha_hetero_2.clone(),\n",
    "                'beta_2': beta_hetero_2.clone()\n",
    "            }\n",
    "\n",
    "            # Save the file in the specified directory\n",
    "            file_path = os.path.join(directory, 'snn.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'accuracy': best_accuracy,\n",
    "                'params': saved_params_hetero,\n",
    "                'loss': loss_hist,\n",
    "                'train_acc_hist': train_acc_hist,\n",
    "                'test_acc_hist': test_acc_hist\n",
    "            }, file_path)\n",
    "        else:\n",
    "            print(\"Best\", best_accuracy)\n",
    "\n",
    "    return loss_hist\n",
    "\n",
    "loaded_weights_snn = torch.load('SSC_test_2/epochs_hetero/snn_15.pth')\n",
    "\n",
    "w1 = torch.nn.Parameter(loaded_weights_snn['params']['w1'].to(device))\n",
    "w2 = torch.nn.Parameter(loaded_weights_snn['params']['w2'].to(device))\n",
    "v1 = torch.nn.Parameter(loaded_weights_snn['params']['v1'].to(device))\n",
    "alpha_hetero_1 = torch.nn.Parameter(loaded_weights_snn['params']['alpha'].to(device))\n",
    "beta_hetero_1 = torch.nn.Parameter(loaded_weights_snn['params']['beta'].to(device))\n",
    "thresholds_1 = torch.nn.Parameter(loaded_weights_snn['params']['threshold'].to(device))\n",
    "reset_1 = torch.nn.Parameter(loaded_weights_snn['params']['reset'].to(device))\n",
    "rest_1 = torch.nn.Parameter(loaded_weights_snn['params']['rest'].to(device))\n",
    "alpha_hetero_2 = torch.nn.Parameter(loaded_weights_snn['params']['alpha_2'].to(device))\n",
    "beta_hetero_2 = torch.nn.Parameter(loaded_weights_snn['params']['beta_2'].to(device))\n",
    "print(loaded_weights_snn['accuracy'])\n",
    "# print(compute_classification_accuracy_hetero(x_train, y_train))\n",
    "\n",
    "\n",
    "nb_epochs_mlp = 80\n",
    "batch_size_hetero = 64\n",
    "mlp_interval = 10\n",
    "mlp_mlp = hetero_mlp_a_b_spikes().to(device)\n",
    "loss_hist_MLP = train_hybrid(mlp_mlp, x_train, y_train, lr=1e-3, nb_epochs=nb_epochs_mlp, mlp_interval = mlp_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double HOMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/pbs.9564545.pbs/ipykernel_57374/4234174762.py:51: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  labels_ = np.array(y,dtype=int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=1.41865\n",
      "ground_loss 1.4065279819583165\n",
      "reg_loss 0.012126956692722209\n",
      "Epoch 1: Train= 0.60949 Test Accuracy=0.57184\n",
      "best_accuracy 0.5718356918238994\n",
      "Epoch 2: loss=1.43688\n",
      "ground_loss 1.4250233568206898\n",
      "reg_loss 0.011852954442441412\n",
      "Epoch 2: Train= 0.59999 Test Accuracy=0.58608\n",
      "best_accuracy 0.5860849056603774\n",
      "Epoch 5: loss=1.44366\n",
      "ground_loss 1.4241916570145767\n",
      "reg_loss 0.019465153803456453\n",
      "Epoch 5: Train= 0.59999 Test Accuracy=0.59547\n",
      "best_accuracy 0.5954697327044025\n",
      "Epoch 6: loss=1.33269\n",
      "ground_loss 1.3222405119807767\n",
      "reg_loss 0.010447952909785322\n",
      "Epoch 6: Train= 0.63031 Test Accuracy=0.60122\n",
      "best_accuracy 0.6012185534591195\n",
      "Epoch 7: loss=1.46227\n",
      "ground_loss 1.453848522785258\n",
      "reg_loss 0.008423414372422743\n",
      "Epoch 7: Train= 0.59686 Test Accuracy=0.56314\n",
      "best_accuracy 0.6012185534591195\n",
      "Epoch 15: loss=1.18872\n",
      "ground_loss 1.1815861485791064\n",
      "reg_loss 0.00713437555580573\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "# print(torch.__version__)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "# import torchvision\n",
    "from torch.utils import data\n",
    "\n",
    "import pickle\n",
    "from utils import get_shd_dataset\n",
    "\n",
    "dtype = torch.float\n",
    "\n",
    "# Check whether a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Here we load the Dataset\n",
    "cache_dir = os.path.expanduser(\"~/data\")\n",
    "cache_subdir = \"ssc_data\"\n",
    "train_file = h5py.File(os.path.join(cache_dir, cache_subdir, 'ssc_train.h5'), 'r')\n",
    "validation_file = h5py.File(os.path.join(cache_dir, cache_subdir, 'ssc_valid.h5'), 'r')\n",
    "test_file = h5py.File(os.path.join(cache_dir, cache_subdir, 'ssc_test.h5'), 'r')\n",
    "\n",
    "x_train = train_file['spikes']\n",
    "y_train = train_file['labels']\n",
    "x_valid = validation_file['spikes']\n",
    "y_valid = validation_file['labels']\n",
    "x_test = test_file['spikes']\n",
    "y_test = test_file['labels']\n",
    "\n",
    "def sparse_data_generator_from_hdf5_spikes(X, y, batch_size, nb_steps, nb_units, max_time, shuffle=True):\n",
    "    \"\"\" This generator takes a spike dataset and generates spiking network input as sparse tensors.\n",
    "\n",
    "    Args:\n",
    "        X: The data ( sample x event x 2 ) the last dim holds (time,neuron) tuples\n",
    "        y: The labels\n",
    "    \"\"\"\n",
    "\n",
    "    labels_ = np.array(y,dtype=int)\n",
    "    number_of_batches = len(labels_)//batch_size\n",
    "    sample_index = np.arange(len(labels_))\n",
    "\n",
    "    # compute discrete firing times\n",
    "    firing_times = X['times']\n",
    "    units_fired = X['units']\n",
    "\n",
    "    time_bins = np.linspace(0, max_time, num=nb_steps)\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "\n",
    "    total_batch_count = 0\n",
    "    counter = 0\n",
    "    while counter<number_of_batches:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "\n",
    "        coo = [ [] for i in range(3) ]\n",
    "        for bc,idx in enumerate(batch_index):\n",
    "            times = np.digitize(firing_times[idx], time_bins)\n",
    "            units = units_fired[idx]\n",
    "            batch = [bc for _ in range(len(times))]\n",
    "\n",
    "            coo[0].extend(batch)\n",
    "            coo[1].extend(times)\n",
    "            coo[2].extend(units)\n",
    "\n",
    "        i = torch.LongTensor(coo).to(device)\n",
    "        v = torch.FloatTensor(np.ones(len(coo[0]))).to(device)\n",
    "\n",
    "        X_batch = torch.sparse.FloatTensor(i, v, torch.Size([batch_size,nb_steps,nb_units])).to(device)\n",
    "        y_batch = torch.tensor(labels_[batch_index],device=device)\n",
    "\n",
    "        yield X_batch.to(device=device), y_batch.to(device=device)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "class SurrGradSpike(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Here we implement our spiking nonlinearity which also implements\n",
    "    the surrogate gradient. By subclassing torch.autograd.Function,\n",
    "    we will be able to use all of PyTorch's autograd functionality.\n",
    "    Here we use the normalized negative part of a fast sigmoid\n",
    "    as this was done in Zenke & Ganguli (2018).\n",
    "    \"\"\"\n",
    "\n",
    "    scale = 100.0 # controls steepness of surrogate gradient\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we compute a step function of the input Tensor\n",
    "        and return it. ctx is a context object that we use to stash information which\n",
    "        we need to later backpropagate our error signals. To achieve this we use the\n",
    "        ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        out = torch.zeros_like(input)\n",
    "        out[input > 0] = 1.0\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor we need to compute the\n",
    "        surrogate gradient of the loss with respect to the input.\n",
    "        Here we use the normalized negative part of a fast sigmoid\n",
    "        as this was done in Zenke & Ganguli (2018).\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad = grad_input/(SurrGradSpike.scale*torch.abs(input)+1.0)**2\n",
    "        return grad\n",
    "\n",
    "# here we overwrite our naive spike function by the \"SurrGradSpike\" nonlinearity which implements a surrogate gradient\n",
    "spike_fn  = SurrGradSpike.apply\n",
    "\n",
    "tau_syn = 10e-3\n",
    "tau_mem = 20e-3\n",
    "\n",
    "time_step = 1e-3\n",
    "nb_steps = 100\n",
    "max_time = 1.4\n",
    "\n",
    "s_batch_size = 64\n",
    "\n",
    "# SNN 1 Network\n",
    "s1_nb_inputs  = 700\n",
    "s1_nb_hidden  = 200\n",
    "s1_nb_outputs = 35\n",
    "\n",
    "# SNN 2 Network\n",
    "s2_nb_inputs = s1_nb_hidden\n",
    "s2_nb_hidden = 14\n",
    "\n",
    "\n",
    "def run_double_snn_homo(s1_inputs, s2_interval):\n",
    "    s1_alpha_1_local = s1_alpha_homo_1\n",
    "    s1_beta_1_local = s1_beta_homo_1\n",
    "    s1_thresholds_local = s1_thresholds_1\n",
    "    s1_reset_local = s1_reset_1\n",
    "    s1_rest_local = s1_rest_1\n",
    "    s1_alpha_2_local = s1_alpha_homo_2\n",
    "    s1_beta_2_local = s1_beta_homo_2\n",
    "    s1_alpha_1_local = s1_alpha_1_local.expand(s_batch_size, 1)\n",
    "    s1_beta_1_local = s1_beta_1_local.expand(s_batch_size, 1)\n",
    "    s1_thresholds_local = s1_thresholds_local.expand(s_batch_size, 1)\n",
    "    s1_reset_local = s1_reset_local.expand(s_batch_size, 1)\n",
    "    s1_rest_local = s1_rest_local.expand(s_batch_size, 1)\n",
    "    s1_alpha_2_local = s1_alpha_2_local.expand(s_batch_size, 1)\n",
    "    s1_beta_2_local = s1_beta_2_local.expand(s_batch_size, 1)\n",
    "\n",
    "    # Initialize synaptic and membrane potentials for SNN 1\n",
    "    s1_syn = torch.zeros((s_batch_size, s1_nb_hidden), device=device, dtype=dtype)\n",
    "    s1_mem = torch.zeros((s_batch_size, s1_nb_hidden), device=device, dtype=dtype)\n",
    "\n",
    "    # Initialize recordings for membrane potentials and spikes for SNN 1\n",
    "    s1_mem_rec = []\n",
    "    s1_spk_rec = []\n",
    "\n",
    "    # Initialize outputs for the hidden layer for SNN 1\n",
    "    s1_out = torch.zeros((s_batch_size, s1_nb_hidden), device=device, dtype=dtype)\n",
    "    s1_h1_from_input = torch.einsum(\"abc,cd->abd\", (s1_inputs, s1_w1))\n",
    "\n",
    "    # Prepare readout layer variables for SNN 1\n",
    "    s1_flt2 = torch.zeros((s_batch_size, s1_nb_outputs), device=device, dtype=dtype)\n",
    "    s1_out2 = torch.zeros((s_batch_size, s1_nb_outputs), device=device, dtype=dtype)\n",
    "    s1_out_rec = [s1_out2]\n",
    "\n",
    "    # Initialize synaptic and membrane potentials for SNN 2\n",
    "    s2_syn = torch.zeros((s_batch_size, s2_nb_hidden), device=device, dtype=dtype)\n",
    "    s2_mem = torch.zeros((s_batch_size, s2_nb_hidden), device=device, dtype=dtype)\n",
    "\n",
    "    # Initialize recordings for membrane potentials and spikes for SNN 2\n",
    "    s2_mem_rec = []\n",
    "    s2_spk_rec = []\n",
    "\n",
    "    # Initialize outputs for the hidden layer for SNN 2\n",
    "    s2_out = torch.zeros((s_batch_size, s2_nb_hidden), device=device, dtype=dtype)\n",
    "\n",
    "    # Buffer to store spikes from the last s2_interval timesteps for SNN 2\n",
    "    s2_spike_buffer = torch.zeros((s2_interval, s_batch_size, s2_nb_hidden), device=device, dtype=dtype)\n",
    "\n",
    "    for s1_t in range(nb_steps):\n",
    "        # Compute hidden layer activity for SNN 1\n",
    "        s1_h1 = s1_h1_from_input[:, s1_t] + torch.einsum(\"ab,bc->ac\", (s1_out, s1_v1))\n",
    "        s1_mthr = s1_mem - s1_thresholds_local\n",
    "        s1_out = spike_fn(s1_mthr)\n",
    "        s1_rst = torch.zeros_like(s1_mem)\n",
    "        c = (s1_mthr > 0)\n",
    "        s1_rst[c] = torch.ones_like(s1_mem)[c]\n",
    "\n",
    "        s1_syn = s1_alpha_1_local * s1_syn + s1_h1  # Reshape and expand to match s1_mem\n",
    "        s1_mem = s1_beta_1_local * (s1_mem - s1_rest_local) + s1_rest_local + (1 - s1_beta_1_local) * s1_syn - s1_rst * (s1_thresholds_local - s1_reset_local)\n",
    "\n",
    "        s1_mem_rec.append(s1_mem)\n",
    "        s1_spk_rec.append(s1_out)\n",
    "\n",
    "        # Compute output for the readout layer for SNN 1\n",
    "        s1_h2_t = torch.einsum(\"ab,bc->ac\", (s1_out, s1_w2))\n",
    "        s1_flt2 = s1_alpha_2_local * s1_flt2 + s1_h2_t  # Reshape and expand to match s1_flt2\n",
    "        s1_out2 = s1_beta_2_local * s1_out2 + (1 - s1_beta_2_local) * s1_flt2  # Reshape and expand to match s1_flt2\n",
    "\n",
    "        s1_out_rec.append(s1_out2)\n",
    "\n",
    "        # Compute hidden layer activity for SNN 2 using spikes from SNN 1 as input\n",
    "        s2_h1 = torch.einsum(\"ab,bc->ac\", (s1_out, s2_w1))\n",
    "        s2_mthr = s2_mem - s2_thresholds_1  # Reshape and expand to match s2_mem\n",
    "        s2_out = spike_fn(s2_mthr)\n",
    "        s2_rst = torch.zeros_like(s2_mem)\n",
    "        c = (s2_mthr > 0)\n",
    "        s2_rst[c] = torch.ones_like(s2_mem)[c]\n",
    "\n",
    "        s2_new_syn = s2_alpha_homo_1 * s2_syn + s2_h1  # Reshape and expand to match s2_syn\n",
    "        s2_new_mem = s2_beta_homo_1 * (s2_mem - s2_rest_1) + s2_rest_1 + (1 - s2_beta_homo_1) * s2_syn - s2_rst * (s2_thresholds_1 - s2_reset_1)\n",
    "\n",
    "        s2_mem_rec.append(s2_mem)\n",
    "        s2_spk_rec.append(s2_out)\n",
    "\n",
    "        s2_mem = s2_new_mem\n",
    "        s2_syn = s2_new_syn\n",
    "\n",
    "        # Update the spike buffer\n",
    "        s2_spike_buffer[s1_t % s2_interval] = s2_out\n",
    "\n",
    "        # Use spikes from the last s2_interval timesteps to modulate parameters of SNN 1\n",
    "        if s1_t > s2_interval and s1_t % s2_interval == 0:\n",
    "            # Aggregate spikes over the last s2_interval timesteps\n",
    "            recent_spikes = s2_spike_buffer.sum(dim=0)\n",
    "\n",
    "            # Neurons for modulating s1_alpha_homo_1\n",
    "            mod_alpha_1_pos = torch.sum(recent_spikes[:, 0:1], dim=1, keepdim=True)\n",
    "            mod_alpha_1_neg = torch.sum(recent_spikes[:, 1:2], dim=1, keepdim=True)\n",
    "\n",
    "            # Neurons for modulating s1_beta_homo_1\n",
    "            mod_beta_1_pos = torch.sum(recent_spikes[:, 2:3], dim=1, keepdim=True)\n",
    "            mod_beta_1_neg = torch.sum(recent_spikes[:, 3:4], dim=1, keepdim=True)\n",
    "\n",
    "            # Neurons for modulating s1_alpha_homo_2\n",
    "            mod_alpha_2_pos = torch.sum(recent_spikes[:, 4:5], dim=1, keepdim=True)\n",
    "            mod_alpha_2_neg = torch.sum(recent_spikes[:, 5:6], dim=1, keepdim=True)\n",
    "\n",
    "            # Neurons for modulating s1_beta_homo_2\n",
    "            mod_beta_2_pos = torch.sum(recent_spikes[:, 6:7], dim=1, keepdim=True)\n",
    "            mod_beta_2_neg = torch.sum(recent_spikes[:, 7:8], dim=1, keepdim=True)\n",
    "\n",
    "            # Neurons for modulating s1_thresholds_1\n",
    "            mod_thresholds_pos = torch.sum(recent_spikes[:, 8:9], dim=1, keepdim=True)\n",
    "            mod_thresholds_neg = torch.sum(recent_spikes[:, 9:10], dim=1, keepdim=True)\n",
    "\n",
    "            # Neurons for modulating s1_reset_1\n",
    "            mod_reset_pos = torch.sum(recent_spikes[:, 10:11], dim=1, keepdim=True)\n",
    "            mod_reset_neg = torch.sum(recent_spikes[:, 11:12], dim=1, keepdim=True)\n",
    "\n",
    "            # Neurons for modulating s1_rest_1\n",
    "            mod_rest_pos = torch.sum(recent_spikes[:, 12:13], dim=1, keepdim=True)\n",
    "            mod_rest_neg = torch.sum(recent_spikes[:, 13:14], dim=1, keepdim=True)\n",
    "\n",
    "            # Calculate modulation effects\n",
    "            mod_alpha_1_effect = mod_alpha_1_pos * s2_alpha_mod_factor1_pos - mod_alpha_1_neg * s2_alpha_mod_factor1_neg\n",
    "            mod_beta_1_effect = mod_beta_1_pos * s2_beta_mod_factor1_pos - mod_beta_1_neg * s2_beta_mod_factor1_neg\n",
    "            mod_alpha_2_effect = mod_alpha_2_pos * s2_alpha_mod_factor2_pos - mod_alpha_2_neg * s2_alpha_mod_factor2_neg\n",
    "            mod_beta_2_effect = mod_beta_2_pos * s2_beta_mod_factor2_pos - mod_beta_2_neg * s2_beta_mod_factor2_neg\n",
    "            mod_thresholds_effect = mod_thresholds_pos * s2_thresholds_mod_factor_pos - mod_thresholds_neg * s2_thresholds_mod_factor_neg\n",
    "            mod_reset_effect = mod_reset_pos * s2_reset_mod_factor_pos - mod_reset_neg * s2_reset_mod_factor_neg\n",
    "            mod_rest_effect = mod_rest_pos * s2_rest_mod_factor_pos - mod_rest_neg * s2_rest_mod_factor_neg\n",
    "\n",
    "            # Update the actual parameters using in-place operations\n",
    "            s1_alpha_1_local = s1_alpha_1_local + mod_alpha_1_effect\n",
    "            s1_beta_1_local = s1_beta_1_local + mod_beta_1_effect\n",
    "            s1_alpha_2_local = s1_alpha_2_local + mod_alpha_2_effect\n",
    "            s1_beta_2_local = s1_beta_2_local + mod_beta_2_effect\n",
    "            s1_thresholds_local = s1_thresholds_local + mod_thresholds_effect\n",
    "            s1_reset_local = s1_reset_local+ mod_reset_effect\n",
    "            s1_rest_local = s1_rest_local + mod_rest_effect\n",
    "\n",
    "            # Ensure modulation effects are stable\n",
    "            s1_alpha_1_local = torch.clamp(s1_alpha_1_local, min=0.367, max=0.995)\n",
    "            s1_beta_1_local = torch.clamp(s1_beta_1_local, min=0.367, max=0.995)\n",
    "            s1_alpha_2_local = torch.clamp(s1_alpha_2_local, min=0.367, max=0.995)\n",
    "            s1_beta_2_local = torch.clamp(s1_beta_2_local, min=0.367, max=0.995)\n",
    "            s1_thresholds_local = torch.clamp(s1_thresholds_local, min=0.5, max=1.5)\n",
    "            s1_reset_local = torch.clamp(s1_reset_local, min=-0.5, max=0.5)\n",
    "            s1_rest_local = torch.clamp(s1_rest_local, min=-0.5, max=0.5)\n",
    "\n",
    "    # Stack recordings for output\n",
    "    s1_mem_rec = torch.stack(s1_mem_rec, dim=1)\n",
    "    s1_spk_rec = torch.stack(s1_spk_rec, dim=1)\n",
    "    s1_out_rec = torch.stack(s1_out_rec[1:], dim=1)  # Skip the initial zero tensor\n",
    "\n",
    "    s1_other_recs = [s1_mem_rec, s1_spk_rec]\n",
    "\n",
    "    return s1_out_rec, s1_other_recs\n",
    "\n",
    "def train_double_snn(x_data, y_data, lr=1e-3, nb_epochs=10, s2_interval=10):\n",
    "    # Include all learnable parameters in the list\n",
    "    params = [\n",
    "        s1_w1, s1_w2, s1_v1,\n",
    "        s2_w1, s2_v1,\n",
    "        s2_alpha_mod_factor1_pos, s2_alpha_mod_factor1_neg,\n",
    "        s2_beta_mod_factor1_pos, s2_beta_mod_factor1_neg,\n",
    "        s2_alpha_mod_factor2_pos, s2_alpha_mod_factor2_neg,\n",
    "        s2_beta_mod_factor2_pos, s2_beta_mod_factor2_neg,\n",
    "        s2_thresholds_mod_factor_pos, s2_thresholds_mod_factor_neg,\n",
    "        s2_reset_mod_factor_pos, s2_reset_mod_factor_neg,\n",
    "        s2_rest_mod_factor_pos, s2_rest_mod_factor_neg,\n",
    "        s1_thresholds_1, s1_reset_1, s1_rest_1,\n",
    "        s1_alpha_homo_1, s1_beta_homo_1,\n",
    "        s1_alpha_homo_2, s1_beta_homo_2,\n",
    "        s2_thresholds_1, s2_reset_1, s2_rest_1,\n",
    "        s2_alpha_homo_1, s2_beta_homo_1\n",
    "    ]\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.Adam(params, lr=lr, betas=(0.9, 0.999))\n",
    "\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    log_softmax_fn = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    best_accuracy = 0\n",
    "    loss_hist = []\n",
    "    train_acc_hist = []\n",
    "    test_acc_hist = []\n",
    "\n",
    "    for epoch in range(nb_epochs):\n",
    "        local_loss = []\n",
    "        local_ground_loss = []\n",
    "        local_reg_loss = []\n",
    "        accs = []\n",
    "#         print(s1_w1)\n",
    "#         print(s2_alpha_mod_factor2_pos)\n",
    "#         print(s1_thresholds_1)\n",
    "#         print(s2_alpha_homo_1)\n",
    "\n",
    "        for x_local, y_local in sparse_data_generator_from_hdf5_spikes(x_data, y_data, s_batch_size, nb_steps, s1_nb_inputs, max_time):\n",
    "            # Ensure x_local is detached from any previous computation graph\n",
    "            x_local = x_local.to_dense().detach()\n",
    "\n",
    "            output, recs = run_double_snn_homo(x_local, s2_interval=s2_interval)\n",
    "            _, spks = recs\n",
    "            m, _ = torch.max(output, 1)\n",
    "\n",
    "            _, am = torch.max(m, 1)  # argmax over output units\n",
    "            tmp = np.mean((y_local == am).detach().cpu().numpy())  # compare to labels\n",
    "            accs.append(tmp)\n",
    "\n",
    "            log_p_y = log_softmax_fn(m)\n",
    "            ground_loss = loss_fn(log_p_y, y_local)\n",
    "\n",
    "            # L2 loss on spikes per neuron\n",
    "#             reg_loss = 1e-6 * torch.sum(spks)  # L1 loss on total number of spikes\n",
    "#             reg_loss += 1e-6 * torch.mean(torch.sum(torch.sum(spks, dim=0), dim=0)**2)  # L2 loss on spikes per neuron\n",
    "            tot_num_neurons = nb_hidden + nb_outputs\n",
    "            N_samp = 6412\n",
    "            T = nb_steps\n",
    "            sl = 1\n",
    "            thetal = 0.01\n",
    "            su = 0.06\n",
    "            thetau = 100\n",
    "            tmp = (torch.clamp((1 / T) * torch.sum(spks, 1) - thetal, min=0.)) ** 2\n",
    "            L1_batch = torch.sum(tmp, (0, 1))\n",
    "            reg_loss = (sl / (N_samp + tot_num_neurons)) * L1_batch\n",
    "\n",
    "            tmp2 = (torch.clamp((1 / nb_hidden) * torch.sum(spks, (1, 2)) - thetau, min=0.)) ** 2\n",
    "            L2_batch = torch.sum(tmp2)\n",
    "            reg_loss += (su / N_samp) * L2_batch\n",
    "            loss_val = ground_loss + reg_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "            local_loss.append(loss_val.item())\n",
    "            local_ground_loss.append(ground_loss.item())\n",
    "            local_reg_loss.append(reg_loss.item())\n",
    "\n",
    "        mean_loss = np.mean(local_loss)\n",
    "        loss_hist.append(mean_loss)\n",
    "        print(f\"Epoch {epoch + 1}: loss={mean_loss:.5f}\")\n",
    "        print(\"ground_loss\", np.mean(local_ground_loss))\n",
    "        print(\"reg_loss\", np.mean(local_reg_loss))\n",
    "        \n",
    "        train_accuracy = np.mean(accs)\n",
    "        test_accuracy = compute_classification_accuracy_double(x_test, y_test, s2_interval)\n",
    "        train_acc_hist.append(train_accuracy)\n",
    "        test_acc_hist.append(test_accuracy)\n",
    "        print(f\"Epoch {epoch + 1}: Train= {train_accuracy:.5f} Test Accuracy={test_accuracy:.5f}\")\n",
    "\n",
    "        if test_accuracy > best_accuracy:\n",
    "            best_accuracy = test_accuracy\n",
    "            print(\"best_accuracy\", best_accuracy)\n",
    "\n",
    "            # Create a dictionary of curent parameters to save\n",
    "            saved_weights_snn = {\n",
    "                's1_w1': s1_w1,\n",
    "                's1_w2': s1_w2,\n",
    "                's1_v1': s1_v1,\n",
    "                's2_w1': s2_w1,\n",
    "                's2_v1': s2_v1,\n",
    "                's2_alpha_mod_factor1_pos': s2_alpha_mod_factor1_pos,\n",
    "                's2_alpha_mod_factor1_neg': s2_alpha_mod_factor1_neg,\n",
    "                's2_beta_mod_factor1_pos': s2_beta_mod_factor1_pos,\n",
    "                's2_beta_mod_factor1_neg': s2_beta_mod_factor1_neg,\n",
    "                's2_alpha_mod_factor2_pos': s2_alpha_mod_factor2_pos,\n",
    "                's2_alpha_mod_factor2_neg': s2_alpha_mod_factor2_neg,\n",
    "                's2_beta_mod_factor2_pos': s2_beta_mod_factor2_pos,\n",
    "                's2_beta_mod_factor2_neg': s2_beta_mod_factor2_neg,\n",
    "                's2_thresholds_mod_factor_pos': s2_thresholds_mod_factor_pos,\n",
    "                's2_thresholds_mod_factor_neg': s2_thresholds_mod_factor_neg,\n",
    "                's2_reset_mod_factor_pos': s2_reset_mod_factor_pos,\n",
    "                's2_reset_mod_factor_neg': s2_reset_mod_factor_neg,\n",
    "                's2_rest_mod_factor_pos': s2_rest_mod_factor_pos,\n",
    "                's2_rest_mod_factor_neg': s2_rest_mod_factor_neg,\n",
    "                's1_thresholds_1': s1_thresholds_1,\n",
    "                's1_reset_1': s1_reset_1,\n",
    "                's1_rest_1': s1_rest_1,\n",
    "                's2_thresholds_1': s2_thresholds_1,\n",
    "                's2_reset_1': s2_reset_1,\n",
    "                's2_rest_1': s2_rest_1,\n",
    "                's2_alpha_homo_1': s2_alpha_homo_1,\n",
    "                's2_beta_homo_1': s2_beta_homo_1,\n",
    "                's1_alpha_homo_1': s1_alpha_homo_1,\n",
    "                's1_beta_homo_1': s1_beta_homo_1,\n",
    "                's1_alpha_homo_2': s1_alpha_homo_2,\n",
    "                's1_beta_homo_2': s1_beta_homo_2\n",
    "            }\n",
    "            \n",
    "            directory = 'SSC_Double_Homo'\n",
    "\n",
    "            # Create the directory if it does not exist\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "\n",
    "            # Save parameters along with the current epoch and accuracy\n",
    "            file_path = os.path.join(directory, 'best_double.pth')\n",
    "\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'accuracy': test_accuracy,\n",
    "                'params': saved_weights_snn,\n",
    "                'loss': loss_hist,\n",
    "                'train_acc_hist': train_acc_hist,\n",
    "                'test_acc_hist': test_acc_hist\n",
    "            }, file_path)\n",
    "        else:\n",
    "            print(\"best_accuracy\", best_accuracy)\n",
    "\n",
    "    return loss_hist\n",
    "\n",
    "def compute_classification_accuracy_double(x_data, y_data, s2_interval):\n",
    "    \"\"\" Computes classification accuracy on supplied data in batches. \"\"\"\n",
    "    accs = []\n",
    "    for x_local, y_local in sparse_data_generator_from_hdf5_spikes(x_data, y_data, s_batch_size, nb_steps, s1_nb_inputs, max_time, shuffle=False):\n",
    "        output,_ = run_double_snn_homo(x_local.to_dense(), s2_interval = s2_interval)\n",
    "        m,_= torch.max(output,1) # max over time\n",
    "        _,am=torch.max(m,1)      # argmax over output units\n",
    "        tmp = np.mean((y_local==am).detach().cpu().numpy()) # compare to labels\n",
    "        accs.append(tmp)\n",
    "    return np.mean(accs)\n",
    "\n",
    "# Initialize SNN 1 and SNN 2 parameters\n",
    "\n",
    "# SNN 1 parameters\n",
    "\n",
    "s1_thresholds_1 = nn.Parameter(torch.ones((1, 1), device=device, dtype=dtype, requires_grad=True))\n",
    "s1_reset_1 = nn.Parameter(torch.zeros((1, 1), device=device, dtype=dtype, requires_grad=True))\n",
    "s1_rest_1 = nn.Parameter(torch.zeros((1, 1), device=device, dtype=dtype, requires_grad=True))\n",
    "\n",
    "const_alpha = float(np.exp(-time_step/tau_syn))\n",
    "const_beta = float(np.exp(-time_step/tau_mem))\n",
    "\n",
    "s1_alpha_homo_1 = nn.Parameter(torch.full((1, 1), const_alpha, device=device, dtype=dtype, requires_grad=True))\n",
    "s1_beta_homo_1 = nn.Parameter(torch.full((1, 1), const_beta, device=device, dtype=dtype, requires_grad=True))\n",
    "\n",
    "s1_alpha_homo_2 = nn.Parameter(torch.full((1, 1), const_alpha, device=device, dtype=dtype, requires_grad=True))\n",
    "s1_beta_homo_2 = nn.Parameter(torch.full((1, 1), const_beta, device=device, dtype=dtype, requires_grad=True))\n",
    "\n",
    "# SNN 2 parameters\n",
    "\n",
    "s2_thresholds_1 = nn.Parameter(torch.ones((1, 1), device=device, dtype=dtype, requires_grad=True))\n",
    "s2_reset_1 = nn.Parameter(torch.zeros((1, 1), device=device, dtype=dtype, requires_grad=True))\n",
    "s2_rest_1 = nn.Parameter(torch.zeros((1, 1), device=device, dtype=dtype, requires_grad=True))\n",
    "\n",
    "s2_alpha_homo_1 = nn.Parameter(torch.full((1, 1), const_alpha, device=device, dtype=dtype, requires_grad=True))\n",
    "s2_beta_homo_1 = nn.Parameter(torch.full((1, 1), const_beta, device=device, dtype=dtype, requires_grad=True))\n",
    "\n",
    "\n",
    "# Initialize modulation factors\n",
    "s2_alpha_mod_factor1_pos = nn.Parameter(torch.full((1, 1), const_alpha, device=device, dtype=dtype, requires_grad=True))\n",
    "s2_alpha_mod_factor1_neg = nn.Parameter(torch.full((1, 1), const_alpha, device=device, dtype=dtype, requires_grad=True))\n",
    "s2_beta_mod_factor1_pos = nn.Parameter(torch.full((1, 1), const_alpha, device=device, dtype=dtype, requires_grad=True))\n",
    "s2_beta_mod_factor1_neg = nn.Parameter(torch.full((1, 1), const_alpha, device=device, dtype=dtype, requires_grad=True))\n",
    "s2_alpha_mod_factor2_pos = nn.Parameter(torch.full((1, 1), const_alpha, device=device, dtype=dtype, requires_grad=True))\n",
    "s2_alpha_mod_factor2_neg = nn.Parameter(torch.full((1, 1), const_alpha, device=device, dtype=dtype, requires_grad=True))\n",
    "s2_beta_mod_factor2_pos = nn.Parameter(torch.full((1, 1), const_alpha, device=device, dtype=dtype, requires_grad=True))\n",
    "s2_beta_mod_factor2_neg = nn.Parameter(torch.full((1, 1), const_alpha, device=device, dtype=dtype, requires_grad=True))\n",
    "s2_thresholds_mod_factor_pos = nn.Parameter(torch.full((1, 1), const_alpha, device=device, dtype=dtype, requires_grad=True))\n",
    "s2_thresholds_mod_factor_neg = nn.Parameter(torch.full((1, 1), const_alpha, device=device, dtype=dtype, requires_grad=True))\n",
    "s2_reset_mod_factor_pos = nn.Parameter(torch.full((1, 1), const_alpha, device=device, dtype=dtype, requires_grad=True))\n",
    "s2_reset_mod_factor_neg = nn.Parameter(torch.full((1, 1), const_alpha, device=device, dtype=dtype, requires_grad=True))\n",
    "s2_rest_mod_factor_pos = nn.Parameter(torch.full((1, 1), const_alpha, device=device, dtype=dtype, requires_grad=True))\n",
    "s2_rest_mod_factor_neg = nn.Parameter(torch.full((1, 1), const_alpha, device=device, dtype=dtype, requires_grad=True))\n",
    "\n",
    "# SNN 1 weights\n",
    "s1_weight_scale = 0.2\n",
    "\n",
    "s1_w1 = torch.empty((s1_nb_inputs, s1_nb_hidden),  device=device, dtype=dtype, requires_grad=True)\n",
    "torch.nn.init.normal_(s1_w1, mean=0.0, std=s1_weight_scale/np.sqrt(s1_nb_inputs))\n",
    "\n",
    "s1_w2 = torch.empty((s1_nb_hidden, s1_nb_outputs), device=device, dtype=dtype, requires_grad=True)\n",
    "torch.nn.init.normal_(s1_w2, mean=0.0, std=s1_weight_scale/np.sqrt(s1_nb_hidden))\n",
    "\n",
    "s1_v1 = torch.empty((s1_nb_hidden, s1_nb_hidden), device=device, dtype=dtype, requires_grad=True)\n",
    "torch.nn.init.normal_(s1_v1, mean=0.0, std=s1_weight_scale/np.sqrt(s1_nb_hidden))\n",
    "\n",
    "# SNN 2 weights\n",
    "s2_weight_scale = 0.2\n",
    "\n",
    "s2_w1 = torch.empty((s2_nb_inputs, s2_nb_hidden),  device=device, dtype=dtype, requires_grad=True)\n",
    "torch.nn.init.normal_(s2_w1, mean=0.0, std=s2_weight_scale/np.sqrt(s2_nb_inputs))\n",
    "\n",
    "s2_v1 = torch.empty((s2_nb_hidden, s2_nb_hidden), device=device, dtype=dtype, requires_grad=True)\n",
    "torch.nn.init.normal_(s2_v1, mean=0.0, std=s2_weight_scale/np.sqrt(s2_nb_hidden))\n",
    "\n",
    "loaded_weights_snn = torch.load('Python_Tests/SSC_homo_no_reg/epochs/snn_13.pth', map_location=torch.device(device))\n",
    "s1_w1 = torch.nn.Parameter(loaded_weights_snn['params']['w1'].to(device))\n",
    "s1_w2 = torch.nn.Parameter(loaded_weights_snn['params']['w2'].to(device))\n",
    "s1_v1 = torch.nn.Parameter(loaded_weights_snn['params']['v1'].to(device))\n",
    "s1_alpha_homo_1 = torch.nn.Parameter(loaded_weights_snn['params']['alpha'].to(device))\n",
    "s1_beta_homo_1 = torch.nn.Parameter(loaded_weights_snn['params']['beta'].to(device))\n",
    "s1_thresholds_1 = torch.nn.Parameter(loaded_weights_snn['params']['threshold'].to(device))\n",
    "s1_reset_1 = torch.nn.Parameter(loaded_weights_snn['params']['reset'].to(device))\n",
    "s1_rest_1 = torch.nn.Parameter(loaded_weights_snn['params']['rest'].to(device))\n",
    "s1_alpha_homo_2 = torch.nn.Parameter(loaded_weights_snn['params']['alpha_2'].to(device))\n",
    "s1_beta_homo_2 = torch.nn.Parameter(loaded_weights_snn['params']['beta_2'].to(device))\n",
    "\n",
    "# INTERVAL (DOUBLE) HERE\n",
    "s2_interval = 10\n",
    "nb_epochs_double = 100\n",
    "s_batch_size = 64\n",
    "lr_double = 2e-4\n",
    "loss_hist_snn = train_double_snn(x_train, y_train, lr=lr_double, nb_epochs=nb_epochs_double, s2_interval = s2_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double HETERO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "0.5602889150943396\n",
      "Parameter containing:\n",
      "tensor([[0.8435, 0.7279, 0.8964, 0.8699, 0.8599, 0.8616, 0.8820, 0.7949, 0.8599,\n",
      "         0.8721, 0.8755, 0.9605, 0.7079, 0.8238, 0.7247, 0.4365, 0.6525, 0.7271,\n",
      "         0.3876, 0.7894, 0.7537, 0.9026, 0.8108, 0.9147, 0.8894, 0.7979, 0.8469,\n",
      "         0.7822, 0.6684, 0.8756, 0.8728, 0.9136, 0.7775, 0.8649, 0.7798]],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "# print(torch.__version__)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "# import torchvision\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "import pickle\n",
    "import math\n",
    "from utils import get_shd_dataset\n",
    "\n",
    "# The coarse network structure and the time steps are dicated by the SHD dataset.\n",
    "nb_inputs  = 700\n",
    "nb_hidden  = 200\n",
    "nb_outputs = 35\n",
    "\n",
    "time_step = 1e-3\n",
    "nb_steps = 100\n",
    "max_time = 1.4\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "dtype = torch.float\n",
    "\n",
    "# Check whether a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Here we load the Dataset\n",
    "cache_dir = os.path.expanduser(\"~/data\")\n",
    "cache_subdir = \"ssc_data\"\n",
    "train_file = h5py.File(os.path.join(cache_dir, cache_subdir, 'ssc_train.h5'), 'r')\n",
    "validation_file = h5py.File(os.path.join(cache_dir, cache_subdir, 'ssc_valid.h5'), 'r')\n",
    "test_file = h5py.File(os.path.join(cache_dir, cache_subdir, 'ssc_test.h5'), 'r')\n",
    "\n",
    "x_train = train_file['spikes']\n",
    "y_train = train_file['labels']\n",
    "x_valid = validation_file['spikes']\n",
    "y_valid = validation_file['labels']\n",
    "x_test = test_file['spikes']\n",
    "y_test = test_file['labels']\n",
    "\n",
    "def sparse_data_generator_from_hdf5_spikes(X, y, batch_size, nb_steps, nb_units, max_time, shuffle=True):\n",
    "    \"\"\" This generator takes a spike dataset and generates spiking network input as sparse tensors.\n",
    "\n",
    "    Args:\n",
    "        X: The data ( sample x event x 2 ) the last dim holds (time,neuron) tuples\n",
    "        y: The labels\n",
    "    \"\"\"\n",
    "\n",
    "    labels_ = np.array(y,dtype=int)\n",
    "    number_of_batches = len(labels_)//batch_size\n",
    "    sample_index = np.arange(len(labels_))\n",
    "\n",
    "    # compute discrete firing times\n",
    "    firing_times = X['times']\n",
    "    units_fired = X['units']\n",
    "\n",
    "    time_bins = np.linspace(0, max_time, num=nb_steps)\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "\n",
    "    total_batch_count = 0\n",
    "    counter = 0\n",
    "    while counter<number_of_batches:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "\n",
    "        coo = [ [] for i in range(3) ]\n",
    "        for bc,idx in enumerate(batch_index):\n",
    "            times = np.digitize(firing_times[idx], time_bins)\n",
    "            units = units_fired[idx]\n",
    "            batch = [bc for _ in range(len(times))]\n",
    "\n",
    "            coo[0].extend(batch)\n",
    "            coo[1].extend(times)\n",
    "            coo[2].extend(units)\n",
    "\n",
    "        i = torch.LongTensor(coo).to(device)\n",
    "        v = torch.FloatTensor(np.ones(len(coo[0]))).to(device)\n",
    "\n",
    "        X_batch = torch.sparse.FloatTensor(i, v, torch.Size([batch_size,nb_steps,nb_units])).to(device)\n",
    "        y_batch = torch.tensor(labels_[batch_index],device=device)\n",
    "\n",
    "        yield X_batch.to(device=device), y_batch.to(device=device)\n",
    "\n",
    "        counter += 1\n",
    "        \n",
    "class SurrGradSpike(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Here we implement our spiking nonlinearity which also implements\n",
    "    the surrogate gradient. By subclassing torch.autograd.Function,\n",
    "    we will be able to use all of PyTorch's autograd functionality.\n",
    "    Here we use the normalized negative part of a fast sigmoid\n",
    "    as this was done in Zenke & Ganguli (2018).\n",
    "    \"\"\"\n",
    "\n",
    "    scale = 100.0 # controls steepness of surrogate gradient\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we compute a step function of the input Tensor\n",
    "        and return it. ctx is a context object that we use to stash information which\n",
    "        we need to later backpropagate our error signals. To achieve this we use the\n",
    "        ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        out = torch.zeros_like(input)\n",
    "        out[input > 0] = 1.0\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor we need to compute the\n",
    "        surrogate gradient of the loss with respect to the input.\n",
    "        Here we use the normalized negative part of a fast sigmoid\n",
    "        as this was done in Zenke & Ganguli (2018).\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad = grad_input/(SurrGradSpike.scale*torch.abs(input)+1.0)**2\n",
    "        return grad\n",
    "\n",
    "# here we overwrite our naive spike function by the \"SurrGradSpike\" nonlinearity which implements a surrogate gradient\n",
    "spike_fn  = SurrGradSpike.apply\n",
    "\n",
    "def dist_fn(dist):\n",
    "    return {\n",
    "        'gamma': lambda mean, k, size: np.random.gamma(k, scale=mean/k, size=size),\n",
    "        'normal': lambda mean, k, size: np.random.normal(loc=mean, scale=mean/np.sqrt(k), size=size), #change standard deviation to match gamma\n",
    "        'uniform': lambda _, maximum, size: np.random.uniform(low=0, high=maximum, size=size),\n",
    "    }[dist.lower()]\n",
    "    \n",
    "    \n",
    "    \n",
    "s_batch_size = 64\n",
    "\n",
    "# SNN 1 Network\n",
    "s1_nb_inputs  = 700\n",
    "s1_nb_hidden  = 200\n",
    "s1_nb_outputs = 35\n",
    "\n",
    "s_group_size = 1\n",
    "\n",
    "# SNN 2 Network\n",
    "s2_nb_inputs = 1961+45\n",
    "s2_nb_hidden = 1070*2/s_group_size\n",
    "\n",
    "\n",
    "\n",
    "def run_double_snn_hetero(s1_inputs, s2_interval):\n",
    "    s1_alpha_1_local = s1_alpha_hetero_1\n",
    "    s1_beta_1_local = s1_beta_hetero_1\n",
    "    s1_thresholds_local = s1_thresholds_1\n",
    "    s1_reset_local = s1_reset_1\n",
    "    s1_rest_local = s1_rest_1\n",
    "    s1_alpha_2_local = s1_alpha_hetero_2\n",
    "    s1_beta_2_local = s1_beta_hetero_2\n",
    "#     mod_factors_local = mod_factors\n",
    "    s1_alpha_1_local = s1_alpha_1_local.expand(s_batch_size, s1_nb_hidden)\n",
    "    s1_beta_1_local = s1_beta_1_local.expand(s_batch_size, s1_nb_hidden)\n",
    "    s1_thresholds_local = s1_thresholds_local.expand(s_batch_size, s1_nb_hidden)\n",
    "    s1_reset_local = s1_reset_local.expand(s_batch_size, s1_nb_hidden)\n",
    "    s1_rest_local = s1_rest_local.expand(s_batch_size, s1_nb_hidden)\n",
    "    s1_alpha_2_local = s1_alpha_2_local.expand(s_batch_size, s1_nb_outputs)\n",
    "    s1_beta_2_local = s1_beta_2_local.expand(s_batch_size, s1_nb_outputs)\n",
    "#     mod_factors_local = mod_factors_local.expand(s_batch_size, \n",
    "\n",
    "    # Initialize synaptic and membrane potentials for SNN 1\n",
    "    s1_syn = torch.zeros((s_batch_size, s1_nb_hidden), device=device, dtype=dtype)\n",
    "    s1_mem = torch.zeros((s_batch_size, s1_nb_hidden), device=device, dtype=dtype)\n",
    "\n",
    "    # Initialize recordings for membrane potentials and spikes for SNN 1\n",
    "    s1_mem_rec = []\n",
    "    s1_spk_rec = []\n",
    "\n",
    "    # Initialize outputs for the hidden layer for SNN 1\n",
    "    s1_out = torch.zeros((s_batch_size, s1_nb_hidden), device=device, dtype=dtype)\n",
    "    s1_h1_from_input = torch.einsum(\"abc,cd->abd\", (s1_inputs, s1_w1))\n",
    "\n",
    "    # Prepare readout layer variables for SNN 1\n",
    "    s1_flt2 = torch.zeros((s_batch_size, s1_nb_outputs), device=device, dtype=dtype)\n",
    "    s1_out2 = torch.zeros((s_batch_size, s1_nb_outputs), device=device, dtype=dtype)\n",
    "    s1_out_rec = [s1_out2]\n",
    "\n",
    "    # Initialize synaptic and membrane potentials for SNN 2\n",
    "    s2_syn = torch.zeros((s_batch_size, s2_nb_hidden), device=device, dtype=dtype)\n",
    "    s2_mem = torch.zeros((s_batch_size, s2_nb_hidden), device=device, dtype=dtype)\n",
    "\n",
    "    # Initialize recordings for membrane potentials and spikes for SNN 2\n",
    "    s2_mem_rec = []\n",
    "    s2_spk_rec = []\n",
    "    \n",
    "    test = 0\n",
    "\n",
    "    # Initialize outputs for the hidden layer for SNN 2\n",
    "    s2_out = torch.zeros((s_batch_size, s2_nb_hidden), device=device, dtype=dtype)\n",
    "\n",
    "    # Buffer to store spikes from the last s2_interval timesteps for SNN 2\n",
    "    s2_spike_buffer = torch.zeros((s2_interval, s_batch_size, s2_nb_hidden), device=device, dtype=dtype)\n",
    "\n",
    "    for s1_t in range(nb_steps):\n",
    "        # Compute hidden layer activity for SNN 1\n",
    "        s1_h1 = s1_h1_from_input[:, s1_t] + torch.einsum(\"ab,bc->ac\", (s1_out, s1_v1))\n",
    "        s1_mthr = s1_mem - s1_thresholds_local\n",
    "        s1_out = spike_fn(s1_mthr)\n",
    "        s1_rst = torch.zeros_like(s1_mem)\n",
    "        c = (s1_mthr > 0)\n",
    "        s1_rst[c] = torch.ones_like(s1_mem)[c]\n",
    "\n",
    "        s1_syn = s1_alpha_1_local * s1_syn + s1_h1\n",
    "        s1_mem = s1_beta_1_local * (s1_mem - s1_rest_local) + s1_rest_local + (1 - s1_beta_1_local) * s1_syn - s1_rst * (s1_thresholds_local - s1_reset_local)\n",
    "\n",
    "        s1_mem_rec.append(s1_mem)\n",
    "        s1_spk_rec.append(s1_out)\n",
    "\n",
    "        # Compute output for the readout layer for SNN 1\n",
    "        s1_h2_t = torch.einsum(\"ab,bc->ac\", (s1_out, s1_w2))\n",
    "        s1_flt2 = s1_alpha_2_local * s1_flt2 + s1_h2_t\n",
    "        s1_out2 = s1_beta_2_local * s1_out2 + (1 - s1_beta_2_local) * s1_flt2\n",
    "\n",
    "        s1_out_rec.append(s1_out2)\n",
    "\n",
    "        s1_vars = torch.cat((s1_alpha_1_local, s1_beta_1_local,\n",
    "                             s1_thresholds_local, s1_reset_local, s1_rest_local,\n",
    "                             s1_alpha_2_local, s1_beta_2_local), dim=1)\n",
    "\n",
    "        s1_time_tensor = torch.full((s_batch_size, 1), s1_t, device=device, dtype=dtype)\n",
    "\n",
    "        # Compute hidden layer activity for SNN 2 using spikes from SNN 1 as input\n",
    "        s2_input_combined = torch.cat((s1_vars, s1_time_tensor, s1_inputs[:, s1_t, :], s1_out, s1_out2), dim=1)\n",
    "        s2_h1 = torch.einsum(\"ab,bc->ac\", (s2_input_combined, s2_w1))\n",
    "        s2_mthr = s2_mem - s2_thresholds_1\n",
    "        s2_out = spike_fn(s2_mthr)\n",
    "        s2_rst = torch.zeros_like(s2_mem)\n",
    "        c = (s2_mthr > 0)\n",
    "        s2_rst[c] = torch.ones_like(s2_mem)[c]\n",
    "\n",
    "        s2_new_syn = s2_alpha_hetero_1 * s2_syn + s2_h1\n",
    "        s2_new_mem = s2_beta_hetero_1 * (s2_mem - s2_rest_1) + s2_rest_1 + (1 - s2_beta_hetero_1) * s2_syn - s2_rst * (s2_thresholds_1 - s2_reset_1)\n",
    "\n",
    "        s2_mem_rec.append(s2_mem)\n",
    "        s2_spk_rec.append(s2_out)\n",
    "\n",
    "        s2_mem = s2_new_mem\n",
    "        s2_syn = s2_new_syn\n",
    "\n",
    "        # Update the spike buffer\n",
    "        s2_spike_buffer[s1_t % s2_interval] = s2_out\n",
    "\n",
    "        # Use spikes from the last s2_interval timesteps to modulate parameters of SNN 1\n",
    "        if s1_t >= s2_interval and s1_t % s2_interval == 0:\n",
    "            # Aggregate spikes over the last s2_interval timesteps\n",
    "            recent_spikes = s2_spike_buffer.sum(dim=0)\n",
    "\n",
    "            # Modulation neurons indices\n",
    "            mod_indices = {\n",
    "                's1_alpha_1': (0, 400),\n",
    "                's1_beta_1': (400, 800),\n",
    "                's1_thresholds': (800, 1200),\n",
    "                's1_reset': (1200, 1600),\n",
    "                's1_rest': (1600, 2000),\n",
    "                's1_alpha_2': (2000, 2070),\n",
    "                's1_beta_2': (2070, 2140)\n",
    "            }\n",
    "#             if test ==0:\n",
    "# #                 test += 1\n",
    "#                 print(\"PRE\")\n",
    "#                 print(s1_alpha_2_local)\n",
    "\n",
    "            for param, (start_idx, end_idx) in mod_indices.items():\n",
    "                # Calculate the midpoint\n",
    "                mid_idx = start_idx + (end_idx - start_idx) // 2\n",
    "                pos_mod = torch.sum(recent_spikes[:, start_idx:mid_idx], dim=1, keepdim=True)\n",
    "                neg_mod = torch.sum(recent_spikes[:, mid_idx:end_idx], dim=1, keepdim=True)\n",
    "                effect = pos_mod * mod_factors[:, start_idx:mid_idx] - neg_mod * mod_factors[:, mid_idx:end_idx]\n",
    "\n",
    "                if param == 's1_alpha_1':\n",
    "#                     print(1, \" \", effect)\n",
    "                    s1_alpha_1_local = s1_alpha_1_local + effect\n",
    "                elif param == 's1_beta_1':\n",
    "#                     print(2, \" \", effect)\n",
    "                    s1_beta_1_local = s1_beta_1_local + effect\n",
    "                elif param == 's1_alpha_2':\n",
    "                    s1_alpha_2_local = s1_alpha_2_local + effect\n",
    "#                     if test == 0:\n",
    "#                             test+=1\n",
    "#                             print(3, \" \", effect)\n",
    "#                             print(s1_alpha_2_local)\n",
    "                elif param == 's1_beta_2':\n",
    "#                     print(4, \" \", effect)\n",
    "                    s1_beta_2_local = s1_beta_2_local + effect\n",
    "                elif param == 's1_thresholds':\n",
    "#                     print(5, \" \", effect)\n",
    "                    s1_thresholds_local = s1_thresholds_local + effect\n",
    "                elif param == 's1_reset':\n",
    "#                     print(6, \" \", effect)\n",
    "                    s1_reset_local = s1_reset_local + effect\n",
    "                elif param == 's1_rest':\n",
    "#                     print(7, \" \", effect)\n",
    "                    s1_rest_local = s1_rest_local + effect\n",
    "#             if test ==0:\n",
    "#                 test += 1\n",
    "#                 print(\"POST\")\n",
    "#                 print(s1_alpha_2_local)\n",
    "\n",
    "            # Ensure modulation effects are stable\n",
    "            s1_alpha_1_local = torch.clamp(s1_alpha_1_local, min=0.367, max=0.995)\n",
    "            s1_beta_1_local = torch.clamp(s1_beta_1_local, min=0.367, max=0.995)\n",
    "            s1_alpha_2_local = torch.clamp(s1_alpha_2_local, min=0.367, max=0.995)\n",
    "            s1_beta_2_local = torch.clamp(s1_beta_2_local, min=0.367, max=0.995)\n",
    "            s1_thresholds_local = torch.clamp(s1_thresholds_local, min=0.5, max=1.5)\n",
    "            s1_reset_local = torch.clamp(s1_reset_local, min=-0.5, max=0.5)\n",
    "            s1_rest_local = torch.clamp(s1_rest_local, min=-0.5, max=0.5)\n",
    "\n",
    "    # Stack recordings for output\n",
    "    s1_mem_rec = torch.stack(s1_mem_rec, dim=1)\n",
    "    s1_spk_rec = torch.stack(s1_spk_rec, dim=1)\n",
    "    s1_out_rec = torch.stack(s1_out_rec[1:], dim=1)  # Skip the initial zero tensor\n",
    "\n",
    "    s1_other_recs = [s1_mem_rec, s1_spk_rec]\n",
    "\n",
    "    return s1_out_rec, s1_other_recs\n",
    "\n",
    "\n",
    "def compute_classification_accuracy_double_hetero(x_data, y_data, s2_interval):\n",
    "    \"\"\" Computes classification accuracy on supplied data in batches. \"\"\"\n",
    "    accs = []\n",
    "    for x_local, y_local in sparse_data_generator_from_hdf5_spikes(x_data, y_data, s_batch_size, nb_steps, s1_nb_inputs, max_time, shuffle=False):\n",
    "        output,_ = run_double_snn_hetero(x_local.to_dense(), s2_interval = s2_interval)\n",
    "        m,_= torch.max(output,1) # max over time\n",
    "        _,am=torch.max(m,1)      # argmax over output units\n",
    "        tmp = np.mean((y_local==am).detach().cpu().numpy()) # compare to labels\n",
    "        accs.append(tmp)\n",
    "    return np.mean(accs)\n",
    "\n",
    "\n",
    "def train_double_snn(x_data, y_data, lr=1e-3, nb_epochs=10, s2_interval=10):\n",
    "    # Include all learnable parameters in the list\n",
    "    params = [\n",
    "        s1_w1, s1_w2, s1_v1,\n",
    "        s2_w1, s2_v1,\n",
    "        mod_factors,\n",
    "        s1_thresholds_1, s1_reset_1, s1_rest_1,\n",
    "        s1_alpha_hetero_1, s1_beta_hetero_1,\n",
    "        s1_alpha_hetero_2, s1_beta_hetero_2,\n",
    "        s2_thresholds_1, s2_reset_1, s2_rest_1,\n",
    "        s2_alpha_hetero_1, s2_beta_hetero_1\n",
    "    ]\n",
    "\n",
    "    optimizer = torch.optim.Adam(params, lr=lr, betas=(0.9, 0.999))\n",
    "\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    log_softmax_fn = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    best_accuracy = 0\n",
    "    loss_hist = []\n",
    "    train_acc_hist = []\n",
    "    test_acc_hist = []\n",
    "\n",
    "    for epoch in range(nb_epochs):\n",
    "        local_loss = []\n",
    "        local_ground_loss = []\n",
    "        local_reg_loss = []\n",
    "        accs = []\n",
    "#         print(\"MOD \", mod_factors)\n",
    "\n",
    "        for x_local, y_local in sparse_data_generator_from_hdf5_spikes(x_data, y_data, s_batch_size, nb_steps, s1_nb_inputs, max_time):\n",
    "            # Ensure x_local is detached from any previous computation graph\n",
    "            x_local = x_local.to_dense().detach()\n",
    "\n",
    "            output, recs = run_double_snn_hetero(x_local, s2_interval=s2_interval)\n",
    "            _, spks = recs\n",
    "            m, _ = torch.max(output, 1)\n",
    "\n",
    "            _, am = torch.max(m, 1)  # argmax over output units\n",
    "            tmp = np.mean((y_local == am).detach().cpu().numpy())  # compare to labels\n",
    "            accs.append(tmp)\n",
    "\n",
    "            log_p_y = log_softmax_fn(m)\n",
    "            ground_loss = loss_fn(log_p_y, y_local)\n",
    "\n",
    "            # L2 loss on spikes per neuron\n",
    "#             reg_loss = 1e-6 * torch.sum(spks)  # L1 loss on total number of spikes\n",
    "#             reg_loss += 1e-6 * torch.mean(torch.sum(torch.sum(spks, dim=0), dim=0)**2)  # L2 loss on spikes per neuron\n",
    "            tot_num_neurons = s1_nb_hidden + s1_nb_outputs\n",
    "            N_samp = 6412\n",
    "            T = nb_steps\n",
    "            sl = 1\n",
    "            thetal = 0.01\n",
    "            su = 0.06\n",
    "            thetau = 100\n",
    "            tmp = (torch.clamp((1 / T) * torch.sum(spks, 1) - thetal, min=0.)) ** 2\n",
    "            L1_batch = torch.sum(tmp, (0, 1))\n",
    "            reg_loss = (sl / (N_samp + tot_num_neurons)) * L1_batch\n",
    "\n",
    "            tmp2 = (torch.clamp((1 / nb_hidden) * torch.sum(spks, (1, 2)) - thetau, min=0.)) ** 2\n",
    "            L2_batch = torch.sum(tmp2)\n",
    "            reg_loss += (su / N_samp) * L2_batch\n",
    "            \n",
    "            loss_val = ground_loss + reg_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            mod_factors.data.clamp_(min=0)\n",
    "            # Clamping the values\n",
    "            with torch.no_grad():\n",
    "                s1_alpha_hetero_1.clamp_(0.367, 0.995)\n",
    "                s1_beta_hetero_1.clamp_(0.367, 0.995)\n",
    "                s1_alpha_hetero_2.clamp_(0.367, 0.995)\n",
    "                s1_beta_hetero_2.clamp_(0.367, 0.995)\n",
    "                s1_thresholds_1.clamp_(0.5, 1.5)\n",
    "            \n",
    "            local_loss.append(loss_val.item())\n",
    "            local_ground_loss.append(ground_loss.item())\n",
    "            local_reg_loss.append(reg_loss.item())\n",
    "\n",
    "        mean_loss = np.mean(local_loss)\n",
    "        loss_hist.append(mean_loss)\n",
    "        print(f\"Epoch {epoch + 1}: loss={mean_loss:.5f}\")\n",
    "        print(\"ground_loss\", np.mean(local_ground_loss))\n",
    "        print(\"reg_loss\", np.mean(local_reg_loss))\n",
    "        train_accuracy = np.mean(accs)\n",
    "        train_acc_hist.append(train_accuracy)\n",
    "\n",
    "        test_accuracy = compute_classification_accuracy_double_hetero(x_test, y_test, s2_interval)\n",
    "        test_acc_hist.append(test_accuracy)\n",
    "        print(f\"Epoch {epoch + 1}: Train= {train_accuracy:.5f} Test Accuracy={test_accuracy:.5f}\")\n",
    "\n",
    "        if test_accuracy > best_accuracy:\n",
    "            best_accuracy = test_accuracy\n",
    "            print(\"best_accuracy\", best_accuracy)\n",
    "\n",
    "            # Create a dictionary of current parameters to save\n",
    "            saved_weights_snn = {\n",
    "                's1_w1': s1_w1,\n",
    "                's1_w2': s1_w2,\n",
    "                's1_v1': s1_v1,\n",
    "                's2_w1': s2_w1,\n",
    "                's2_v1': s2_v1,\n",
    "                'mod_factors': mod_factors,\n",
    "                's1_thresholds_1': s1_thresholds_1,\n",
    "                's1_reset_1': s1_reset_1,\n",
    "                's1_rest_1': s1_rest_1,\n",
    "                's2_thresholds_1': s2_thresholds_1,\n",
    "                's2_reset_1': s2_reset_1,\n",
    "                's2_rest_1': s2_rest_1,\n",
    "                's2_alpha_hetero_1': s2_alpha_hetero_1,\n",
    "                's2_beta_hetero_1': s2_beta_hetero_1,\n",
    "                's1_alpha_hetero_1': s1_alpha_hetero_1,\n",
    "                's1_beta_hetero_1': s1_beta_hetero_1,\n",
    "                's1_alpha_hetero_2': s1_alpha_hetero_2,\n",
    "                's1_beta_hetero_2': s1_beta_hetero_2\n",
    "            }\n",
    "\n",
    "            directory = 'SSC_Double_Hetero'\n",
    "\n",
    "            # Create the directory if it does not exist\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "\n",
    "            # Save parameters along with the current epoch and accuracy\n",
    "            file_path = os.path.join(directory, 'best_double_hetero.pth')\n",
    "\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'accuracy': test_accuracy,\n",
    "                'params': saved_weights_snn,\n",
    "                'loss': loss_hist,\n",
    "                'train_acc_hist': train_acc_hist,\n",
    "                'test_acc_hist': test_acc_hist\n",
    "            }, file_path)\n",
    "        else:\n",
    "            print(\"best_accuracy\", best_accuracy)\n",
    "\n",
    "    return loss_hist\n",
    "\n",
    "s_batch_size = 64\n",
    "\n",
    "# SNN 1 Network\n",
    "s1_nb_inputs  = 700\n",
    "s1_nb_hidden  = 200\n",
    "s1_nb_outputs = 35\n",
    "\n",
    "s_group_size = 1\n",
    "\n",
    "# SNN 2 Network\n",
    "s2_nb_inputs = 2006\n",
    "s2_nb_hidden = int(1070 * 2 / s_group_size)\n",
    "\n",
    "# Initialize SNN 1 and SNN 2 parameters\n",
    "\n",
    "# SNN 1 parameters\n",
    "tau_syn = 10e-3\n",
    "tau_mem = 20e-3\n",
    "\n",
    "s1_thresholds_1 = nn.Parameter(torch.ones((1, s1_nb_hidden), device=device, dtype=dtype, requires_grad=True))\n",
    "s1_reset_1 = nn.Parameter(torch.zeros((1, s1_nb_hidden), device=device, dtype=dtype, requires_grad=True))\n",
    "s1_rest_1 = nn.Parameter(torch.zeros((1, s1_nb_hidden), device=device, dtype=dtype, requires_grad=True))\n",
    "\n",
    "const_alpha = float(np.exp(-time_step/tau_syn))\n",
    "const_beta = float(np.exp(-time_step/tau_mem))\n",
    "\n",
    "s1_alpha_hetero_1 = nn.Parameter(torch.full((1, s1_nb_hidden), const_alpha, device=device, dtype=dtype, requires_grad=True))\n",
    "s1_beta_hetero_1 = nn.Parameter(torch.full((1, s1_nb_hidden), const_beta, device=device, dtype=dtype, requires_grad=True))\n",
    "\n",
    "s1_alpha_hetero_2 = nn.Parameter(torch.full((1, s1_nb_outputs), const_alpha, device=device, dtype=dtype, requires_grad=True))\n",
    "s1_beta_hetero_2 = nn.Parameter(torch.full((1, s1_nb_outputs), const_beta, device=device, dtype=dtype, requires_grad=True))\n",
    "\n",
    "# SNN 2 parameters\n",
    "\n",
    "s2_thresholds_1 = nn.Parameter(torch.ones((1, s2_nb_hidden), device=device, dtype=dtype, requires_grad=True))\n",
    "s2_reset_1 = nn.Parameter(torch.zeros((1, s2_nb_hidden), device=device, dtype=dtype, requires_grad=True))\n",
    "s2_rest_1 = nn.Parameter(torch.zeros((1, s2_nb_hidden), device=device, dtype=dtype, requires_grad=True))\n",
    "\n",
    "s2_alpha_hetero_1 = nn.Parameter(torch.full((1, s2_nb_hidden), const_alpha, device=device, dtype=dtype, requires_grad=True))\n",
    "s2_beta_hetero_1 = nn.Parameter(torch.full((1, s2_nb_hidden), const_beta, device=device, dtype=dtype, requires_grad=True))\n",
    "\n",
    "# Initialize modulation factors as a single tensor\n",
    "mod_factors = nn.Parameter(torch.full((1, s2_nb_hidden), 0.01, device=device, dtype=dtype, requires_grad=True))\n",
    "\n",
    "# SNN 1 weights\n",
    "s1_weight_scale = 0.2\n",
    "\n",
    "s1_w1 = torch.empty((s1_nb_inputs, s1_nb_hidden), device=device, dtype=dtype, requires_grad=True)\n",
    "torch.nn.init.normal_(s1_w1, mean=0.0, std=s1_weight_scale/np.sqrt(s1_nb_inputs))\n",
    "\n",
    "s1_w2 = torch.empty((s1_nb_hidden, s1_nb_outputs), device=device, dtype=dtype, requires_grad=True)\n",
    "torch.nn.init.normal_(s1_w2, mean=0.0, std=s1_weight_scale/np.sqrt(s1_nb_hidden))\n",
    "\n",
    "s1_v1 = torch.empty((s1_nb_hidden, s1_nb_hidden), device=device, dtype=dtype, requires_grad=True)\n",
    "torch.nn.init.normal_(s1_v1, mean=0.0, std=s1_weight_scale/np.sqrt(s1_nb_hidden))\n",
    "\n",
    "# SNN 2 weights\n",
    "s2_weight_scale = 0.2\n",
    "\n",
    "s2_w1 = nn.Parameter(torch.zeros((s2_nb_inputs, s2_nb_hidden), device=device, dtype=dtype, requires_grad=True))\n",
    "for i in range(min(s2_nb_hidden, s2_nb_inputs, s2_nb_hidden)):\n",
    "    s2_w1.data[i, i] = 0.2\n",
    "\n",
    "s2_v1 = nn.Parameter(torch.zeros((s2_nb_hidden, s2_nb_hidden), device=device, dtype=dtype, requires_grad=True))\n",
    "for i in range(min(s2_nb_hidden, s2_nb_hidden)):\n",
    "    s2_v1.data[i, i] = 0.2\n",
    "    \n",
    "# print(s2_v1)\n",
    "\n",
    "loaded_weights_snn = torch.load('Python_Tests/SSC_hetero_no_reg/epochs_hetero/snn_13.pth', map_location=torch.device(device))\n",
    "\n",
    "# Convert tensors to parameters and ensure they are leaf tensors by re-wrapping them\n",
    "# Move tensors to device first and then wrap them as parameters\n",
    "s1_w1 = torch.nn.Parameter(loaded_weights_snn['params']['w1'].to(device))\n",
    "s1_w2 = torch.nn.Parameter(loaded_weights_snn['params']['w2'].to(device))\n",
    "s1_v1 = torch.nn.Parameter(loaded_weights_snn['params']['v1'].to(device))\n",
    "s1_alpha_hetero_1 = torch.nn.Parameter(loaded_weights_snn['params']['alpha'].to(device))\n",
    "s1_beta_hetero_1 = torch.nn.Parameter(loaded_weights_snn['params']['beta'].to(device))\n",
    "s1_thresholds_1 = torch.nn.Parameter(loaded_weights_snn['params']['threshold'].to(device))\n",
    "s1_reset_1 = torch.nn.Parameter(loaded_weights_snn['params']['reset'].to(device))\n",
    "s1_rest_1 = torch.nn.Parameter(loaded_weights_snn['params']['rest'].to(device))\n",
    "s1_alpha_hetero_2 = torch.nn.Parameter(loaded_weights_snn['params']['alpha_2'].to(device))\n",
    "s1_beta_hetero_2 = torch.nn.Parameter(loaded_weights_snn['params']['beta_2'].to(device))\n",
    "print(loaded_weights_snn['accuracy'])\n",
    "\n",
    "print(s1_beta_hetero_2)\n",
    "\n",
    "# INTERVAL (DOUBLE) HERE\n",
    "# s2_interval = 5\n",
    "# nb_epochs_double = 50\n",
    "# s_batch_size = 64\n",
    "# lr_double = 2e-4\n",
    "# loss_hist_snn = train_double_snn(x_train, y_train, lr=lr_double, nb_epochs=nb_epochs_double, s2_interval = s2_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.9 (test1)",
   "language": "python",
   "name": "python39_test1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
